[2019-01-30 18:27:27] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-01-30 18:27:27] [marian] Running on gpu-e-64 as process 74411 with command line:
[2019-01-30 18:27:27] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus00.nrm.en.gz ../data/corpus00.nrm.cs.gz --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-01-30 18:27:27] [config] after-batches: 0
[2019-01-30 18:27:27] [config] after-epochs: 0
[2019-01-30 18:27:27] [config] allow-unk: false
[2019-01-30 18:27:27] [config] beam-size: 8
[2019-01-30 18:27:27] [config] bert-class-symbol: "[CLS]"
[2019-01-30 18:27:27] [config] bert-mask-symbol: "[MASK]"
[2019-01-30 18:27:27] [config] bert-masking-fraction: 0.15
[2019-01-30 18:27:27] [config] bert-sep-symbol: "[SEP]"
[2019-01-30 18:27:27] [config] best-deep: false
[2019-01-30 18:27:27] [config] clip-gemm: 0
[2019-01-30 18:27:27] [config] clip-norm: 5
[2019-01-30 18:27:27] [config] cost-type: ce-mean
[2019-01-30 18:27:27] [config] cpu-threads: 0
[2019-01-30 18:27:27] [config] data-weighting-type: sentence
[2019-01-30 18:27:27] [config] dec-cell: gru
[2019-01-30 18:27:27] [config] dec-cell-base-depth: 2
[2019-01-30 18:27:27] [config] dec-cell-high-depth: 1
[2019-01-30 18:27:27] [config] dec-depth: 6
[2019-01-30 18:27:27] [config] devices:
[2019-01-30 18:27:27] [config]   - 0
[2019-01-30 18:27:27] [config]   - 1
[2019-01-30 18:27:27] [config]   - 2
[2019-01-30 18:27:27] [config]   - 3
[2019-01-30 18:27:27] [config] dim-emb: 512
[2019-01-30 18:27:27] [config] dim-rnn: 1024
[2019-01-30 18:27:27] [config] dim-vocabs:
[2019-01-30 18:27:27] [config]   - 32000
[2019-01-30 18:27:27] [config]   - 32000
[2019-01-30 18:27:27] [config] disp-first: 0
[2019-01-30 18:27:27] [config] disp-freq: 1000
[2019-01-30 18:27:27] [config] disp-label-counts: false
[2019-01-30 18:27:27] [config] dropout-rnn: 0
[2019-01-30 18:27:27] [config] dropout-src: 0
[2019-01-30 18:27:27] [config] dropout-trg: 0
[2019-01-30 18:27:27] [config] early-stopping: 5
[2019-01-30 18:27:27] [config] embedding-fix-src: false
[2019-01-30 18:27:27] [config] embedding-fix-trg: false
[2019-01-30 18:27:27] [config] embedding-normalization: false
[2019-01-30 18:27:27] [config] enc-cell: gru
[2019-01-30 18:27:27] [config] enc-cell-depth: 1
[2019-01-30 18:27:27] [config] enc-depth: 6
[2019-01-30 18:27:27] [config] enc-type: bidirectional
[2019-01-30 18:27:27] [config] exponential-smoothing: 0.0001
[2019-01-30 18:27:27] [config] grad-dropping-momentum: 0
[2019-01-30 18:27:27] [config] grad-dropping-rate: 0
[2019-01-30 18:27:27] [config] grad-dropping-warmup: 100
[2019-01-30 18:27:27] [config] guided-alignment: none
[2019-01-30 18:27:27] [config] guided-alignment-cost: mse
[2019-01-30 18:27:27] [config] guided-alignment-weight: 0.1
[2019-01-30 18:27:27] [config] ignore-model-config: false
[2019-01-30 18:27:27] [config] input-types:
[2019-01-30 18:27:27] [config]   []
[2019-01-30 18:27:27] [config] interpolate-env-vars: false
[2019-01-30 18:27:27] [config] keep-best: true
[2019-01-30 18:27:27] [config] label-smoothing: 0.1
[2019-01-30 18:27:27] [config] layer-normalization: true
[2019-01-30 18:27:27] [config] learn-rate: 0.0003
[2019-01-30 18:27:27] [config] log: ./models/encs/00.baseline/train.log
[2019-01-30 18:27:27] [config] log-level: info
[2019-01-30 18:27:27] [config] lr-decay: 0
[2019-01-30 18:27:27] [config] lr-decay-freq: 50000
[2019-01-30 18:27:27] [config] lr-decay-inv-sqrt:
[2019-01-30 18:27:27] [config]   - 16000
[2019-01-30 18:27:27] [config] lr-decay-repeat-warmup: false
[2019-01-30 18:27:27] [config] lr-decay-reset-optimizer: false
[2019-01-30 18:27:27] [config] lr-decay-start:
[2019-01-30 18:27:27] [config]   - 10
[2019-01-30 18:27:27] [config]   - 1
[2019-01-30 18:27:27] [config] lr-decay-strategy: epoch+stalled
[2019-01-30 18:27:27] [config] lr-report: true
[2019-01-30 18:27:27] [config] lr-warmup: 16000
[2019-01-30 18:27:27] [config] lr-warmup-at-reload: false
[2019-01-30 18:27:27] [config] lr-warmup-cycle: false
[2019-01-30 18:27:27] [config] lr-warmup-start-rate: 0
[2019-01-30 18:27:27] [config] max-length: 120
[2019-01-30 18:27:27] [config] max-length-crop: false
[2019-01-30 18:27:27] [config] max-length-factor: 3
[2019-01-30 18:27:27] [config] maxi-batch: 1000
[2019-01-30 18:27:27] [config] maxi-batch-sort: trg
[2019-01-30 18:27:27] [config] mini-batch: 1000
[2019-01-30 18:27:27] [config] mini-batch-fit: true
[2019-01-30 18:27:27] [config] mini-batch-fit-step: 10
[2019-01-30 18:27:27] [config] mini-batch-overstuff: 1
[2019-01-30 18:27:27] [config] mini-batch-track-lr: false
[2019-01-30 18:27:27] [config] mini-batch-understuff: 1
[2019-01-30 18:27:27] [config] mini-batch-warmup: 0
[2019-01-30 18:27:27] [config] mini-batch-words: 0
[2019-01-30 18:27:27] [config] mini-batch-words-ref: 0
[2019-01-30 18:27:27] [config] model: ./models/encs/00.baseline/model.npz
[2019-01-30 18:27:27] [config] multi-loss-type: sum
[2019-01-30 18:27:27] [config] multi-node: false
[2019-01-30 18:27:27] [config] multi-node-overlap: true
[2019-01-30 18:27:27] [config] n-best: false
[2019-01-30 18:27:27] [config] no-nccl: false
[2019-01-30 18:27:27] [config] no-reload: false
[2019-01-30 18:27:27] [config] no-restore-corpus: false
[2019-01-30 18:27:27] [config] no-shuffle: false
[2019-01-30 18:27:27] [config] normalize: 1
[2019-01-30 18:27:27] [config] optimizer: adam
[2019-01-30 18:27:27] [config] optimizer-delay: 1
[2019-01-30 18:27:27] [config] optimizer-params:
[2019-01-30 18:27:27] [config]   - 0.9
[2019-01-30 18:27:27] [config]   - 0.98
[2019-01-30 18:27:27] [config]   - 1e-09
[2019-01-30 18:27:27] [config] overwrite: true
[2019-01-30 18:27:27] [config] quiet: false
[2019-01-30 18:27:27] [config] quiet-translation: true
[2019-01-30 18:27:27] [config] relative-paths: false
[2019-01-30 18:27:27] [config] right-left: false
[2019-01-30 18:27:27] [config] save-freq: 5000
[2019-01-30 18:27:27] [config] seed: 0
[2019-01-30 18:27:27] [config] sentencepiece-alphas:
[2019-01-30 18:27:27] [config]   []
[2019-01-30 18:27:27] [config] sentencepiece-max-lines: 10000000
[2019-01-30 18:27:27] [config] sentencepiece-options: ""
[2019-01-30 18:27:27] [config] shuffle-in-ram: false
[2019-01-30 18:27:27] [config] skip: false
[2019-01-30 18:27:27] [config] sqlite: ""
[2019-01-30 18:27:27] [config] sqlite-drop: false
[2019-01-30 18:27:27] [config] sync-sgd: true
[2019-01-30 18:27:27] [config] tempdir: /tmp
[2019-01-30 18:27:27] [config] tied-embeddings: false
[2019-01-30 18:27:27] [config] tied-embeddings-all: true
[2019-01-30 18:27:27] [config] tied-embeddings-src: false
[2019-01-30 18:27:27] [config] train-sets:
[2019-01-30 18:27:27] [config]   - ../data/corpus00.nrm.en.gz
[2019-01-30 18:27:27] [config]   - ../data/corpus00.nrm.cs.gz
[2019-01-30 18:27:27] [config] transformer-aan-activation: swish
[2019-01-30 18:27:27] [config] transformer-aan-depth: 2
[2019-01-30 18:27:27] [config] transformer-aan-nogate: false
[2019-01-30 18:27:27] [config] transformer-decoder-autoreg: self-attention
[2019-01-30 18:27:27] [config] transformer-dim-aan: 2048
[2019-01-30 18:27:27] [config] transformer-dim-ffn: 2048
[2019-01-30 18:27:27] [config] transformer-dropout: 0.1
[2019-01-30 18:27:27] [config] transformer-dropout-attention: 0
[2019-01-30 18:27:27] [config] transformer-dropout-ffn: 0
[2019-01-30 18:27:27] [config] transformer-ffn-activation: swish
[2019-01-30 18:27:27] [config] transformer-ffn-depth: 2
[2019-01-30 18:27:27] [config] transformer-guided-alignment-layer: last
[2019-01-30 18:27:27] [config] transformer-heads: 8
[2019-01-30 18:27:27] [config] transformer-no-projection: false
[2019-01-30 18:27:27] [config] transformer-postprocess: da
[2019-01-30 18:27:27] [config] transformer-postprocess-emb: d
[2019-01-30 18:27:27] [config] transformer-preprocess: n
[2019-01-30 18:27:27] [config] transformer-tied-layers:
[2019-01-30 18:27:27] [config]   []
[2019-01-30 18:27:27] [config] transformer-train-positions: false
[2019-01-30 18:27:27] [config] type: transformer
[2019-01-30 18:27:27] [config] ulr: false
[2019-01-30 18:27:27] [config] ulr-dim-emb: 0
[2019-01-30 18:27:27] [config] ulr-dropout: 0
[2019-01-30 18:27:27] [config] ulr-keys-vectors: ""
[2019-01-30 18:27:27] [config] ulr-query-vectors: ""
[2019-01-30 18:27:27] [config] ulr-softmax-temperature: 1
[2019-01-30 18:27:27] [config] ulr-trainable-transformation: false
[2019-01-30 18:27:27] [config] valid-freq: 5000
[2019-01-30 18:27:27] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-01-30 18:27:27] [config] valid-max-length: 1000
[2019-01-30 18:27:27] [config] valid-metrics:
[2019-01-30 18:27:27] [config]   - ce-mean-words
[2019-01-30 18:27:27] [config]   - bleu-detok
[2019-01-30 18:27:27] [config] valid-mini-batch: 32
[2019-01-30 18:27:27] [config] valid-sets:
[2019-01-30 18:27:27] [config]   - ../data/newstest2016.en
[2019-01-30 18:27:27] [config]   - ../data/newstest2016.cs
[2019-01-30 18:27:27] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-01-30 18:27:27] [config] vocabs:
[2019-01-30 18:27:27] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-01-30 18:27:27] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-01-30 18:27:27] [config] word-penalty: 0
[2019-01-30 18:27:27] [config] workspace: 10000
[2019-01-30 18:27:27] [config] Model is being created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-01-30 18:27:27] Using synchronous training
[2019-01-30 18:27:27] [SentencePiece] Training SentencePiece vocabulary ./models/encs/00.baseline/vocab.encs.spm
[2019-01-30 18:27:27] [SentencePiece] Creating temporary file /tmp/marian.6R7apd 
[2019-01-30 18:27:27] [SentencePiece] Sampling at most 10000000 lines from ../data/corpus00.nrm.cs.gz, ../data/corpus00.nrm.en.gz
[2019-01-30 18:28:38] [SentencePiece] Selected 10000000 lines
[2019-01-30 18:32:20] [SentencePiece] Removing ./models/encs/00.baseline/vocab.encs.spm.vocab
[2019-01-30 18:32:20] [SentencePiece] Renaming ./models/encs/00.baseline/vocab.encs.spm.model to ./models/encs/00.baseline/vocab.encs.spm
[2019-01-30 18:32:20] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-01-30 18:32:20] [data] Setting vocabulary size for input 0 to 32000
[2019-01-30 18:32:20] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-01-30 18:32:20] [data] Setting vocabulary size for input 1 to 32000
[2019-01-30 18:32:20] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-01-30 18:32:20] [batching] Collecting statistics for batch fitting with step size 10
[2019-01-30 18:32:21] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-01-30 18:32:22] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-01-30 18:32:22] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-01-30 18:32:23] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-01-30 18:32:23] [comm] Using NCCL 2.3.7 for GPU communication
[2019-01-30 18:32:23] [comm] NCCLCommunicator constructed successfully.
[2019-01-30 18:32:23] [training] Using 4 GPUs
[2019-01-30 18:32:23] [memory] Reserving 230 MB, device gpu0
[2019-01-30 18:32:23] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-01-30 18:32:23] [memory] Reserving 230 MB, device gpu0
[2019-01-30 18:32:44] [batching] Done. Typical MB size is 27396 target words
[2019-01-30 18:32:44] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-01-30 18:32:44] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-01-30 18:32:44] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-01-30 18:32:44] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-01-30 18:32:44] [comm] Using NCCL 2.3.7 for GPU communication
[2019-01-30 18:32:44] [comm] NCCLCommunicator constructed successfully.
[2019-01-30 18:32:44] [training] Using 4 GPUs
[2019-01-30 18:32:44] Training started
[2019-01-30 18:32:44] [data] Shuffling data
[2019-01-30 18:33:41] [data] Done reading 58408180 sentences
[2019-01-30 18:35:39] [data] Done shuffling 58408180 sentences to temp files
[2019-01-30 18:36:28] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-01-30 18:36:28] [memory] Reserving 230 MB, device gpu2
[2019-01-30 18:36:28] [memory] Reserving 230 MB, device gpu1
[2019-01-30 18:36:28] [memory] Reserving 230 MB, device gpu0
[2019-01-30 18:36:28] [memory] Reserving 230 MB, device gpu3
[2019-01-30 18:36:29] [memory] Reserving 230 MB, device gpu2
[2019-01-30 18:36:29] [memory] Reserving 230 MB, device gpu1
[2019-01-30 18:36:29] [memory] Reserving 230 MB, device gpu3
[2019-01-30 18:36:29] [memory] Reserving 230 MB, device gpu0
[2019-01-30 18:36:29] [memory] Reserving 57 MB, device gpu0
[2019-01-30 18:36:29] [memory] Reserving 57 MB, device gpu1
[2019-01-30 18:36:29] [memory] Reserving 57 MB, device gpu2
[2019-01-30 18:36:29] [memory] Reserving 57 MB, device gpu3
[2019-01-30 18:36:29] [memory] Reserving 115 MB, device gpu0
[2019-01-30 18:36:29] [memory] Reserving 115 MB, device gpu2
[2019-01-30 18:36:29] [memory] Reserving 115 MB, device gpu1
[2019-01-30 18:36:29] [memory] Reserving 115 MB, device gpu3
[2019-01-30 18:43:55] Ep. 1 : Up. 1000 : Sen. 1,268,128 : Cost 960.60028076 : Time 694.66s : 28141.49 words/s : L.r. 1.8750e-05
[2019-01-30 18:51:17] Ep. 1 : Up. 2000 : Sen. 2,544,889 : Cost 114.62493896 : Time 442.08s : 43554.14 words/s : L.r. 3.7500e-05
[2019-01-30 18:58:42] Ep. 1 : Up. 3000 : Sen. 3,796,038 : Cost 104.97997284 : Time 444.92s : 43759.03 words/s : L.r. 5.6250e-05
[2019-01-30 19:06:07] Ep. 1 : Up. 4000 : Sen. 5,066,210 : Cost 94.58237457 : Time 445.44s : 43784.94 words/s : L.r. 7.5000e-05
[2019-01-30 19:13:30] Ep. 1 : Up. 5000 : Sen. 6,351,926 : Cost 85.36559296 : Time 442.91s : 43723.52 words/s : L.r. 9.3750e-05
[2019-01-30 19:13:30] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 19:13:31] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 19:13:32] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 19:13:35] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 19:13:36] [valid] Ep. 1 : Up. 5000 : ce-mean-words : 5.59928 : new best
[2019-01-30 19:14:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 19:15:00] [valid] Ep. 1 : Up. 5000 : bleu-detok : 1.86438 : new best
[2019-01-30 19:22:24] Ep. 1 : Up. 6000 : Sen. 7,602,535 : Cost 81.25579834 : Time 534.21s : 36259.06 words/s : L.r. 1.1250e-04
[2019-01-30 19:29:45] Ep. 1 : Up. 7000 : Sen. 8,862,260 : Cost 75.28531647 : Time 441.35s : 43902.17 words/s : L.r. 1.3125e-04
[2019-01-30 19:37:11] Ep. 1 : Up. 8000 : Sen. 10,131,226 : Cost 70.53614807 : Time 445.23s : 43958.87 words/s : L.r. 1.5000e-04
[2019-01-30 19:44:32] Ep. 1 : Up. 9000 : Sen. 11,399,372 : Cost 65.33296967 : Time 441.32s : 43720.69 words/s : L.r. 1.6875e-04
[2019-01-30 19:51:58] Ep. 1 : Up. 10000 : Sen. 12,657,584 : Cost 63.77661133 : Time 445.93s : 43952.28 words/s : L.r. 1.8750e-04
[2019-01-30 19:51:58] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 19:51:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 19:52:00] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 19:52:04] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 19:52:04] [valid] Ep. 1 : Up. 10000 : ce-mean-words : 3.37276 : new best
[2019-01-30 19:52:37] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 19:52:38] [valid] Ep. 1 : Up. 10000 : bleu-detok : 10.6244 : new best
[2019-01-30 20:00:01] Ep. 1 : Up. 11000 : Sen. 13,914,644 : Cost 60.38432693 : Time 482.58s : 39949.80 words/s : L.r. 2.0625e-04
[2019-01-30 20:07:22] Ep. 1 : Up. 12000 : Sen. 15,173,838 : Cost 57.92797470 : Time 441.23s : 43713.94 words/s : L.r. 2.2500e-04
[2019-01-30 20:14:46] Ep. 1 : Up. 13000 : Sen. 16,475,499 : Cost 54.75212479 : Time 443.84s : 43757.66 words/s : L.r. 2.4375e-04
[2019-01-30 20:22:10] Ep. 1 : Up. 14000 : Sen. 17,703,839 : Cost 56.83251190 : Time 444.32s : 43872.23 words/s : L.r. 2.6250e-04
[2019-01-30 20:29:35] Ep. 1 : Up. 15000 : Sen. 18,980,207 : Cost 53.52359772 : Time 444.69s : 43895.43 words/s : L.r. 2.8125e-04
[2019-01-30 20:29:35] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 20:29:36] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 20:29:37] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 20:29:40] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 20:29:41] [valid] Ep. 1 : Up. 15000 : ce-mean-words : 2.5412 : new best
[2019-01-30 20:30:13] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 20:30:14] [valid] Ep. 1 : Up. 15000 : bleu-detok : 17.5929 : new best
[2019-01-30 20:37:38] Ep. 1 : Up. 16000 : Sen. 20,245,548 : Cost 52.98038101 : Time 483.48s : 40228.34 words/s : L.r. 3.0000e-04
[2019-01-30 20:45:02] Ep. 1 : Up. 17000 : Sen. 21,502,290 : Cost 52.34523392 : Time 443.82s : 43786.16 words/s : L.r. 2.9104e-04
[2019-01-30 20:52:29] Ep. 1 : Up. 18000 : Sen. 22,776,013 : Cost 51.04891586 : Time 446.88s : 43826.39 words/s : L.r. 2.8284e-04
[2019-01-30 20:59:49] Ep. 1 : Up. 19000 : Sen. 24,046,570 : Cost 49.68536377 : Time 440.54s : 43673.05 words/s : L.r. 2.7530e-04
[2019-01-30 21:07:16] Ep. 1 : Up. 20000 : Sen. 25,342,211 : Cost 48.77717590 : Time 446.54s : 43769.25 words/s : L.r. 2.6833e-04
[2019-01-30 21:07:16] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 21:07:17] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 21:07:18] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 21:07:21] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 21:07:22] [valid] Ep. 1 : Up. 20000 : ce-mean-words : 2.21229 : new best
[2019-01-30 21:07:55] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 21:07:56] [valid] Ep. 1 : Up. 20000 : bleu-detok : 20.0505 : new best
[2019-01-30 21:15:17] Ep. 1 : Up. 21000 : Sen. 26,578,151 : Cost 49.72614670 : Time 480.82s : 40017.20 words/s : L.r. 2.6186e-04
[2019-01-30 21:22:42] Ep. 1 : Up. 22000 : Sen. 27,832,567 : Cost 49.27510834 : Time 444.85s : 43897.02 words/s : L.r. 2.5584e-04
[2019-01-30 21:30:04] Ep. 1 : Up. 23000 : Sen. 29,112,381 : Cost 47.39990616 : Time 442.75s : 43731.88 words/s : L.r. 2.5022e-04
[2019-01-30 21:37:27] Ep. 1 : Up. 24000 : Sen. 30,368,948 : Cost 48.33938980 : Time 442.80s : 43869.61 words/s : L.r. 2.4495e-04
[2019-01-30 21:44:50] Ep. 1 : Up. 25000 : Sen. 31,635,994 : Cost 47.10427094 : Time 442.84s : 43765.54 words/s : L.r. 2.4000e-04
[2019-01-30 21:44:50] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 21:44:51] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 21:44:52] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 21:44:56] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 21:44:56] [valid] Ep. 1 : Up. 25000 : ce-mean-words : 2.04935 : new best
[2019-01-30 21:45:28] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 21:45:29] [valid] Ep. 1 : Up. 25000 : bleu-detok : 21.7068 : new best
[2019-01-30 21:52:55] Ep. 1 : Up. 26000 : Sen. 32,906,798 : Cost 47.34358215 : Time 485.04s : 40342.05 words/s : L.r. 2.3534e-04
[2019-01-30 22:00:17] Ep. 1 : Up. 27000 : Sen. 34,174,882 : Cost 46.44772339 : Time 442.02s : 43715.19 words/s : L.r. 2.3094e-04
[2019-01-30 22:07:38] Ep. 1 : Up. 28000 : Sen. 35,451,739 : Cost 46.06380844 : Time 441.38s : 43674.09 words/s : L.r. 2.2678e-04
[2019-01-30 22:15:03] Ep. 1 : Up. 29000 : Sen. 36,680,781 : Cost 48.03289413 : Time 445.09s : 43955.11 words/s : L.r. 2.2283e-04
[2019-01-30 22:22:26] Ep. 1 : Up. 30000 : Sen. 37,956,291 : Cost 45.57673264 : Time 442.78s : 43693.75 words/s : L.r. 2.1909e-04
[2019-01-30 22:22:26] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 22:22:27] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 22:22:28] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 22:22:31] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 22:22:32] [valid] Ep. 1 : Up. 30000 : ce-mean-words : 1.95456 : new best
[2019-01-30 22:23:04] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 22:23:05] [valid] Ep. 1 : Up. 30000 : bleu-detok : 22.4759 : new best
[2019-01-30 22:30:28] Ep. 1 : Up. 31000 : Sen. 39,207,925 : Cost 46.33778763 : Time 481.89s : 40187.72 words/s : L.r. 2.1553e-04
[2019-01-30 22:37:53] Ep. 1 : Up. 32000 : Sen. 40,463,935 : Cost 46.31540680 : Time 444.91s : 43762.86 words/s : L.r. 2.1213e-04
[2019-01-30 22:45:18] Ep. 1 : Up. 33000 : Sen. 41,746,861 : Cost 45.31924820 : Time 445.20s : 43924.09 words/s : L.r. 2.0889e-04
[2019-01-30 22:52:41] Ep. 1 : Up. 34000 : Sen. 43,017,639 : Cost 45.14039612 : Time 442.80s : 43722.91 words/s : L.r. 2.0580e-04
[2019-01-30 23:00:04] Ep. 1 : Up. 35000 : Sen. 44,297,339 : Cost 44.89164352 : Time 442.53s : 43830.11 words/s : L.r. 2.0284e-04
[2019-01-30 23:00:04] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 23:00:05] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 23:00:06] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 23:00:09] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 23:00:10] [valid] Ep. 1 : Up. 35000 : ce-mean-words : 1.88984 : new best
[2019-01-30 23:00:41] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 23:00:42] [valid] Ep. 1 : Up. 35000 : bleu-detok : 23.1262 : new best
[2019-01-30 23:08:06] Ep. 1 : Up. 36000 : Sen. 45,573,242 : Cost 44.76400375 : Time 482.69s : 40252.01 words/s : L.r. 2.0000e-04
[2019-01-30 23:15:29] Ep. 1 : Up. 37000 : Sen. 46,809,324 : Cost 46.10036087 : Time 442.91s : 43837.66 words/s : L.r. 1.9728e-04
[2019-01-30 23:22:52] Ep. 1 : Up. 38000 : Sen. 48,067,801 : Cost 45.18626404 : Time 442.70s : 43791.91 words/s : L.r. 1.9467e-04
[2019-01-30 23:30:15] Ep. 1 : Up. 39000 : Sen. 49,344,461 : Cost 44.41716003 : Time 443.62s : 43793.87 words/s : L.r. 1.9215e-04
[2019-01-30 23:37:37] Ep. 1 : Up. 40000 : Sen. 50,586,887 : Cost 45.17347336 : Time 441.05s : 43645.68 words/s : L.r. 1.8974e-04
[2019-01-30 23:37:37] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-30 23:37:38] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-30 23:37:39] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-30 23:37:42] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-30 23:37:42] [valid] Ep. 1 : Up. 40000 : ce-mean-words : 1.84377 : new best
[2019-01-30 23:38:14] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-30 23:38:15] [valid] Ep. 1 : Up. 40000 : bleu-detok : 23.432 : new best
[2019-01-30 23:45:41] Ep. 1 : Up. 41000 : Sen. 51,862,204 : Cost 44.59251022 : Time 484.31s : 40404.41 words/s : L.r. 1.8741e-04
[2019-01-30 23:53:07] Ep. 1 : Up. 42000 : Sen. 53,156,123 : Cost 43.75651169 : Time 446.22s : 43860.11 words/s : L.r. 1.8516e-04
[2019-01-31 00:00:29] Ep. 1 : Up. 43000 : Sen. 54,409,587 : Cost 44.43588638 : Time 442.43s : 43571.32 words/s : L.r. 1.8300e-04
[2019-01-31 00:07:54] Ep. 1 : Up. 44000 : Sen. 55,693,557 : Cost 44.02434540 : Time 445.00s : 43939.63 words/s : L.r. 1.8091e-04
[2019-01-31 00:15:18] Ep. 1 : Up. 45000 : Sen. 56,946,353 : Cost 44.63029861 : Time 443.14s : 43796.63 words/s : L.r. 1.7889e-04
[2019-01-31 00:15:18] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 00:15:19] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 00:15:20] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 00:15:23] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 00:15:24] [valid] Ep. 1 : Up. 45000 : ce-mean-words : 1.80975 : new best
[2019-01-31 00:15:56] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 00:15:57] [valid] Ep. 1 : Up. 45000 : bleu-detok : 23.8025 : new best
[2019-01-31 00:23:16] Ep. 1 : Up. 46000 : Sen. 58,180,758 : Cost 44.31914139 : Time 478.02s : 39748.10 words/s : L.r. 1.7693e-04
[2019-01-31 00:23:54] Seen 58282224 samples
[2019-01-31 00:23:54] Starting epoch 2
[2019-01-31 00:23:54] [data] Shuffling data
[2019-01-31 00:24:52] [data] Done reading 58408180 sentences
[2019-01-31 00:26:49] [data] Done shuffling 58408180 sentences to temp files
[2019-01-31 00:34:18] Ep. 2 : Up. 47000 : Sen. 1,150,152 : Cost 43.42130280 : Time 661.96s : 28661.24 words/s : L.r. 1.7504e-04
[2019-01-31 00:41:42] Ep. 2 : Up. 48000 : Sen. 2,407,677 : Cost 44.23918533 : Time 444.45s : 43782.54 words/s : L.r. 1.7321e-04
[2019-01-31 00:49:07] Ep. 2 : Up. 49000 : Sen. 3,681,085 : Cost 43.79583740 : Time 444.66s : 43933.56 words/s : L.r. 1.7143e-04
[2019-01-31 00:56:30] Ep. 2 : Up. 50000 : Sen. 4,942,629 : Cost 43.76106262 : Time 443.47s : 43683.11 words/s : L.r. 1.6971e-04
[2019-01-31 00:56:30] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 00:56:31] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 00:56:32] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 00:56:36] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 00:56:37] [valid] Ep. 2 : Up. 50000 : ce-mean-words : 1.78361 : new best
[2019-01-31 00:57:08] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 00:57:09] [valid] Ep. 2 : Up. 50000 : bleu-detok : 24.0602 : new best
[2019-01-31 01:04:35] Ep. 2 : Up. 51000 : Sen. 6,221,233 : Cost 43.35967636 : Time 484.37s : 40239.35 words/s : L.r. 1.6803e-04
[2019-01-31 01:11:57] Ep. 2 : Up. 52000 : Sen. 7,485,338 : Cost 43.55918121 : Time 442.64s : 43705.53 words/s : L.r. 1.6641e-04
[2019-01-31 01:19:20] Ep. 2 : Up. 53000 : Sen. 8,727,375 : Cost 44.16017914 : Time 442.39s : 43705.92 words/s : L.r. 1.6483e-04
[2019-01-31 01:26:43] Ep. 2 : Up. 54000 : Sen. 10,000,000 : Cost 43.41888428 : Time 443.35s : 43955.17 words/s : L.r. 1.6330e-04
[2019-01-31 01:34:08] Ep. 2 : Up. 55000 : Sen. 11,255,887 : Cost 44.13540268 : Time 445.33s : 43905.70 words/s : L.r. 1.6181e-04
[2019-01-31 01:34:08] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 01:34:09] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 01:34:11] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 01:34:14] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 01:34:15] [valid] Ep. 2 : Up. 55000 : ce-mean-words : 1.76221 : new best
[2019-01-31 01:34:47] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 01:34:47] [valid] Ep. 2 : Up. 55000 : bleu-detok : 24.1947 : new best
[2019-01-31 01:42:10] Ep. 2 : Up. 56000 : Sen. 12,505,517 : Cost 43.68479538 : Time 481.66s : 40060.19 words/s : L.r. 1.6036e-04
[2019-01-31 01:49:34] Ep. 2 : Up. 57000 : Sen. 13,809,961 : Cost 42.02773666 : Time 444.23s : 43793.32 words/s : L.r. 1.5894e-04
[2019-01-31 01:56:58] Ep. 2 : Up. 58000 : Sen. 15,063,999 : Cost 43.81348801 : Time 444.09s : 43786.42 words/s : L.r. 1.5757e-04
[2019-01-31 02:04:22] Ep. 2 : Up. 59000 : Sen. 16,328,790 : Cost 43.37924957 : Time 443.91s : 43844.36 words/s : L.r. 1.5623e-04
[2019-01-31 02:11:47] Ep. 2 : Up. 60000 : Sen. 17,585,415 : Cost 43.94829559 : Time 444.56s : 44021.96 words/s : L.r. 1.5492e-04
[2019-01-31 02:11:47] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 02:11:48] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 02:11:49] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 02:11:52] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 02:11:53] [valid] Ep. 2 : Up. 60000 : ce-mean-words : 1.74315 : new best
[2019-01-31 02:12:25] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 02:12:26] [valid] Ep. 2 : Up. 60000 : bleu-detok : 24.3165 : new best
[2019-01-31 02:19:48] Ep. 2 : Up. 61000 : Sen. 18,877,053 : Cost 41.78416061 : Time 481.30s : 39945.99 words/s : L.r. 1.5364e-04
[2019-01-31 02:27:10] Ep. 2 : Up. 62000 : Sen. 20,124,957 : Cost 43.49015045 : Time 442.04s : 43798.17 words/s : L.r. 1.5240e-04
[2019-01-31 02:34:35] Ep. 2 : Up. 63000 : Sen. 21,373,701 : Cost 44.09489059 : Time 445.01s : 43979.58 words/s : L.r. 1.5119e-04
[2019-01-31 02:41:57] Ep. 2 : Up. 64000 : Sen. 22,648,933 : Cost 42.19246292 : Time 441.55s : 43663.68 words/s : L.r. 1.5000e-04
[2019-01-31 02:49:20] Ep. 2 : Up. 65000 : Sen. 23,919,587 : Cost 43.04640961 : Time 443.09s : 43922.59 words/s : L.r. 1.4884e-04
[2019-01-31 02:49:20] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 02:49:21] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 02:49:22] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 02:49:25] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 02:49:26] [valid] Ep. 2 : Up. 65000 : ce-mean-words : 1.72774 : new best
[2019-01-31 02:49:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 02:50:00] [valid] Ep. 2 : Up. 65000 : bleu-detok : 24.5238 : new best
[2019-01-31 02:57:22] Ep. 2 : Up. 66000 : Sen. 25,182,854 : Cost 42.79197693 : Time 482.67s : 40041.92 words/s : L.r. 1.4771e-04
[2019-01-31 03:04:47] Ep. 2 : Up. 67000 : Sen. 26,431,481 : Cost 43.71972275 : Time 444.17s : 43987.52 words/s : L.r. 1.4660e-04
[2019-01-31 03:12:09] Ep. 2 : Up. 68000 : Sen. 27,707,526 : Cost 42.39660645 : Time 442.10s : 43724.67 words/s : L.r. 1.4552e-04
[2019-01-31 03:19:32] Ep. 2 : Up. 69000 : Sen. 28,978,328 : Cost 42.63807297 : Time 443.43s : 43740.13 words/s : L.r. 1.4446e-04
[2019-01-31 03:26:55] Ep. 2 : Up. 70000 : Sen. 30,218,347 : Cost 43.59060287 : Time 442.70s : 43813.68 words/s : L.r. 1.4343e-04
[2019-01-31 03:26:55] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 03:26:56] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 03:26:57] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 03:27:00] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 03:27:01] [valid] Ep. 2 : Up. 70000 : ce-mean-words : 1.71475 : new best
[2019-01-31 03:27:33] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 03:27:34] [valid] Ep. 2 : Up. 70000 : bleu-detok : 24.6025 : new best
[2019-01-31 03:34:57] Ep. 2 : Up. 71000 : Sen. 31,515,868 : Cost 41.53672409 : Time 482.44s : 40117.53 words/s : L.r. 1.4241e-04
[2019-01-31 03:42:23] Ep. 2 : Up. 72000 : Sen. 32,779,107 : Cost 43.29471970 : Time 445.37s : 43996.95 words/s : L.r. 1.4142e-04
[2019-01-31 03:49:45] Ep. 2 : Up. 73000 : Sen. 34,033,375 : Cost 42.77255249 : Time 442.43s : 43675.29 words/s : L.r. 1.4045e-04
[2019-01-31 03:57:07] Ep. 2 : Up. 74000 : Sen. 35,322,612 : Cost 41.82691574 : Time 442.27s : 43782.99 words/s : L.r. 1.3950e-04
[2019-01-31 04:04:31] Ep. 2 : Up. 75000 : Sen. 36,567,824 : Cost 43.31494141 : Time 443.51s : 43902.83 words/s : L.r. 1.3856e-04
[2019-01-31 04:04:31] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 04:04:32] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 04:04:33] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 04:04:36] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 04:04:37] [valid] Ep. 2 : Up. 75000 : ce-mean-words : 1.70275 : new best
[2019-01-31 04:05:09] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 04:05:10] [valid] Ep. 2 : Up. 75000 : bleu-detok : 24.8231 : new best
[2019-01-31 04:12:33] Ep. 2 : Up. 76000 : Sen. 37,837,811 : Cost 42.27544785 : Time 482.24s : 40084.70 words/s : L.r. 1.3765e-04
[2019-01-31 04:19:56] Ep. 2 : Up. 77000 : Sen. 39,097,729 : Cost 42.74981308 : Time 443.06s : 43741.33 words/s : L.r. 1.3675e-04
[2019-01-31 04:27:19] Ep. 2 : Up. 78000 : Sen. 40,379,935 : Cost 41.79759979 : Time 442.74s : 43769.75 words/s : L.r. 1.3587e-04
[2019-01-31 04:34:42] Ep. 2 : Up. 79000 : Sen. 41,623,241 : Cost 43.35747147 : Time 443.45s : 43811.69 words/s : L.r. 1.3501e-04
[2019-01-31 04:42:03] Ep. 2 : Up. 80000 : Sen. 42,878,594 : Cost 42.47091675 : Time 440.37s : 43757.61 words/s : L.r. 1.3416e-04
[2019-01-31 04:42:03] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 04:42:04] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 04:42:05] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 04:42:08] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 04:42:09] [valid] Ep. 2 : Up. 80000 : ce-mean-words : 1.69243 : new best
[2019-01-31 04:42:41] [valid] Ep. 2 : Up. 80000 : bleu-detok : 24.7781 : stalled 1 times (last best: 24.8231)
[2019-01-31 04:50:06] Ep. 2 : Up. 81000 : Sen. 44,151,405 : Cost 42.38847351 : Time 483.59s : 40354.46 words/s : L.r. 1.3333e-04
[2019-01-31 04:57:33] Ep. 2 : Up. 82000 : Sen. 45,420,306 : Cost 42.70063019 : Time 446.49s : 43941.89 words/s : L.r. 1.3252e-04
[2019-01-31 05:04:55] Ep. 2 : Up. 83000 : Sen. 46,671,350 : Cost 42.69536591 : Time 442.31s : 43741.26 words/s : L.r. 1.3172e-04
[2019-01-31 05:12:16] Ep. 2 : Up. 84000 : Sen. 47,932,101 : Cost 42.13240814 : Time 440.50s : 43651.89 words/s : L.r. 1.3093e-04
[2019-01-31 05:19:41] Ep. 2 : Up. 85000 : Sen. 49,204,880 : Cost 42.25270844 : Time 445.05s : 43852.60 words/s : L.r. 1.3016e-04
[2019-01-31 05:19:41] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 05:19:42] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 05:19:43] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 05:19:46] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 05:19:47] [valid] Ep. 2 : Up. 85000 : ce-mean-words : 1.68415 : new best
[2019-01-31 05:20:19] [valid] Ep. 2 : Up. 85000 : bleu-detok : 24.8103 : stalled 2 times (last best: 24.8231)
[2019-01-31 05:27:43] Ep. 2 : Up. 86000 : Sen. 50,472,710 : Cost 42.41961288 : Time 482.36s : 40372.64 words/s : L.r. 1.2940e-04
[2019-01-31 05:35:06] Ep. 2 : Up. 87000 : Sen. 51,733,771 : Cost 42.40067291 : Time 443.35s : 43739.85 words/s : L.r. 1.2865e-04
[2019-01-31 05:42:31] Ep. 2 : Up. 88000 : Sen. 53,005,264 : Cost 42.15352631 : Time 444.99s : 43807.96 words/s : L.r. 1.2792e-04
[2019-01-31 05:49:54] Ep. 2 : Up. 89000 : Sen. 54,285,092 : Cost 41.57426834 : Time 442.94s : 43685.68 words/s : L.r. 1.2720e-04
[2019-01-31 05:57:20] Ep. 2 : Up. 90000 : Sen. 55,549,586 : Cost 42.54662704 : Time 446.04s : 43876.55 words/s : L.r. 1.2649e-04
[2019-01-31 05:57:20] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 05:57:21] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 05:57:23] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 05:57:26] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 05:57:26] [valid] Ep. 2 : Up. 90000 : ce-mean-words : 1.67606 : new best
[2019-01-31 05:57:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 05:58:00] [valid] Ep. 2 : Up. 90000 : bleu-detok : 24.9459 : new best
[2019-01-31 06:05:23] Ep. 2 : Up. 91000 : Sen. 56,798,514 : Cost 42.81097031 : Time 482.61s : 40244.35 words/s : L.r. 1.2579e-04
[2019-01-31 06:12:44] Ep. 2 : Up. 92000 : Sen. 58,070,063 : Cost 41.53666306 : Time 441.35s : 43572.84 words/s : L.r. 1.2511e-04
[2019-01-31 06:14:08] Seen 58282224 samples
[2019-01-31 06:14:08] Starting epoch 3
[2019-01-31 06:14:08] [data] Shuffling data
[2019-01-31 06:15:06] [data] Done reading 58408180 sentences
[2019-01-31 06:17:02] [data] Done shuffling 58408180 sentences to temp files
[2019-01-31 06:23:44] Ep. 3 : Up. 93000 : Sen. 1,009,744 : Cost 42.34087753 : Time 660.23s : 28604.00 words/s : L.r. 1.2443e-04
[2019-01-31 06:31:07] Ep. 3 : Up. 94000 : Sen. 2,262,332 : Cost 42.07936478 : Time 442.80s : 43672.74 words/s : L.r. 1.2377e-04
[2019-01-31 06:38:32] Ep. 3 : Up. 95000 : Sen. 3,527,763 : Cost 42.26990509 : Time 444.79s : 43904.00 words/s : L.r. 1.2312e-04
[2019-01-31 06:38:32] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 06:38:33] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 06:38:35] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 06:38:38] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 06:38:39] [valid] Ep. 3 : Up. 95000 : ce-mean-words : 1.66835 : new best
[2019-01-31 06:39:11] [valid] Ep. 3 : Up. 95000 : bleu-detok : 24.9439 : stalled 1 times (last best: 24.9459)
[2019-01-31 06:46:35] Ep. 3 : Up. 96000 : Sen. 4,796,871 : Cost 41.92282867 : Time 483.30s : 40254.93 words/s : L.r. 1.2247e-04
[2019-01-31 06:53:58] Ep. 3 : Up. 97000 : Sen. 6,068,533 : Cost 41.85089493 : Time 442.88s : 43839.22 words/s : L.r. 1.2184e-04
[2019-01-31 07:01:23] Ep. 3 : Up. 98000 : Sen. 7,331,589 : Cost 42.06764984 : Time 445.07s : 43914.34 words/s : L.r. 1.2122e-04
[2019-01-31 07:08:47] Ep. 3 : Up. 99000 : Sen. 8,620,142 : Cost 41.17943954 : Time 443.97s : 43798.94 words/s : L.r. 1.2060e-04
[2019-01-31 07:16:09] Ep. 3 : Up. 100000 : Sen. 9,868,004 : Cost 42.52096939 : Time 441.93s : 43760.02 words/s : L.r. 1.2000e-04
[2019-01-31 07:16:09] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 07:16:10] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 07:16:12] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 07:16:15] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 07:16:16] [valid] Ep. 3 : Up. 100000 : ce-mean-words : 1.66173 : new best
[2019-01-31 07:16:48] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 07:16:49] [valid] Ep. 3 : Up. 100000 : bleu-detok : 25.027 : new best
[2019-01-31 07:24:13] Ep. 3 : Up. 101000 : Sen. 11,145,074 : Cost 41.62890625 : Time 484.14s : 40215.58 words/s : L.r. 1.1940e-04
[2019-01-31 07:31:37] Ep. 3 : Up. 102000 : Sen. 12,391,252 : Cost 42.46628189 : Time 443.71s : 43806.62 words/s : L.r. 1.1882e-04
[2019-01-31 07:38:59] Ep. 3 : Up. 103000 : Sen. 13,652,899 : Cost 41.89707947 : Time 441.69s : 43719.63 words/s : L.r. 1.1824e-04
[2019-01-31 07:46:23] Ep. 3 : Up. 104000 : Sen. 14,925,904 : Cost 41.71971512 : Time 444.58s : 43861.42 words/s : L.r. 1.1767e-04
[2019-01-31 07:53:47] Ep. 3 : Up. 105000 : Sen. 16,178,026 : Cost 42.32958603 : Time 443.42s : 43834.86 words/s : L.r. 1.1711e-04
[2019-01-31 07:53:47] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 07:53:48] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 07:53:49] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 07:53:53] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 07:53:54] [valid] Ep. 3 : Up. 105000 : ce-mean-words : 1.65559 : new best
[2019-01-31 07:54:26] [valid] Ep. 3 : Up. 105000 : bleu-detok : 24.9823 : stalled 1 times (last best: 25.027)
[2019-01-31 08:01:50] Ep. 3 : Up. 106000 : Sen. 17,489,453 : Cost 40.25431824 : Time 483.29s : 40174.67 words/s : L.r. 1.1655e-04
[2019-01-31 08:09:15] Ep. 3 : Up. 107000 : Sen. 18,730,168 : Cost 42.84260178 : Time 444.45s : 43851.13 words/s : L.r. 1.1601e-04
[2019-01-31 08:16:39] Ep. 3 : Up. 108000 : Sen. 20,010,260 : Cost 41.43853378 : Time 444.67s : 43762.44 words/s : L.r. 1.1547e-04
[2019-01-31 08:24:04] Ep. 3 : Up. 109000 : Sen. 21,271,640 : Cost 42.17891693 : Time 445.05s : 43961.76 words/s : L.r. 1.1494e-04
[2019-01-31 08:31:25] Ep. 3 : Up. 110000 : Sen. 22,537,571 : Cost 41.39791870 : Time 440.29s : 43640.41 words/s : L.r. 1.1442e-04
[2019-01-31 08:31:25] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 08:31:26] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 08:31:27] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 08:31:30] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 08:31:31] [valid] Ep. 3 : Up. 110000 : ce-mean-words : 1.64997 : new best
[2019-01-31 08:32:04] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 08:32:04] [valid] Ep. 3 : Up. 110000 : bleu-detok : 25.0818 : new best
[2019-01-31 08:39:30] Ep. 3 : Up. 111000 : Sen. 23,816,584 : Cost 41.52787018 : Time 485.26s : 40301.83 words/s : L.r. 1.1390e-04
[2019-01-31 08:46:54] Ep. 3 : Up. 112000 : Sen. 25,080,559 : Cost 41.91761398 : Time 444.22s : 43840.57 words/s : L.r. 1.1339e-04
[2019-01-31 08:54:14] Ep. 3 : Up. 113000 : Sen. 26,337,267 : Cost 41.50818253 : Time 440.24s : 43542.45 words/s : L.r. 1.1289e-04
[2019-01-31 09:01:39] Ep. 3 : Up. 114000 : Sen. 27,603,703 : Cost 41.92311478 : Time 444.97s : 43979.63 words/s : L.r. 1.1239e-04
[2019-01-31 09:09:00] Ep. 3 : Up. 115000 : Sen. 28,832,929 : Cost 42.63568115 : Time 440.93s : 43658.07 words/s : L.r. 1.1190e-04
[2019-01-31 09:09:00] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 09:09:02] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 09:09:03] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 09:09:06] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 09:09:07] [valid] Ep. 3 : Up. 115000 : ce-mean-words : 1.64484 : new best
[2019-01-31 09:09:40] [valid] Ep. 3 : Up. 115000 : bleu-detok : 25.0812 : stalled 1 times (last best: 25.0818)
[2019-01-31 09:17:03] Ep. 3 : Up. 116000 : Sen. 30,126,422 : Cost 40.64176941 : Time 483.21s : 40177.21 words/s : L.r. 1.1142e-04
[2019-01-31 09:24:27] Ep. 3 : Up. 117000 : Sen. 31,388,683 : Cost 41.85239792 : Time 443.46s : 43870.30 words/s : L.r. 1.1094e-04
[2019-01-31 09:31:49] Ep. 3 : Up. 118000 : Sen. 32,649,891 : Cost 41.69807816 : Time 442.32s : 43704.90 words/s : L.r. 1.1047e-04
[2019-01-31 09:39:14] Ep. 3 : Up. 119000 : Sen. 33,915,551 : Cost 41.74032974 : Time 444.59s : 43863.52 words/s : L.r. 1.1000e-04
[2019-01-31 09:46:36] Ep. 3 : Up. 120000 : Sen. 35,186,678 : Cost 41.35739517 : Time 442.56s : 43736.85 words/s : L.r. 1.0954e-04
[2019-01-31 09:46:36] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 09:46:37] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 09:46:39] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 09:46:42] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 09:46:43] [valid] Ep. 3 : Up. 120000 : ce-mean-words : 1.63961 : new best
[2019-01-31 09:47:16] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 09:47:17] [valid] Ep. 3 : Up. 120000 : bleu-detok : 25.3185 : new best
[2019-01-31 09:54:42] Ep. 3 : Up. 121000 : Sen. 36,440,862 : Cost 42.24686813 : Time 485.62s : 40241.56 words/s : L.r. 1.0909e-04
[2019-01-31 10:02:06] Ep. 3 : Up. 122000 : Sen. 37,724,973 : Cost 40.92412567 : Time 444.00s : 43756.23 words/s : L.r. 1.0864e-04
[2019-01-31 10:09:30] Ep. 3 : Up. 123000 : Sen. 38,984,221 : Cost 41.81359100 : Time 443.71s : 43833.14 words/s : L.r. 1.0820e-04
[2019-01-31 10:16:54] Ep. 3 : Up. 124000 : Sen. 40,241,350 : Cost 42.11847687 : Time 444.50s : 43884.07 words/s : L.r. 1.0776e-04
[2019-01-31 10:24:18] Ep. 3 : Up. 125000 : Sen. 41,503,633 : Cost 41.86005783 : Time 444.21s : 43921.40 words/s : L.r. 1.0733e-04
[2019-01-31 10:24:18] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 10:24:20] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 10:24:21] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 10:24:24] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 10:24:25] [valid] Ep. 3 : Up. 125000 : ce-mean-words : 1.63477 : new best
[2019-01-31 10:24:57] [valid] Ep. 3 : Up. 125000 : bleu-detok : 25.2241 : stalled 1 times (last best: 25.3185)
[2019-01-31 10:32:17] Ep. 3 : Up. 126000 : Sen. 42,785,236 : Cost 40.43108368 : Time 478.92s : 40027.85 words/s : L.r. 1.0690e-04
[2019-01-31 10:39:43] Ep. 3 : Up. 127000 : Sen. 44,039,089 : Cost 42.15284729 : Time 445.22s : 43902.90 words/s : L.r. 1.0648e-04
[2019-01-31 10:47:05] Ep. 3 : Up. 128000 : Sen. 45,313,747 : Cost 41.05812836 : Time 442.69s : 43660.23 words/s : L.r. 1.0607e-04
[2019-01-31 10:54:30] Ep. 3 : Up. 129000 : Sen. 46,566,080 : Cost 42.11420059 : Time 444.50s : 43868.39 words/s : L.r. 1.0565e-04
[2019-01-31 11:01:53] Ep. 3 : Up. 130000 : Sen. 47,843,922 : Cost 41.07408524 : Time 443.68s : 43762.14 words/s : L.r. 1.0525e-04
[2019-01-31 11:01:53] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 11:01:55] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 11:01:56] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 11:01:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 11:02:00] [valid] Ep. 3 : Up. 130000 : ce-mean-words : 1.6309 : new best
[2019-01-31 11:02:33] [valid] Ep. 3 : Up. 130000 : bleu-detok : 25.3033 : stalled 2 times (last best: 25.3185)
[2019-01-31 11:09:57] Ep. 3 : Up. 131000 : Sen. 49,110,497 : Cost 41.44830704 : Time 483.18s : 40197.32 words/s : L.r. 1.0484e-04
[2019-01-31 11:17:19] Ep. 3 : Up. 132000 : Sen. 50,374,181 : Cost 41.33826065 : Time 442.36s : 43653.91 words/s : L.r. 1.0445e-04
[2019-01-31 11:24:44] Ep. 3 : Up. 133000 : Sen. 51,650,787 : Cost 41.45672226 : Time 445.32s : 44022.25 words/s : L.r. 1.0405e-04
[2019-01-31 11:32:06] Ep. 3 : Up. 134000 : Sen. 52,897,280 : Cost 41.74922943 : Time 441.47s : 43653.16 words/s : L.r. 1.0366e-04
[2019-01-31 11:39:33] Ep. 3 : Up. 135000 : Sen. 54,167,111 : Cost 41.76358414 : Time 447.22s : 43903.13 words/s : L.r. 1.0328e-04
[2019-01-31 11:39:33] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 11:39:34] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 11:39:35] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 11:39:38] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 11:39:39] [valid] Ep. 3 : Up. 135000 : ce-mean-words : 1.62755 : new best
[2019-01-31 11:40:12] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 11:40:13] [valid] Ep. 3 : Up. 135000 : bleu-detok : 25.3266 : new best
[2019-01-31 11:47:37] Ep. 3 : Up. 136000 : Sen. 55,424,438 : Cost 41.50294876 : Time 483.77s : 40012.19 words/s : L.r. 1.0290e-04
[2019-01-31 11:55:00] Ep. 3 : Up. 137000 : Sen. 56,712,855 : Cost 40.72121811 : Time 442.98s : 43825.04 words/s : L.r. 1.0252e-04
[2019-01-31 12:02:22] Ep. 3 : Up. 138000 : Sen. 57,965,815 : Cost 41.76068497 : Time 442.54s : 43830.69 words/s : L.r. 1.0215e-04
[2019-01-31 12:04:22] Seen 58282224 samples
[2019-01-31 12:04:22] Starting epoch 4
[2019-01-31 12:04:22] [data] Shuffling data
[2019-01-31 12:05:19] [data] Done reading 58408180 sentences
[2019-01-31 12:07:15] [data] Done shuffling 58408180 sentences to temp files
[2019-01-31 12:13:19] Ep. 4 : Up. 139000 : Sen. 898,035 : Cost 41.16284561 : Time 656.92s : 28317.29 words/s : L.r. 1.0178e-04
[2019-01-31 12:20:44] Ep. 4 : Up. 140000 : Sen. 2,161,813 : Cost 41.53961945 : Time 444.55s : 43904.58 words/s : L.r. 1.0142e-04
[2019-01-31 12:20:44] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 12:20:45] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 12:20:46] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 12:20:49] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 12:20:50] [valid] Ep. 4 : Up. 140000 : ce-mean-words : 1.62434 : new best
[2019-01-31 12:21:24] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 12:21:24] [valid] Ep. 4 : Up. 140000 : bleu-detok : 25.4108 : new best
[2019-01-31 12:28:46] Ep. 4 : Up. 141000 : Sen. 3,413,314 : Cost 41.28986740 : Time 481.78s : 39902.72 words/s : L.r. 1.0106e-04
[2019-01-31 12:36:13] Ep. 4 : Up. 142000 : Sen. 4,674,080 : Cost 41.88105392 : Time 447.26s : 43996.35 words/s : L.r. 1.0070e-04
[2019-01-31 12:43:36] Ep. 4 : Up. 143000 : Sen. 5,962,624 : Cost 40.54099655 : Time 443.66s : 43775.09 words/s : L.r. 1.0035e-04
[2019-01-31 12:50:58] Ep. 4 : Up. 144000 : Sen. 7,217,195 : Cost 41.51598740 : Time 441.99s : 43800.36 words/s : L.r. 1.0000e-04
[2019-01-31 12:58:23] Ep. 4 : Up. 145000 : Sen. 8,494,085 : Cost 40.94236374 : Time 444.56s : 43830.71 words/s : L.r. 9.9655e-05
[2019-01-31 12:58:23] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 12:58:24] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 12:58:25] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 12:58:29] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 12:58:29] [valid] Ep. 4 : Up. 145000 : ce-mean-words : 1.62174 : new best
[2019-01-31 12:59:03] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 12:59:04] [valid] Ep. 4 : Up. 145000 : bleu-detok : 25.4821 : new best
[2019-01-31 13:06:26] Ep. 4 : Up. 146000 : Sen. 9,761,842 : Cost 41.07623672 : Time 482.82s : 40045.61 words/s : L.r. 9.9313e-05
[2019-01-31 13:13:50] Ep. 4 : Up. 147000 : Sen. 11,024,895 : Cost 41.34151077 : Time 444.56s : 43827.40 words/s : L.r. 9.8974e-05
[2019-01-31 13:21:14] Ep. 4 : Up. 148000 : Sen. 12,309,557 : Cost 40.44845963 : Time 443.68s : 43659.64 words/s : L.r. 9.8639e-05
[2019-01-31 13:28:37] Ep. 4 : Up. 149000 : Sen. 13,552,967 : Cost 41.99188614 : Time 443.09s : 43863.61 words/s : L.r. 9.8308e-05
[2019-01-31 13:36:01] Ep. 4 : Up. 150000 : Sen. 14,824,993 : Cost 41.07794189 : Time 444.10s : 43785.74 words/s : L.r. 9.7980e-05
[2019-01-31 13:36:01] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 13:36:02] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 13:36:03] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 13:36:07] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 13:36:07] [valid] Ep. 4 : Up. 150000 : ce-mean-words : 1.61902 : new best
[2019-01-31 13:36:40] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 13:36:41] [valid] Ep. 4 : Up. 150000 : bleu-detok : 25.5064 : new best
[2019-01-31 13:44:04] Ep. 4 : Up. 151000 : Sen. 16,080,928 : Cost 41.35258484 : Time 482.92s : 40141.37 words/s : L.r. 9.7655e-05
[2019-01-31 13:51:26] Ep. 4 : Up. 152000 : Sen. 17,348,768 : Cost 40.95818710 : Time 441.58s : 43801.06 words/s : L.r. 9.7333e-05
[2019-01-31 13:58:50] Ep. 4 : Up. 153000 : Sen. 18,581,132 : Cost 42.24246597 : Time 443.84s : 43669.79 words/s : L.r. 9.7014e-05
[2019-01-31 14:06:13] Ep. 4 : Up. 154000 : Sen. 19,871,825 : Cost 40.39072418 : Time 443.82s : 43802.66 words/s : L.r. 9.6699e-05
[2019-01-31 14:13:36] Ep. 4 : Up. 155000 : Sen. 21,151,117 : Cost 40.65085602 : Time 442.40s : 43824.73 words/s : L.r. 9.6386e-05
[2019-01-31 14:13:36] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 14:13:37] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 14:13:38] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 14:13:41] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 14:13:42] [valid] Ep. 4 : Up. 155000 : ce-mean-words : 1.61644 : new best
[2019-01-31 14:14:15] [valid] Ep. 4 : Up. 155000 : bleu-detok : 25.3898 : stalled 1 times (last best: 25.5064)
[2019-01-31 14:21:40] Ep. 4 : Up. 156000 : Sen. 22,402,012 : Cost 41.77671051 : Time 483.83s : 40334.60 words/s : L.r. 9.6077e-05
[2019-01-31 14:29:00] Ep. 4 : Up. 157000 : Sen. 23,675,686 : Cost 40.68052292 : Time 440.49s : 43768.90 words/s : L.r. 9.5770e-05
[2019-01-31 14:36:26] Ep. 4 : Up. 158000 : Sen. 24,935,018 : Cost 41.48543167 : Time 445.38s : 43797.77 words/s : L.r. 9.5467e-05
[2019-01-31 14:43:50] Ep. 4 : Up. 159000 : Sen. 26,200,283 : Cost 41.48925400 : Time 444.61s : 43934.82 words/s : L.r. 9.5166e-05
[2019-01-31 14:51:14] Ep. 4 : Up. 160000 : Sen. 27,457,269 : Cost 41.23484802 : Time 443.72s : 43719.20 words/s : L.r. 9.4868e-05
[2019-01-31 14:51:14] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 14:51:15] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 14:51:16] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 14:51:19] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 14:51:20] [valid] Ep. 4 : Up. 160000 : ce-mean-words : 1.6139 : new best
[2019-01-31 14:51:53] [valid] Ep. 4 : Up. 160000 : bleu-detok : 25.4501 : stalled 2 times (last best: 25.5064)
[2019-01-31 14:59:16] Ep. 4 : Up. 161000 : Sen. 28,745,229 : Cost 40.27197266 : Time 482.39s : 40099.52 words/s : L.r. 9.4573e-05
[2019-01-31 15:06:42] Ep. 4 : Up. 162000 : Sen. 30,004,255 : Cost 41.52428818 : Time 445.27s : 43857.61 words/s : L.r. 9.4281e-05
[2019-01-31 15:14:05] Ep. 4 : Up. 163000 : Sen. 31,259,062 : Cost 41.42473602 : Time 443.01s : 43786.42 words/s : L.r. 9.3991e-05
[2019-01-31 15:21:29] Ep. 4 : Up. 164000 : Sen. 32,530,014 : Cost 41.18325043 : Time 444.27s : 43891.01 words/s : L.r. 9.3704e-05
[2019-01-31 15:28:52] Ep. 4 : Up. 165000 : Sen. 33,797,949 : Cost 40.85828400 : Time 442.96s : 43734.29 words/s : L.r. 9.3420e-05
[2019-01-31 15:28:52] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 15:28:53] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 15:28:54] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 15:28:57] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 15:28:58] [valid] Ep. 4 : Up. 165000 : ce-mean-words : 1.61143 : new best
[2019-01-31 15:29:30] [valid] Ep. 4 : Up. 165000 : bleu-detok : 25.506 : stalled 3 times (last best: 25.5064)
[2019-01-31 15:36:55] Ep. 4 : Up. 166000 : Sen. 35,049,819 : Cost 41.66622162 : Time 483.49s : 40324.63 words/s : L.r. 9.3138e-05
[2019-01-31 15:44:18] Ep. 4 : Up. 167000 : Sen. 36,339,460 : Cost 40.18462753 : Time 442.67s : 43678.30 words/s : L.r. 9.2859e-05
[2019-01-31 15:51:42] Ep. 4 : Up. 168000 : Sen. 37,584,069 : Cost 41.86536407 : Time 444.47s : 43865.13 words/s : L.r. 9.2582e-05
[2019-01-31 15:59:04] Ep. 4 : Up. 169000 : Sen. 38,852,790 : Cost 40.70286942 : Time 441.31s : 43702.13 words/s : L.r. 9.2308e-05
[2019-01-31 16:06:27] Ep. 4 : Up. 170000 : Sen. 40,115,429 : Cost 41.18161011 : Time 443.20s : 43839.81 words/s : L.r. 9.2036e-05
[2019-01-31 16:06:27] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 16:06:28] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 16:06:29] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 16:06:33] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 16:06:33] [valid] Ep. 4 : Up. 170000 : ce-mean-words : 1.60904 : new best
[2019-01-31 16:07:06] [valid] Ep. 4 : Up. 170000 : bleu-detok : 25.4063 : stalled 4 times (last best: 25.5064)
[2019-01-31 16:14:31] Ep. 4 : Up. 171000 : Sen. 41,388,161 : Cost 40.83348465 : Time 483.85s : 40229.71 words/s : L.r. 9.1766e-05
[2019-01-31 16:21:54] Ep. 4 : Up. 172000 : Sen. 42,666,555 : Cost 40.89700317 : Time 443.73s : 44018.16 words/s : L.r. 9.1499e-05
[2019-01-31 16:29:17] Ep. 4 : Up. 173000 : Sen. 43,918,704 : Cost 41.32261276 : Time 442.98s : 43681.82 words/s : L.r. 9.1234e-05
[2019-01-31 16:36:41] Ep. 4 : Up. 174000 : Sen. 45,212,339 : Cost 39.90036392 : Time 443.43s : 43671.75 words/s : L.r. 9.0972e-05
[2019-01-31 16:44:07] Ep. 4 : Up. 175000 : Sen. 46,475,783 : Cost 41.76445389 : Time 445.94s : 44202.22 words/s : L.r. 9.0711e-05
[2019-01-31 16:44:07] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 16:44:08] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 16:44:09] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 16:44:12] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 16:44:13] [valid] Ep. 4 : Up. 175000 : ce-mean-words : 1.60705 : new best
[2019-01-31 16:44:45] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 16:44:47] [valid] Ep. 4 : Up. 175000 : bleu-detok : 25.5066 : new best
[2019-01-31 16:52:09] Ep. 4 : Up. 176000 : Sen. 47,711,433 : Cost 41.48671722 : Time 481.70s : 39812.97 words/s : L.r. 9.0453e-05
[2019-01-31 16:59:32] Ep. 4 : Up. 177000 : Sen. 48,985,722 : Cost 40.85964966 : Time 443.96s : 43853.81 words/s : L.r. 9.0198e-05
[2019-01-31 17:06:56] Ep. 4 : Up. 178000 : Sen. 50,269,925 : Cost 40.55322266 : Time 443.79s : 43867.34 words/s : L.r. 8.9944e-05
[2019-01-31 17:14:20] Ep. 4 : Up. 179000 : Sen. 51,542,454 : Cost 40.60504532 : Time 443.70s : 43673.26 words/s : L.r. 8.9692e-05
[2019-01-31 17:21:43] Ep. 4 : Up. 180000 : Sen. 52,780,911 : Cost 42.01880646 : Time 442.82s : 43932.90 words/s : L.r. 8.9443e-05
[2019-01-31 17:21:43] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 17:21:44] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 17:21:45] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 17:21:48] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 17:21:49] [valid] Ep. 4 : Up. 180000 : ce-mean-words : 1.60582 : new best
[2019-01-31 17:22:22] [valid] Ep. 4 : Up. 180000 : bleu-detok : 25.4987 : stalled 1 times (last best: 25.5066)
[2019-01-31 17:29:46] Ep. 4 : Up. 181000 : Sen. 54,048,438 : Cost 40.78028107 : Time 482.75s : 40185.43 words/s : L.r. 8.9195e-05
[2019-01-31 17:37:10] Ep. 4 : Up. 182000 : Sen. 55,330,072 : Cost 40.56765747 : Time 444.06s : 43777.86 words/s : L.r. 8.8950e-05
[2019-01-31 17:44:30] Ep. 4 : Up. 183000 : Sen. 56,589,323 : Cost 40.79146957 : Time 440.61s : 43729.63 words/s : L.r. 8.8707e-05
[2019-01-31 17:51:56] Ep. 4 : Up. 184000 : Sen. 57,854,409 : Cost 41.23221207 : Time 445.70s : 43885.82 words/s : L.r. 8.8465e-05
[2019-01-31 17:54:37] Seen 58282224 samples
[2019-01-31 17:54:37] Starting epoch 5
[2019-01-31 17:54:37] [data] Shuffling data
[2019-01-31 17:55:34] [data] Done reading 58408180 sentences
[2019-01-31 17:57:31] [data] Done shuffling 58408180 sentences to temp files
[2019-01-31 18:02:54] Ep. 5 : Up. 185000 : Sen. 797,667 : Cost 40.72083282 : Time 657.94s : 28452.06 words/s : L.r. 8.8226e-05
[2019-01-31 18:02:54] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 18:02:55] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 18:02:56] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 18:02:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 18:03:00] [valid] Ep. 5 : Up. 185000 : ce-mean-words : 1.60381 : new best
[2019-01-31 18:03:33] [valid] Ep. 5 : Up. 185000 : bleu-detok : 25.4682 : stalled 2 times (last best: 25.5066)
[2019-01-31 18:10:58] Ep. 5 : Up. 186000 : Sen. 2,044,523 : Cost 41.67557144 : Time 484.07s : 40318.73 words/s : L.r. 8.7988e-05
[2019-01-31 18:18:22] Ep. 5 : Up. 187000 : Sen. 3,312,201 : Cost 40.63158798 : Time 443.95s : 43653.02 words/s : L.r. 8.7753e-05
[2019-01-31 18:25:44] Ep. 5 : Up. 188000 : Sen. 4,597,719 : Cost 40.24817276 : Time 442.48s : 43844.37 words/s : L.r. 8.7519e-05
[2019-01-31 18:33:08] Ep. 5 : Up. 189000 : Sen. 5,849,808 : Cost 41.15557861 : Time 443.83s : 43702.37 words/s : L.r. 8.7287e-05
[2019-01-31 18:40:33] Ep. 5 : Up. 190000 : Sen. 7,110,423 : Cost 41.38910294 : Time 445.14s : 44036.72 words/s : L.r. 8.7057e-05
[2019-01-31 18:40:33] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 18:40:34] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 18:40:36] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 18:40:39] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 18:40:40] [valid] Ep. 5 : Up. 190000 : ce-mean-words : 1.60302 : new best
[2019-01-31 18:41:13] [valid] Ep. 5 : Up. 190000 : bleu-detok : 25.345 : stalled 3 times (last best: 25.5066)
[2019-01-31 18:48:33] Ep. 5 : Up. 191000 : Sen. 8,400,315 : Cost 39.29724503 : Time 479.38s : 39761.57 words/s : L.r. 8.6829e-05
[2019-01-31 18:55:57] Ep. 5 : Up. 192000 : Sen. 9,614,249 : Cost 42.84322357 : Time 444.45s : 43953.22 words/s : L.r. 8.6603e-05
[2019-01-31 19:03:22] Ep. 5 : Up. 193000 : Sen. 10,892,175 : Cost 40.55950165 : Time 444.86s : 43847.92 words/s : L.r. 8.6378e-05
[2019-01-31 19:10:44] Ep. 5 : Up. 194000 : Sen. 12,179,265 : Cost 39.88102341 : Time 441.61s : 43756.16 words/s : L.r. 8.6155e-05
[2019-01-31 19:18:07] Ep. 5 : Up. 195000 : Sen. 13,440,293 : Cost 41.07731247 : Time 443.58s : 43780.82 words/s : L.r. 8.5934e-05
[2019-01-31 19:18:07] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 19:18:08] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 19:18:09] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 19:18:13] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 19:18:13] [valid] Ep. 5 : Up. 195000 : ce-mean-words : 1.60153 : new best
[2019-01-31 19:18:46] [valid] Ep. 5 : Up. 195000 : bleu-detok : 25.4658 : stalled 4 times (last best: 25.5066)
[2019-01-31 19:26:13] Ep. 5 : Up. 196000 : Sen. 14,692,927 : Cost 41.47453308 : Time 485.37s : 40383.19 words/s : L.r. 8.5714e-05
[2019-01-31 19:33:35] Ep. 5 : Up. 197000 : Sen. 15,958,123 : Cost 40.83719254 : Time 442.47s : 43830.26 words/s : L.r. 8.5496e-05
[2019-01-31 19:40:58] Ep. 5 : Up. 198000 : Sen. 17,230,886 : Cost 40.38466644 : Time 442.75s : 43711.22 words/s : L.r. 8.5280e-05
[2019-01-31 19:48:18] Ep. 5 : Up. 199000 : Sen. 18,461,083 : Cost 41.77841949 : Time 440.66s : 43801.17 words/s : L.r. 8.5066e-05
[2019-01-31 19:55:44] Ep. 5 : Up. 200000 : Sen. 19,740,720 : Cost 40.71538925 : Time 445.81s : 43956.35 words/s : L.r. 8.4853e-05
[2019-01-31 19:55:44] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 19:55:45] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 19:55:47] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 19:55:50] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 19:55:50] [valid] Ep. 5 : Up. 200000 : ce-mean-words : 1.60034 : new best
[2019-01-31 19:56:23] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 19:56:24] [valid] Ep. 5 : Up. 200000 : bleu-detok : 25.5439 : new best
[2019-01-31 20:03:48] Ep. 5 : Up. 201000 : Sen. 21,018,575 : Cost 40.21818924 : Time 483.37s : 40021.10 words/s : L.r. 8.4641e-05
[2019-01-31 20:11:10] Ep. 5 : Up. 202000 : Sen. 22,276,604 : Cost 40.74013138 : Time 442.34s : 43714.72 words/s : L.r. 8.4432e-05
[2019-01-31 20:18:33] Ep. 5 : Up. 203000 : Sen. 23,548,404 : Cost 40.77703094 : Time 442.52s : 43918.45 words/s : L.r. 8.4223e-05
[2019-01-31 20:25:58] Ep. 5 : Up. 204000 : Sen. 24,812,973 : Cost 40.80214310 : Time 445.47s : 43823.26 words/s : L.r. 8.4017e-05
[2019-01-31 20:33:18] Ep. 5 : Up. 205000 : Sen. 26,063,066 : Cost 40.87796402 : Time 440.31s : 43528.31 words/s : L.r. 8.3812e-05
[2019-01-31 20:33:18] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 20:33:19] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 20:33:20] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 20:33:23] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 20:33:25] [valid] Ep. 5 : Up. 205000 : ce-mean-words : 1.59837 : new best
[2019-01-31 20:33:58] [valid] Ep. 5 : Up. 205000 : bleu-detok : 25.3641 : stalled 1 times (last best: 25.5439)
[2019-01-31 20:41:21] Ep. 5 : Up. 206000 : Sen. 27,366,965 : Cost 39.64696121 : Time 483.05s : 40233.30 words/s : L.r. 8.3608e-05
[2019-01-31 20:48:44] Ep. 5 : Up. 207000 : Sen. 28,578,704 : Cost 42.48041916 : Time 442.97s : 43782.98 words/s : L.r. 8.3406e-05
[2019-01-31 20:56:08] Ep. 5 : Up. 208000 : Sen. 29,865,062 : Cost 40.19095993 : Time 444.07s : 43885.65 words/s : L.r. 8.3205e-05
[2019-01-31 21:03:30] Ep. 5 : Up. 209000 : Sen. 31,114,291 : Cost 41.08678436 : Time 441.45s : 43774.30 words/s : L.r. 8.3006e-05
[2019-01-31 21:10:55] Ep. 5 : Up. 210000 : Sen. 32,397,763 : Cost 40.37855148 : Time 445.13s : 43838.10 words/s : L.r. 8.2808e-05
[2019-01-31 21:10:55] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 21:10:56] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 21:10:57] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 21:11:00] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 21:11:01] [valid] Ep. 5 : Up. 210000 : ce-mean-words : 1.59726 : new best
[2019-01-31 21:11:34] [valid] Ep. 5 : Up. 210000 : bleu-detok : 25.3519 : stalled 2 times (last best: 25.5439)
[2019-01-31 21:18:57] Ep. 5 : Up. 211000 : Sen. 33,659,494 : Cost 40.89156723 : Time 482.30s : 40206.64 words/s : L.r. 8.2611e-05
[2019-01-31 21:26:20] Ep. 5 : Up. 212000 : Sen. 34,916,018 : Cost 40.77777863 : Time 442.64s : 43722.97 words/s : L.r. 8.2416e-05
[2019-01-31 21:33:42] Ep. 5 : Up. 213000 : Sen. 36,181,665 : Cost 40.45838165 : Time 442.33s : 43668.35 words/s : L.r. 8.2223e-05
[2019-01-31 21:41:04] Ep. 5 : Up. 214000 : Sen. 37,446,257 : Cost 40.74324036 : Time 441.92s : 43813.79 words/s : L.r. 8.2030e-05
[2019-01-31 21:48:31] Ep. 5 : Up. 215000 : Sen. 38,701,671 : Cost 41.52110672 : Time 446.85s : 43997.88 words/s : L.r. 8.1839e-05
[2019-01-31 21:48:31] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 21:48:32] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 21:48:33] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 21:48:37] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 21:48:38] [valid] Ep. 5 : Up. 215000 : ce-mean-words : 1.59557 : new best
[2019-01-31 21:49:11] [valid] Ep. 5 : Up. 215000 : bleu-detok : 25.471 : stalled 3 times (last best: 25.5439)
[2019-01-31 21:56:35] Ep. 5 : Up. 216000 : Sen. 39,979,572 : Cost 40.28419876 : Time 483.54s : 40113.98 words/s : L.r. 8.1650e-05
[2019-01-31 22:03:59] Ep. 5 : Up. 217000 : Sen. 41,249,389 : Cost 40.66312408 : Time 444.16s : 43867.21 words/s : L.r. 8.1461e-05
[2019-01-31 22:11:20] Ep. 5 : Up. 218000 : Sen. 42,503,988 : Cost 40.80135727 : Time 440.86s : 43723.28 words/s : L.r. 8.1274e-05
[2019-01-31 22:18:43] Ep. 5 : Up. 219000 : Sen. 43,767,996 : Cost 40.97850418 : Time 443.34s : 43855.79 words/s : L.r. 8.1088e-05
[2019-01-31 22:26:07] Ep. 5 : Up. 220000 : Sen. 45,049,832 : Cost 40.13110733 : Time 443.93s : 43816.96 words/s : L.r. 8.0904e-05
[2019-01-31 22:26:07] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 22:26:08] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 22:26:09] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 22:26:13] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 22:26:13] [valid] Ep. 5 : Up. 220000 : ce-mean-words : 1.59361 : new best
[2019-01-31 22:26:47] [valid] Ep. 5 : Up. 220000 : bleu-detok : 25.4777 : stalled 4 times (last best: 25.5439)
[2019-01-31 22:34:10] Ep. 5 : Up. 221000 : Sen. 46,299,233 : Cost 41.03086472 : Time 482.86s : 40048.54 words/s : L.r. 8.0721e-05
[2019-01-31 22:41:36] Ep. 5 : Up. 222000 : Sen. 47,548,236 : Cost 41.59677505 : Time 446.53s : 43964.12 words/s : L.r. 8.0539e-05
[2019-01-31 22:48:58] Ep. 5 : Up. 223000 : Sen. 48,836,311 : Cost 39.93128204 : Time 441.80s : 43833.88 words/s : L.r. 8.0358e-05
[2019-01-31 22:56:20] Ep. 5 : Up. 224000 : Sen. 50,109,316 : Cost 40.13569260 : Time 441.93s : 43604.66 words/s : L.r. 8.0178e-05
[2019-01-31 23:03:45] Ep. 5 : Up. 225000 : Sen. 51,376,497 : Cost 40.72694397 : Time 444.72s : 43867.88 words/s : L.r. 8.0000e-05
[2019-01-31 23:03:45] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 23:03:46] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 23:03:47] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 23:03:50] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 23:03:51] [valid] Ep. 5 : Up. 225000 : ce-mean-words : 1.59173 : new best
[2019-01-31 23:04:24] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-01-31 23:04:25] [valid] Ep. 5 : Up. 225000 : bleu-detok : 25.6647 : new best
[2019-01-31 23:11:48] Ep. 5 : Up. 226000 : Sen. 52,642,502 : Cost 40.62012100 : Time 483.28s : 40132.26 words/s : L.r. 7.9823e-05
[2019-01-31 23:19:13] Ep. 5 : Up. 227000 : Sen. 53,911,480 : Cost 40.70363235 : Time 444.76s : 43833.35 words/s : L.r. 7.9647e-05
[2019-01-31 23:26:36] Ep. 5 : Up. 228000 : Sen. 55,168,072 : Cost 41.00344467 : Time 443.20s : 43771.10 words/s : L.r. 7.9472e-05
[2019-01-31 23:33:59] Ep. 5 : Up. 229000 : Sen. 56,430,226 : Cost 40.73258591 : Time 443.49s : 43809.25 words/s : L.r. 7.9298e-05
[2019-01-31 23:41:23] Ep. 5 : Up. 230000 : Sen. 57,694,522 : Cost 40.75223923 : Time 443.89s : 43800.51 words/s : L.r. 7.9126e-05
[2019-01-31 23:41:23] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-01-31 23:41:24] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-01-31 23:41:25] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-01-31 23:41:28] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-01-31 23:41:29] [valid] Ep. 5 : Up. 230000 : ce-mean-words : 1.59035 : new best
[2019-01-31 23:42:02] [valid] Ep. 5 : Up. 230000 : bleu-detok : 25.6416 : stalled 1 times (last best: 25.6647)
[2019-01-31 23:45:31] Seen 58282224 samples
[2019-01-31 23:45:31] Starting epoch 6
[2019-01-31 23:45:31] [data] Shuffling data
[2019-01-31 23:46:29] [data] Done reading 58408180 sentences
[2019-01-31 23:48:25] [data] Done shuffling 58408180 sentences to temp files
[2019-01-31 23:53:01] Ep. 6 : Up. 231000 : Sen. 651,086 : Cost 40.11099625 : Time 697.94s : 26847.89 words/s : L.r. 7.8954e-05
[2019-02-01 00:00:26] Ep. 6 : Up. 232000 : Sen. 1,910,473 : Cost 40.72961426 : Time 444.39s : 43840.73 words/s : L.r. 7.8784e-05
[2019-02-01 00:07:48] Ep. 6 : Up. 233000 : Sen. 3,156,273 : Cost 41.06105423 : Time 442.22s : 43747.53 words/s : L.r. 7.8615e-05
[2019-02-01 00:15:12] Ep. 6 : Up. 234000 : Sen. 4,447,642 : Cost 39.75716400 : Time 444.38s : 43822.38 words/s : L.r. 7.8446e-05
[2019-02-01 00:22:36] Ep. 6 : Up. 235000 : Sen. 5,713,717 : Cost 40.46557617 : Time 443.75s : 43810.46 words/s : L.r. 7.8279e-05
[2019-02-01 00:22:36] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 00:22:37] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 00:22:38] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 00:22:41] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 00:22:42] [valid] Ep. 6 : Up. 235000 : ce-mean-words : 1.58999 : new best
[2019-02-01 00:23:16] [valid] Ep. 6 : Up. 235000 : bleu-detok : 25.5454 : stalled 2 times (last best: 25.6647)
[2019-02-01 00:30:40] Ep. 6 : Up. 236000 : Sen. 6,973,364 : Cost 40.82249069 : Time 484.06s : 40135.35 words/s : L.r. 7.8113e-05
[2019-02-01 00:38:02] Ep. 6 : Up. 237000 : Sen. 8,245,514 : Cost 40.07488251 : Time 442.12s : 43697.12 words/s : L.r. 7.7948e-05
[2019-02-01 00:45:27] Ep. 6 : Up. 238000 : Sen. 9,491,834 : Cost 41.61120987 : Time 444.33s : 44037.96 words/s : L.r. 7.7784e-05
[2019-02-01 00:52:51] Ep. 6 : Up. 239000 : Sen. 10,744,209 : Cost 40.88582230 : Time 444.01s : 43740.26 words/s : L.r. 7.7622e-05
[2019-02-01 01:00:12] Ep. 6 : Up. 240000 : Sen. 12,025,448 : Cost 39.83663177 : Time 441.74s : 43760.98 words/s : L.r. 7.7460e-05
[2019-02-01 01:00:12] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 01:00:13] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 01:00:15] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 01:00:18] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 01:00:19] [valid] Ep. 6 : Up. 240000 : ce-mean-words : 1.58965 : new best
[2019-02-01 01:00:51] [valid] Ep. 6 : Up. 240000 : bleu-detok : 25.5099 : stalled 3 times (last best: 25.6647)
[2019-02-01 01:08:14] Ep. 6 : Up. 241000 : Sen. 13,275,499 : Cost 40.91535187 : Time 481.74s : 40126.68 words/s : L.r. 7.7299e-05
[2019-02-01 01:15:38] Ep. 6 : Up. 242000 : Sen. 14,563,725 : Cost 39.94068527 : Time 443.93s : 43860.57 words/s : L.r. 7.7139e-05
[2019-02-01 01:23:00] Ep. 6 : Up. 243000 : Sen. 15,823,023 : Cost 40.56765747 : Time 442.45s : 43773.31 words/s : L.r. 7.6980e-05
[2019-02-01 01:30:23] Ep. 6 : Up. 244000 : Sen. 17,093,470 : Cost 40.20948410 : Time 442.45s : 43707.40 words/s : L.r. 7.6822e-05
[2019-02-01 01:37:46] Ep. 6 : Up. 245000 : Sen. 18,336,059 : Cost 41.56007767 : Time 442.76s : 43971.39 words/s : L.r. 7.6665e-05
[2019-02-01 01:37:46] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 01:37:47] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 01:37:48] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 01:37:51] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 01:37:52] [valid] Ep. 6 : Up. 245000 : ce-mean-words : 1.58893 : new best
[2019-02-01 01:38:24] [valid] Ep. 6 : Up. 245000 : bleu-detok : 25.5721 : stalled 4 times (last best: 25.6647)
[2019-02-01 01:45:50] Ep. 6 : Up. 246000 : Sen. 19,613,597 : Cost 40.20393753 : Time 484.26s : 40230.79 words/s : L.r. 7.6509e-05
[2019-02-01 01:53:12] Ep. 6 : Up. 247000 : Sen. 20,877,223 : Cost 40.12440872 : Time 441.81s : 43601.25 words/s : L.r. 7.6354e-05
[2019-02-01 02:00:36] Ep. 6 : Up. 248000 : Sen. 22,141,948 : Cost 40.74242783 : Time 444.13s : 43905.43 words/s : L.r. 7.6200e-05
[2019-02-01 02:08:01] Ep. 6 : Up. 249000 : Sen. 23,407,026 : Cost 40.87159729 : Time 444.97s : 43952.50 words/s : L.r. 7.6047e-05
[2019-02-01 02:15:21] Ep. 6 : Up. 250000 : Sen. 24,692,780 : Cost 39.54125977 : Time 440.29s : 43622.05 words/s : L.r. 7.5895e-05
[2019-02-01 02:15:21] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 02:15:22] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 02:15:24] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 02:15:27] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 02:15:28] [valid] Ep. 6 : Up. 250000 : ce-mean-words : 1.58794 : new best
[2019-02-01 02:16:00] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-02-01 02:16:01] [valid] Ep. 6 : Up. 250000 : bleu-detok : 25.6882 : new best
[2019-02-01 02:23:27] Ep. 6 : Up. 251000 : Sen. 25,942,751 : Cost 40.98015594 : Time 486.06s : 40127.69 words/s : L.r. 7.5743e-05
[2019-02-01 02:30:51] Ep. 6 : Up. 252000 : Sen. 27,221,109 : Cost 40.33914566 : Time 444.06s : 43868.86 words/s : L.r. 7.5593e-05
[2019-02-01 02:38:16] Ep. 6 : Up. 253000 : Sen. 28,465,128 : Cost 41.42486191 : Time 444.70s : 43876.91 words/s : L.r. 7.5443e-05
[2019-02-01 02:45:41] Ep. 6 : Up. 254000 : Sen. 29,731,928 : Cost 40.48859406 : Time 444.99s : 43687.36 words/s : L.r. 7.5295e-05
[2019-02-01 02:53:05] Ep. 6 : Up. 255000 : Sen. 31,013,761 : Cost 40.16384506 : Time 444.50s : 43896.33 words/s : L.r. 7.5147e-05
[2019-02-01 02:53:05] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 02:53:07] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 02:53:08] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 02:53:11] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 02:53:12] [valid] Ep. 6 : Up. 255000 : ce-mean-words : 1.58612 : new best
[2019-02-01 02:53:45] [valid] Ep. 6 : Up. 255000 : bleu-detok : 25.6235 : stalled 1 times (last best: 25.6882)
[2019-02-01 03:01:09] Ep. 6 : Up. 256000 : Sen. 32,287,886 : Cost 40.15056229 : Time 483.46s : 40081.69 words/s : L.r. 7.5000e-05
[2019-02-01 03:08:32] Ep. 6 : Up. 257000 : Sen. 33,531,059 : Cost 41.32492828 : Time 442.83s : 43834.95 words/s : L.r. 7.4854e-05
[2019-02-01 03:15:56] Ep. 6 : Up. 258000 : Sen. 34,827,188 : Cost 39.46963501 : Time 444.77s : 43780.14 words/s : L.r. 7.4709e-05
[2019-02-01 03:23:19] Ep. 6 : Up. 259000 : Sen. 36,067,363 : Cost 41.27164459 : Time 442.16s : 43714.92 words/s : L.r. 7.4564e-05
[2019-02-01 03:30:44] Ep. 6 : Up. 260000 : Sen. 37,351,373 : Cost 40.10242844 : Time 445.35s : 43896.61 words/s : L.r. 7.4421e-05
[2019-02-01 03:30:44] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 03:30:45] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 03:30:46] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 03:30:49] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 03:30:50] [valid] Ep. 6 : Up. 260000 : ce-mean-words : 1.58415 : new best
[2019-02-01 03:31:23] [valid] Ep. 6 : Up. 260000 : bleu-detok : 25.6578 : stalled 2 times (last best: 25.6882)
[2019-02-01 03:38:44] Ep. 6 : Up. 261000 : Sen. 38,588,676 : Cost 41.10745239 : Time 479.91s : 40143.67 words/s : L.r. 7.4278e-05
[2019-02-01 03:46:10] Ep. 6 : Up. 262000 : Sen. 39,873,959 : Cost 40.24292374 : Time 446.27s : 43864.73 words/s : L.r. 7.4136e-05
[2019-02-01 03:53:33] Ep. 6 : Up. 263000 : Sen. 41,146,602 : Cost 40.11122131 : Time 442.57s : 43696.66 words/s : L.r. 7.3995e-05
[2019-02-01 04:00:58] Ep. 6 : Up. 264000 : Sen. 42,393,936 : Cost 41.08966827 : Time 445.16s : 43719.97 words/s : L.r. 7.3855e-05
[2019-02-01 04:08:22] Ep. 6 : Up. 265000 : Sen. 43,674,715 : Cost 40.26126862 : Time 444.45s : 43850.12 words/s : L.r. 7.3715e-05
[2019-02-01 04:08:22] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 04:08:23] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 04:08:25] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 04:08:28] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 04:08:29] [valid] Ep. 6 : Up. 265000 : ce-mean-words : 1.5826 : new best
[2019-02-01 04:09:01] [valid] Ep. 6 : Up. 265000 : bleu-detok : 25.6723 : stalled 3 times (last best: 25.6882)
[2019-02-01 04:16:26] Ep. 6 : Up. 266000 : Sen. 44,950,270 : Cost 40.20006943 : Time 483.63s : 40226.02 words/s : L.r. 7.3577e-05
[2019-02-01 04:23:50] Ep. 6 : Up. 267000 : Sen. 46,218,052 : Cost 40.35993195 : Time 443.98s : 43786.98 words/s : L.r. 7.3439e-05
[2019-02-01 04:31:14] Ep. 6 : Up. 268000 : Sen. 47,462,368 : Cost 41.52235794 : Time 444.30s : 43983.15 words/s : L.r. 7.3302e-05
[2019-02-01 04:38:38] Ep. 6 : Up. 269000 : Sen. 48,731,694 : Cost 40.12590408 : Time 443.42s : 43631.49 words/s : L.r. 7.3165e-05
[2019-02-01 04:46:01] Ep. 6 : Up. 270000 : Sen. 50,010,629 : Cost 40.20314407 : Time 443.41s : 43871.55 words/s : L.r. 7.3030e-05
[2019-02-01 04:46:01] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 04:46:02] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 04:46:03] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 04:46:07] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 04:46:07] [valid] Ep. 6 : Up. 270000 : ce-mean-words : 1.58098 : new best
[2019-02-01 04:46:43] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-02-01 04:46:44] [valid] Ep. 6 : Up. 270000 : bleu-detok : 25.7414 : new best
[2019-02-01 04:54:06] Ep. 6 : Up. 271000 : Sen. 51,256,427 : Cost 40.92331314 : Time 484.65s : 39950.89 words/s : L.r. 7.2895e-05
[2019-02-01 05:01:29] Ep. 6 : Up. 272000 : Sen. 52,533,488 : Cost 39.99200439 : Time 443.65s : 43714.84 words/s : L.r. 7.2761e-05
[2019-02-01 05:08:54] Ep. 6 : Up. 273000 : Sen. 53,794,490 : Cost 40.86772919 : Time 444.78s : 43917.02 words/s : L.r. 7.2627e-05
[2019-02-01 05:16:19] Ep. 6 : Up. 274000 : Sen. 55,072,141 : Cost 40.05508041 : Time 444.39s : 43714.34 words/s : L.r. 7.2495e-05
[2019-02-01 05:23:42] Ep. 6 : Up. 275000 : Sen. 56,351,947 : Cost 40.00500870 : Time 443.59s : 43710.15 words/s : L.r. 7.2363e-05
[2019-02-01 05:23:42] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 05:23:43] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 05:23:44] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 05:23:48] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 05:23:49] [valid] Ep. 6 : Up. 275000 : ce-mean-words : 1.57966 : new best
[2019-02-01 05:24:24] [valid] Ep. 6 : Up. 275000 : bleu-detok : 25.7226 : stalled 1 times (last best: 25.7414)
[2019-02-01 05:31:49] Ep. 6 : Up. 276000 : Sen. 57,626,139 : Cost 40.63177109 : Time 486.91s : 40202.37 words/s : L.r. 7.2232e-05
[2019-02-01 05:35:52] Seen 58282224 samples
[2019-02-01 05:35:52] Starting epoch 7
[2019-02-01 05:35:52] [data] Shuffling data
[2019-02-01 05:36:50] [data] Done reading 58408180 sentences
[2019-02-01 05:38:47] [data] Done shuffling 58408180 sentences to temp files
[2019-02-01 05:42:47] Ep. 7 : Up. 277000 : Sen. 541,919 : Cost 40.74498749 : Time 657.98s : 28225.02 words/s : L.r. 7.2101e-05
[2019-02-01 05:50:10] Ep. 7 : Up. 278000 : Sen. 1,825,068 : Cost 39.61102676 : Time 443.22s : 43699.07 words/s : L.r. 7.1971e-05
[2019-02-01 05:57:34] Ep. 7 : Up. 279000 : Sen. 3,086,120 : Cost 40.47995758 : Time 443.95s : 43827.36 words/s : L.r. 7.1842e-05
[2019-02-01 06:04:58] Ep. 7 : Up. 280000 : Sen. 4,331,519 : Cost 40.97801590 : Time 443.63s : 43798.94 words/s : L.r. 7.1714e-05
[2019-02-01 06:04:58] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 06:04:59] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz
[2019-02-01 06:05:00] Saving Adam parameters to ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 06:05:03] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-ce-mean-words.npz
[2019-02-01 06:05:04] [valid] Ep. 7 : Up. 280000 : ce-mean-words : 1.57876 : new best
[2019-02-01 06:05:39] Saving model weights and runtime parameters to ./models/encs/00.baseline/model.npz.best-bleu-detok.npz
[2019-02-01 06:05:40] [valid] Ep. 7 : Up. 280000 : bleu-detok : 25.8126 : new best
[2019-02-01 06:13:04] Ep. 7 : Up. 281000 : Sen. 5,630,740 : Cost 39.41253281 : Time 486.09s : 40023.06 words/s : L.r. 7.1586e-05
[2019-02-01 06:20:27] Ep. 7 : Up. 282000 : Sen. 6,887,803 : Cost 40.65205765 : Time 443.22s : 43834.16 words/s : L.r. 7.1459e-05
[2019-02-01 06:39:04] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 06:39:04] [marian] Running on gpu-e-13 as process 60505 with command line:
[2019-02-01 06:39:04] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus01.nrm.en.gz ../data/corpus01.nrm.cs.gz --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --sentencepiece-alphas 0.2 0 --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-02-01 06:39:05] [config] after-batches: 0
[2019-02-01 06:39:05] [config] after-epochs: 0
[2019-02-01 06:39:05] [config] allow-unk: false
[2019-02-01 06:39:05] [config] beam-size: 8
[2019-02-01 06:39:05] [config] bert-class-symbol: "[CLS]"
[2019-02-01 06:39:05] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 06:39:05] [config] bert-masking-fraction: 0.15
[2019-02-01 06:39:05] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 06:39:05] [config] best-deep: false
[2019-02-01 06:39:05] [config] clip-gemm: 0
[2019-02-01 06:39:05] [config] clip-norm: 5
[2019-02-01 06:39:05] [config] cost-type: ce-mean
[2019-02-01 06:39:05] [config] cpu-threads: 0
[2019-02-01 06:39:05] [config] data-weighting-type: sentence
[2019-02-01 06:39:05] [config] dec-cell: gru
[2019-02-01 06:39:05] [config] dec-cell-base-depth: 2
[2019-02-01 06:39:05] [config] dec-cell-high-depth: 1
[2019-02-01 06:39:05] [config] dec-depth: 6
[2019-02-01 06:39:05] [config] devices:
[2019-02-01 06:39:05] [config]   - 0
[2019-02-01 06:39:05] [config]   - 1
[2019-02-01 06:39:05] [config]   - 2
[2019-02-01 06:39:05] [config]   - 3
[2019-02-01 06:39:05] [config] dim-emb: 512
[2019-02-01 06:39:05] [config] dim-rnn: 1024
[2019-02-01 06:39:05] [config] dim-vocabs:
[2019-02-01 06:39:05] [config]   - 32000
[2019-02-01 06:39:05] [config]   - 32000
[2019-02-01 06:39:05] [config] disp-first: 0
[2019-02-01 06:39:05] [config] disp-freq: 1000
[2019-02-01 06:39:05] [config] disp-label-counts: false
[2019-02-01 06:39:05] [config] dropout-rnn: 0
[2019-02-01 06:39:05] [config] dropout-src: 0
[2019-02-01 06:39:05] [config] dropout-trg: 0
[2019-02-01 06:39:05] [config] early-stopping: 5
[2019-02-01 06:39:05] [config] embedding-fix-src: false
[2019-02-01 06:39:05] [config] embedding-fix-trg: false
[2019-02-01 06:39:05] [config] embedding-normalization: false
[2019-02-01 06:39:05] [config] enc-cell: gru
[2019-02-01 06:39:05] [config] enc-cell-depth: 1
[2019-02-01 06:39:05] [config] enc-depth: 6
[2019-02-01 06:39:05] [config] enc-type: bidirectional
[2019-02-01 06:39:05] [config] exponential-smoothing: 0.0001
[2019-02-01 06:39:05] [config] grad-dropping-momentum: 0
[2019-02-01 06:39:05] [config] grad-dropping-rate: 0
[2019-02-01 06:39:05] [config] grad-dropping-warmup: 100
[2019-02-01 06:39:05] [config] guided-alignment: none
[2019-02-01 06:39:05] [config] guided-alignment-cost: mse
[2019-02-01 06:39:05] [config] guided-alignment-weight: 0.1
[2019-02-01 06:39:05] [config] ignore-model-config: false
[2019-02-01 06:39:05] [config] input-types:
[2019-02-01 06:39:05] [config]   []
[2019-02-01 06:39:05] [config] interpolate-env-vars: false
[2019-02-01 06:39:05] [config] keep-best: true
[2019-02-01 06:39:05] [config] label-smoothing: 0.1
[2019-02-01 06:39:05] [config] layer-normalization: true
[2019-02-01 06:39:05] [config] learn-rate: 0.0003
[2019-02-01 06:39:05] [config] log: ./models/encs/00.baseline/train.log
[2019-02-01 06:39:05] [config] log-level: info
[2019-02-01 06:39:05] [config] lr-decay: 0
[2019-02-01 06:39:05] [config] lr-decay-freq: 50000
[2019-02-01 06:39:05] [config] lr-decay-inv-sqrt:
[2019-02-01 06:39:05] [config]   - 16000
[2019-02-01 06:39:05] [config] lr-decay-repeat-warmup: false
[2019-02-01 06:39:05] [config] lr-decay-reset-optimizer: false
[2019-02-01 06:39:05] [config] lr-decay-start:
[2019-02-01 06:39:05] [config]   - 10
[2019-02-01 06:39:05] [config]   - 1
[2019-02-01 06:39:05] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 06:39:05] [config] lr-report: true
[2019-02-01 06:39:05] [config] lr-warmup: 16000
[2019-02-01 06:39:05] [config] lr-warmup-at-reload: false
[2019-02-01 06:39:05] [config] lr-warmup-cycle: false
[2019-02-01 06:39:05] [config] lr-warmup-start-rate: 0
[2019-02-01 06:39:05] [config] max-length: 120
[2019-02-01 06:39:05] [config] max-length-crop: false
[2019-02-01 06:39:05] [config] max-length-factor: 3
[2019-02-01 06:39:05] [config] maxi-batch: 1000
[2019-02-01 06:39:05] [config] maxi-batch-sort: trg
[2019-02-01 06:39:05] [config] mini-batch: 1000
[2019-02-01 06:39:05] [config] mini-batch-fit: true
[2019-02-01 06:39:05] [config] mini-batch-fit-step: 10
[2019-02-01 06:39:05] [config] mini-batch-overstuff: 1
[2019-02-01 06:39:05] [config] mini-batch-track-lr: false
[2019-02-01 06:39:05] [config] mini-batch-understuff: 1
[2019-02-01 06:39:05] [config] mini-batch-warmup: 0
[2019-02-01 06:39:05] [config] mini-batch-words: 0
[2019-02-01 06:39:05] [config] mini-batch-words-ref: 0
[2019-02-01 06:39:05] [config] model: ./models/encs/00.baseline/model.npz
[2019-02-01 06:39:05] [config] multi-loss-type: sum
[2019-02-01 06:39:05] [config] multi-node: false
[2019-02-01 06:39:05] [config] multi-node-overlap: true
[2019-02-01 06:39:05] [config] n-best: false
[2019-02-01 06:39:05] [config] no-nccl: false
[2019-02-01 06:39:05] [config] no-reload: false
[2019-02-01 06:39:05] [config] no-restore-corpus: false
[2019-02-01 06:39:05] [config] no-shuffle: false
[2019-02-01 06:39:05] [config] normalize: 1
[2019-02-01 06:39:05] [config] optimizer: adam
[2019-02-01 06:39:05] [config] optimizer-delay: 1
[2019-02-01 06:39:05] [config] optimizer-params:
[2019-02-01 06:39:05] [config]   - 0.9
[2019-02-01 06:39:05] [config]   - 0.98
[2019-02-01 06:39:05] [config]   - 1e-09
[2019-02-01 06:39:05] [config] overwrite: true
[2019-02-01 06:39:05] [config] quiet: false
[2019-02-01 06:39:05] [config] quiet-translation: true
[2019-02-01 06:39:05] [config] relative-paths: false
[2019-02-01 06:39:05] [config] right-left: false
[2019-02-01 06:39:05] [config] save-freq: 5000
[2019-02-01 06:39:05] [config] seed: 0
[2019-02-01 06:39:05] [config] sentencepiece-alphas:
[2019-02-01 06:39:05] [config]   - 0.2
[2019-02-01 06:39:05] [config]   - 0
[2019-02-01 06:39:05] [config] sentencepiece-max-lines: 10000000
[2019-02-01 06:39:05] [config] sentencepiece-options: ""
[2019-02-01 06:39:05] [config] shuffle-in-ram: false
[2019-02-01 06:39:05] [config] skip: false
[2019-02-01 06:39:05] [config] sqlite: ""
[2019-02-01 06:39:05] [config] sqlite-drop: false
[2019-02-01 06:39:05] [config] sync-sgd: true
[2019-02-01 06:39:05] [config] tempdir: /tmp
[2019-02-01 06:39:05] [config] tied-embeddings: false
[2019-02-01 06:39:05] [config] tied-embeddings-all: true
[2019-02-01 06:39:05] [config] tied-embeddings-src: false
[2019-02-01 06:39:05] [config] train-sets:
[2019-02-01 06:39:05] [config]   - ../data/corpus01.nrm.en.gz
[2019-02-01 06:39:05] [config]   - ../data/corpus01.nrm.cs.gz
[2019-02-01 06:39:05] [config] transformer-aan-activation: swish
[2019-02-01 06:39:05] [config] transformer-aan-depth: 2
[2019-02-01 06:39:05] [config] transformer-aan-nogate: false
[2019-02-01 06:39:05] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 06:39:05] [config] transformer-dim-aan: 2048
[2019-02-01 06:39:05] [config] transformer-dim-ffn: 2048
[2019-02-01 06:39:05] [config] transformer-dropout: 0.1
[2019-02-01 06:39:05] [config] transformer-dropout-attention: 0
[2019-02-01 06:39:05] [config] transformer-dropout-ffn: 0
[2019-02-01 06:39:05] [config] transformer-ffn-activation: swish
[2019-02-01 06:39:05] [config] transformer-ffn-depth: 2
[2019-02-01 06:39:05] [config] transformer-guided-alignment-layer: last
[2019-02-01 06:39:05] [config] transformer-heads: 8
[2019-02-01 06:39:05] [config] transformer-no-projection: false
[2019-02-01 06:39:05] [config] transformer-postprocess: da
[2019-02-01 06:39:05] [config] transformer-postprocess-emb: d
[2019-02-01 06:39:05] [config] transformer-preprocess: n
[2019-02-01 06:39:05] [config] transformer-tied-layers:
[2019-02-01 06:39:05] [config]   []
[2019-02-01 06:39:05] [config] transformer-train-positions: false
[2019-02-01 06:39:05] [config] type: transformer
[2019-02-01 06:39:05] [config] ulr: false
[2019-02-01 06:39:05] [config] ulr-dim-emb: 0
[2019-02-01 06:39:05] [config] ulr-dropout: 0
[2019-02-01 06:39:05] [config] ulr-keys-vectors: ""
[2019-02-01 06:39:05] [config] ulr-query-vectors: ""
[2019-02-01 06:39:05] [config] ulr-softmax-temperature: 1
[2019-02-01 06:39:05] [config] ulr-trainable-transformation: false
[2019-02-01 06:39:05] [config] valid-freq: 5000
[2019-02-01 06:39:05] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-02-01 06:39:05] [config] valid-max-length: 1000
[2019-02-01 06:39:05] [config] valid-metrics:
[2019-02-01 06:39:05] [config]   - ce-mean-words
[2019-02-01 06:39:05] [config]   - bleu-detok
[2019-02-01 06:39:05] [config] valid-mini-batch: 32
[2019-02-01 06:39:05] [config] valid-sets:
[2019-02-01 06:39:05] [config]   - ../data/newstest2016.en
[2019-02-01 06:39:05] [config]   - ../data/newstest2016.cs
[2019-02-01 06:39:05] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-02-01 06:39:05] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 06:39:05] [config] vocabs:
[2019-02-01 06:39:05] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 06:39:05] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 06:39:05] [config] word-penalty: 0
[2019-02-01 06:39:05] [config] workspace: 10000
[2019-02-01 06:39:05] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 06:39:05] Using synchronous training
[2019-02-01 06:39:05] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 06:39:05] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 06:39:05] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 06:39:05] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 06:39:05] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 06:39:05] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 06:39:06] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 06:39:08] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 06:39:08] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 06:39:09] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 06:39:09] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 06:39:09] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 06:39:09] [training] Using 4 GPUs
[2019-02-01 06:39:09] [memory] Reserving 230 MB, device gpu0
[2019-02-01 06:39:09] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 06:39:09] [memory] Reserving 230 MB, device gpu0
[2019-02-01 06:39:30] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 06:39:30] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 06:39:30] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 06:39:31] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 06:39:31] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 06:39:31] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 06:39:31] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 06:39:31] [training] Using 4 GPUs
[2019-02-01 06:39:31] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 06:39:31] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 06:39:32] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 06:39:32] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 06:39:32] Loading Adam parameters from ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 06:39:33] [memory] Reserving 115 MB, device gpu0
[2019-02-01 06:39:33] [memory] Reserving 115 MB, device gpu1
[2019-02-01 06:39:33] [memory] Reserving 115 MB, device gpu2
[2019-02-01 06:39:33] [memory] Reserving 115 MB, device gpu3
[2019-02-01 06:39:33] [training] Model reloaded from ./models/encs/00.baseline/model.npz
[2019-02-01 06:39:33] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 06:39:33] [data] Shuffling data
[2019-02-01 06:40:37] [data] Done reading 63898897 sentences
[2019-02-01 06:42:47] [data] Done shuffling 63898897 sentences to temp files
[2019-02-01 06:47:39] Training started
[2019-02-01 06:47:39] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu3
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu1
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu2
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu0
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu3
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu1
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu2
[2019-02-01 06:47:39] [memory] Reserving 230 MB, device gpu0
[2019-02-01 06:47:39] Loading model from ./models/encs/00.baseline/model.npz
[2019-02-01 06:47:40] [memory] Reserving 230 MB, device cpu0
[2019-02-01 06:47:40] [memory] Reserving 57 MB, device gpu0
[2019-02-01 06:47:40] [memory] Reserving 57 MB, device gpu1
[2019-02-01 06:47:40] [memory] Reserving 57 MB, device gpu2
[2019-02-01 06:47:40] [memory] Reserving 57 MB, device gpu3
[2019-02-01 07:08:58] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 07:08:58] [marian] Running on gpu-e-12 as process 165358 with command line:
[2019-02-01 07:08:58] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus01.nrm.en.gz ../data/corpus01.nrm.cs.gz --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --sentencepiece-alphas 0.2 0 --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-02-01 07:08:59] [config] after-batches: 0
[2019-02-01 07:08:59] [config] after-epochs: 0
[2019-02-01 07:08:59] [config] allow-unk: false
[2019-02-01 07:08:59] [config] beam-size: 8
[2019-02-01 07:08:59] [config] bert-class-symbol: "[CLS]"
[2019-02-01 07:08:59] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 07:08:59] [config] bert-masking-fraction: 0.15
[2019-02-01 07:08:59] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 07:08:59] [config] best-deep: false
[2019-02-01 07:08:59] [config] clip-gemm: 0
[2019-02-01 07:08:59] [config] clip-norm: 5
[2019-02-01 07:08:59] [config] cost-type: ce-mean
[2019-02-01 07:08:59] [config] cpu-threads: 0
[2019-02-01 07:08:59] [config] data-weighting-type: sentence
[2019-02-01 07:08:59] [config] dec-cell: gru
[2019-02-01 07:08:59] [config] dec-cell-base-depth: 2
[2019-02-01 07:08:59] [config] dec-cell-high-depth: 1
[2019-02-01 07:08:59] [config] dec-depth: 6
[2019-02-01 07:08:59] [config] devices:
[2019-02-01 07:08:59] [config]   - 0
[2019-02-01 07:08:59] [config]   - 1
[2019-02-01 07:08:59] [config]   - 2
[2019-02-01 07:08:59] [config]   - 3
[2019-02-01 07:08:59] [config] dim-emb: 512
[2019-02-01 07:08:59] [config] dim-rnn: 1024
[2019-02-01 07:08:59] [config] dim-vocabs:
[2019-02-01 07:08:59] [config]   - 32000
[2019-02-01 07:08:59] [config]   - 32000
[2019-02-01 07:08:59] [config] disp-first: 0
[2019-02-01 07:08:59] [config] disp-freq: 1000
[2019-02-01 07:08:59] [config] disp-label-counts: false
[2019-02-01 07:08:59] [config] dropout-rnn: 0
[2019-02-01 07:08:59] [config] dropout-src: 0
[2019-02-01 07:08:59] [config] dropout-trg: 0
[2019-02-01 07:08:59] [config] early-stopping: 5
[2019-02-01 07:08:59] [config] embedding-fix-src: false
[2019-02-01 07:08:59] [config] embedding-fix-trg: false
[2019-02-01 07:08:59] [config] embedding-normalization: false
[2019-02-01 07:08:59] [config] enc-cell: gru
[2019-02-01 07:08:59] [config] enc-cell-depth: 1
[2019-02-01 07:08:59] [config] enc-depth: 6
[2019-02-01 07:08:59] [config] enc-type: bidirectional
[2019-02-01 07:08:59] [config] exponential-smoothing: 0.0001
[2019-02-01 07:08:59] [config] grad-dropping-momentum: 0
[2019-02-01 07:08:59] [config] grad-dropping-rate: 0
[2019-02-01 07:08:59] [config] grad-dropping-warmup: 100
[2019-02-01 07:08:59] [config] guided-alignment: none
[2019-02-01 07:08:59] [config] guided-alignment-cost: mse
[2019-02-01 07:08:59] [config] guided-alignment-weight: 0.1
[2019-02-01 07:08:59] [config] ignore-model-config: false
[2019-02-01 07:08:59] [config] input-types:
[2019-02-01 07:08:59] [config]   []
[2019-02-01 07:08:59] [config] interpolate-env-vars: false
[2019-02-01 07:08:59] [config] keep-best: true
[2019-02-01 07:08:59] [config] label-smoothing: 0.1
[2019-02-01 07:08:59] [config] layer-normalization: true
[2019-02-01 07:08:59] [config] learn-rate: 0.0003
[2019-02-01 07:08:59] [config] log: ./models/encs/00.baseline/train.log
[2019-02-01 07:08:59] [config] log-level: info
[2019-02-01 07:08:59] [config] lr-decay: 0
[2019-02-01 07:08:59] [config] lr-decay-freq: 50000
[2019-02-01 07:08:59] [config] lr-decay-inv-sqrt:
[2019-02-01 07:08:59] [config]   - 16000
[2019-02-01 07:08:59] [config] lr-decay-repeat-warmup: false
[2019-02-01 07:08:59] [config] lr-decay-reset-optimizer: false
[2019-02-01 07:08:59] [config] lr-decay-start:
[2019-02-01 07:08:59] [config]   - 10
[2019-02-01 07:08:59] [config]   - 1
[2019-02-01 07:08:59] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 07:08:59] [config] lr-report: true
[2019-02-01 07:08:59] [config] lr-warmup: 16000
[2019-02-01 07:08:59] [config] lr-warmup-at-reload: false
[2019-02-01 07:08:59] [config] lr-warmup-cycle: false
[2019-02-01 07:08:59] [config] lr-warmup-start-rate: 0
[2019-02-01 07:08:59] [config] max-length: 120
[2019-02-01 07:08:59] [config] max-length-crop: false
[2019-02-01 07:08:59] [config] max-length-factor: 3
[2019-02-01 07:08:59] [config] maxi-batch: 1000
[2019-02-01 07:08:59] [config] maxi-batch-sort: trg
[2019-02-01 07:08:59] [config] mini-batch: 1000
[2019-02-01 07:08:59] [config] mini-batch-fit: true
[2019-02-01 07:08:59] [config] mini-batch-fit-step: 10
[2019-02-01 07:08:59] [config] mini-batch-overstuff: 1
[2019-02-01 07:08:59] [config] mini-batch-track-lr: false
[2019-02-01 07:08:59] [config] mini-batch-understuff: 1
[2019-02-01 07:08:59] [config] mini-batch-warmup: 0
[2019-02-01 07:08:59] [config] mini-batch-words: 0
[2019-02-01 07:08:59] [config] mini-batch-words-ref: 0
[2019-02-01 07:08:59] [config] model: ./models/encs/00.baseline/model.npz
[2019-02-01 07:08:59] [config] multi-loss-type: sum
[2019-02-01 07:08:59] [config] multi-node: false
[2019-02-01 07:08:59] [config] multi-node-overlap: true
[2019-02-01 07:08:59] [config] n-best: false
[2019-02-01 07:08:59] [config] no-nccl: false
[2019-02-01 07:08:59] [config] no-reload: false
[2019-02-01 07:08:59] [config] no-restore-corpus: false
[2019-02-01 07:08:59] [config] no-shuffle: false
[2019-02-01 07:08:59] [config] normalize: 1
[2019-02-01 07:08:59] [config] optimizer: adam
[2019-02-01 07:08:59] [config] optimizer-delay: 1
[2019-02-01 07:08:59] [config] optimizer-params:
[2019-02-01 07:08:59] [config]   - 0.9
[2019-02-01 07:08:59] [config]   - 0.98
[2019-02-01 07:08:59] [config]   - 1e-09
[2019-02-01 07:08:59] [config] overwrite: true
[2019-02-01 07:08:59] [config] quiet: false
[2019-02-01 07:08:59] [config] quiet-translation: true
[2019-02-01 07:08:59] [config] relative-paths: false
[2019-02-01 07:08:59] [config] right-left: false
[2019-02-01 07:08:59] [config] save-freq: 5000
[2019-02-01 07:08:59] [config] seed: 0
[2019-02-01 07:08:59] [config] sentencepiece-alphas:
[2019-02-01 07:08:59] [config]   - 0.2
[2019-02-01 07:08:59] [config]   - 0
[2019-02-01 07:08:59] [config] sentencepiece-max-lines: 10000000
[2019-02-01 07:08:59] [config] sentencepiece-options: ""
[2019-02-01 07:08:59] [config] shuffle-in-ram: false
[2019-02-01 07:08:59] [config] skip: false
[2019-02-01 07:08:59] [config] sqlite: ""
[2019-02-01 07:08:59] [config] sqlite-drop: false
[2019-02-01 07:08:59] [config] sync-sgd: true
[2019-02-01 07:08:59] [config] tempdir: /tmp
[2019-02-01 07:08:59] [config] tied-embeddings: false
[2019-02-01 07:08:59] [config] tied-embeddings-all: true
[2019-02-01 07:08:59] [config] tied-embeddings-src: false
[2019-02-01 07:08:59] [config] train-sets:
[2019-02-01 07:08:59] [config]   - ../data/corpus01.nrm.en.gz
[2019-02-01 07:08:59] [config]   - ../data/corpus01.nrm.cs.gz
[2019-02-01 07:08:59] [config] transformer-aan-activation: swish
[2019-02-01 07:08:59] [config] transformer-aan-depth: 2
[2019-02-01 07:08:59] [config] transformer-aan-nogate: false
[2019-02-01 07:08:59] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 07:08:59] [config] transformer-dim-aan: 2048
[2019-02-01 07:08:59] [config] transformer-dim-ffn: 2048
[2019-02-01 07:08:59] [config] transformer-dropout: 0.1
[2019-02-01 07:08:59] [config] transformer-dropout-attention: 0
[2019-02-01 07:08:59] [config] transformer-dropout-ffn: 0
[2019-02-01 07:08:59] [config] transformer-ffn-activation: swish
[2019-02-01 07:08:59] [config] transformer-ffn-depth: 2
[2019-02-01 07:08:59] [config] transformer-guided-alignment-layer: last
[2019-02-01 07:08:59] [config] transformer-heads: 8
[2019-02-01 07:08:59] [config] transformer-no-projection: false
[2019-02-01 07:08:59] [config] transformer-postprocess: da
[2019-02-01 07:08:59] [config] transformer-postprocess-emb: d
[2019-02-01 07:08:59] [config] transformer-preprocess: n
[2019-02-01 07:08:59] [config] transformer-tied-layers:
[2019-02-01 07:08:59] [config]   []
[2019-02-01 07:08:59] [config] transformer-train-positions: false
[2019-02-01 07:08:59] [config] type: transformer
[2019-02-01 07:08:59] [config] ulr: false
[2019-02-01 07:08:59] [config] ulr-dim-emb: 0
[2019-02-01 07:08:59] [config] ulr-dropout: 0
[2019-02-01 07:08:59] [config] ulr-keys-vectors: ""
[2019-02-01 07:08:59] [config] ulr-query-vectors: ""
[2019-02-01 07:08:59] [config] ulr-softmax-temperature: 1
[2019-02-01 07:08:59] [config] ulr-trainable-transformation: false
[2019-02-01 07:08:59] [config] valid-freq: 5000
[2019-02-01 07:08:59] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-02-01 07:08:59] [config] valid-max-length: 1000
[2019-02-01 07:08:59] [config] valid-metrics:
[2019-02-01 07:08:59] [config]   - ce-mean-words
[2019-02-01 07:08:59] [config]   - bleu-detok
[2019-02-01 07:08:59] [config] valid-mini-batch: 32
[2019-02-01 07:08:59] [config] valid-sets:
[2019-02-01 07:08:59] [config]   - ../data/newstest2016.en
[2019-02-01 07:08:59] [config]   - ../data/newstest2016.cs
[2019-02-01 07:08:59] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-02-01 07:08:59] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 07:08:59] [config] vocabs:
[2019-02-01 07:08:59] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 07:08:59] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 07:08:59] [config] word-penalty: 0
[2019-02-01 07:08:59] [config] workspace: 10000
[2019-02-01 07:08:59] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 07:08:59] Using synchronous training
[2019-02-01 07:08:59] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 07:08:59] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 07:08:59] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 07:08:59] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 07:08:59] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 07:08:59] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 07:09:00] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 07:09:01] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 07:09:01] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 07:09:02] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 07:09:02] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 07:09:02] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 07:09:02] [training] Using 4 GPUs
[2019-02-01 07:09:02] [memory] Reserving 230 MB, device gpu0
[2019-02-01 07:09:02] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 07:09:02] [memory] Reserving 230 MB, device gpu0
[2019-02-01 07:09:23] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 07:09:23] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 07:09:24] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 07:09:24] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 07:09:24] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 07:09:24] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 07:09:24] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 07:09:24] [training] Using 4 GPUs
[2019-02-01 07:09:24] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 07:09:24] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 07:09:25] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 07:09:25] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 07:09:25] Loading Adam parameters from ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 07:09:26] [memory] Reserving 115 MB, device gpu0
[2019-02-01 07:09:26] [memory] Reserving 115 MB, device gpu1
[2019-02-01 07:09:26] [memory] Reserving 115 MB, device gpu2
[2019-02-01 07:09:26] [memory] Reserving 115 MB, device gpu3
[2019-02-01 07:09:26] [training] Model reloaded from ./models/encs/00.baseline/model.npz
[2019-02-01 07:09:26] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 07:09:26] [data] Shuffling data
[2019-02-01 07:10:30] [data] Done reading 63898897 sentences
[2019-02-01 07:12:40] [data] Done shuffling 63898897 sentences to temp files
[2019-02-01 07:17:29] Training started
[2019-02-01 07:17:29] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu2
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu3
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu0
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu1
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu3
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu2
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu0
[2019-02-01 07:17:29] [memory] Reserving 230 MB, device gpu1
[2019-02-01 07:17:29] Loading model from ./models/encs/00.baseline/model.npz
[2019-02-01 07:17:30] [memory] Reserving 230 MB, device cpu0
[2019-02-01 07:17:30] [memory] Reserving 57 MB, device gpu0
[2019-02-01 07:17:30] [memory] Reserving 57 MB, device gpu1
[2019-02-01 07:17:30] [memory] Reserving 57 MB, device gpu2
[2019-02-01 07:17:30] [memory] Reserving 57 MB, device gpu3
[2019-02-01 09:34:09] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:34:09] [marian] Running on gpu-e-87 as process 108374 with command line:
[2019-02-01 09:34:09] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus01.nrm.en.gz ../data/corpus01.nrm.cs.gz --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --sentencepiece-alphas 0.2 0 --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-02-01 09:34:10] [config] after-batches: 0
[2019-02-01 09:34:10] [config] after-epochs: 0
[2019-02-01 09:34:10] [config] allow-unk: false
[2019-02-01 09:34:10] [config] beam-size: 8
[2019-02-01 09:34:10] [config] bert-class-symbol: "[CLS]"
[2019-02-01 09:34:10] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 09:34:10] [config] bert-masking-fraction: 0.15
[2019-02-01 09:34:10] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 09:34:10] [config] best-deep: false
[2019-02-01 09:34:10] [config] clip-gemm: 0
[2019-02-01 09:34:10] [config] clip-norm: 5
[2019-02-01 09:34:10] [config] cost-type: ce-mean
[2019-02-01 09:34:10] [config] cpu-threads: 0
[2019-02-01 09:34:10] [config] data-weighting-type: sentence
[2019-02-01 09:34:10] [config] dec-cell: gru
[2019-02-01 09:34:10] [config] dec-cell-base-depth: 2
[2019-02-01 09:34:10] [config] dec-cell-high-depth: 1
[2019-02-01 09:34:10] [config] dec-depth: 6
[2019-02-01 09:34:10] [config] devices:
[2019-02-01 09:34:10] [config]   - 0
[2019-02-01 09:34:10] [config]   - 1
[2019-02-01 09:34:10] [config]   - 2
[2019-02-01 09:34:10] [config]   - 3
[2019-02-01 09:34:10] [config] dim-emb: 512
[2019-02-01 09:34:10] [config] dim-rnn: 1024
[2019-02-01 09:34:10] [config] dim-vocabs:
[2019-02-01 09:34:10] [config]   - 32000
[2019-02-01 09:34:10] [config]   - 32000
[2019-02-01 09:34:10] [config] disp-first: 0
[2019-02-01 09:34:10] [config] disp-freq: 1000
[2019-02-01 09:34:10] [config] disp-label-counts: false
[2019-02-01 09:34:10] [config] dropout-rnn: 0
[2019-02-01 09:34:10] [config] dropout-src: 0
[2019-02-01 09:34:10] [config] dropout-trg: 0
[2019-02-01 09:34:10] [config] early-stopping: 5
[2019-02-01 09:34:10] [config] embedding-fix-src: false
[2019-02-01 09:34:10] [config] embedding-fix-trg: false
[2019-02-01 09:34:10] [config] embedding-normalization: false
[2019-02-01 09:34:10] [config] enc-cell: gru
[2019-02-01 09:34:10] [config] enc-cell-depth: 1
[2019-02-01 09:34:10] [config] enc-depth: 6
[2019-02-01 09:34:10] [config] enc-type: bidirectional
[2019-02-01 09:34:10] [config] exponential-smoothing: 0.0001
[2019-02-01 09:34:10] [config] grad-dropping-momentum: 0
[2019-02-01 09:34:10] [config] grad-dropping-rate: 0
[2019-02-01 09:34:10] [config] grad-dropping-warmup: 100
[2019-02-01 09:34:10] [config] guided-alignment: none
[2019-02-01 09:34:10] [config] guided-alignment-cost: mse
[2019-02-01 09:34:10] [config] guided-alignment-weight: 0.1
[2019-02-01 09:34:10] [config] ignore-model-config: false
[2019-02-01 09:34:10] [config] input-types:
[2019-02-01 09:34:10] [config]   []
[2019-02-01 09:34:10] [config] interpolate-env-vars: false
[2019-02-01 09:34:10] [config] keep-best: true
[2019-02-01 09:34:10] [config] label-smoothing: 0.1
[2019-02-01 09:34:10] [config] layer-normalization: true
[2019-02-01 09:34:10] [config] learn-rate: 0.0003
[2019-02-01 09:34:10] [config] log: ./models/encs/00.baseline/train.log
[2019-02-01 09:34:10] [config] log-level: info
[2019-02-01 09:34:10] [config] lr-decay: 0
[2019-02-01 09:34:10] [config] lr-decay-freq: 50000
[2019-02-01 09:34:10] [config] lr-decay-inv-sqrt:
[2019-02-01 09:34:10] [config]   - 16000
[2019-02-01 09:34:10] [config] lr-decay-repeat-warmup: false
[2019-02-01 09:34:10] [config] lr-decay-reset-optimizer: false
[2019-02-01 09:34:10] [config] lr-decay-start:
[2019-02-01 09:34:10] [config]   - 10
[2019-02-01 09:34:10] [config]   - 1
[2019-02-01 09:34:10] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 09:34:10] [config] lr-report: true
[2019-02-01 09:34:10] [config] lr-warmup: 16000
[2019-02-01 09:34:10] [config] lr-warmup-at-reload: false
[2019-02-01 09:34:10] [config] lr-warmup-cycle: false
[2019-02-01 09:34:10] [config] lr-warmup-start-rate: 0
[2019-02-01 09:34:10] [config] max-length: 120
[2019-02-01 09:34:10] [config] max-length-crop: false
[2019-02-01 09:34:10] [config] max-length-factor: 3
[2019-02-01 09:34:10] [config] maxi-batch: 1000
[2019-02-01 09:34:10] [config] maxi-batch-sort: trg
[2019-02-01 09:34:10] [config] mini-batch: 1000
[2019-02-01 09:34:10] [config] mini-batch-fit: true
[2019-02-01 09:34:10] [config] mini-batch-fit-step: 10
[2019-02-01 09:34:10] [config] mini-batch-overstuff: 1
[2019-02-01 09:34:10] [config] mini-batch-track-lr: false
[2019-02-01 09:34:10] [config] mini-batch-understuff: 1
[2019-02-01 09:34:10] [config] mini-batch-warmup: 0
[2019-02-01 09:34:10] [config] mini-batch-words: 0
[2019-02-01 09:34:10] [config] mini-batch-words-ref: 0
[2019-02-01 09:34:10] [config] model: ./models/encs/00.baseline/model.npz
[2019-02-01 09:34:10] [config] multi-loss-type: sum
[2019-02-01 09:34:10] [config] multi-node: false
[2019-02-01 09:34:10] [config] multi-node-overlap: true
[2019-02-01 09:34:10] [config] n-best: false
[2019-02-01 09:34:10] [config] no-nccl: false
[2019-02-01 09:34:10] [config] no-reload: false
[2019-02-01 09:34:10] [config] no-restore-corpus: false
[2019-02-01 09:34:10] [config] no-shuffle: false
[2019-02-01 09:34:10] [config] normalize: 1
[2019-02-01 09:34:10] [config] optimizer: adam
[2019-02-01 09:34:10] [config] optimizer-delay: 1
[2019-02-01 09:34:10] [config] optimizer-params:
[2019-02-01 09:34:10] [config]   - 0.9
[2019-02-01 09:34:10] [config]   - 0.98
[2019-02-01 09:34:10] [config]   - 1e-09
[2019-02-01 09:34:10] [config] overwrite: true
[2019-02-01 09:34:10] [config] quiet: false
[2019-02-01 09:34:10] [config] quiet-translation: true
[2019-02-01 09:34:10] [config] relative-paths: false
[2019-02-01 09:34:10] [config] right-left: false
[2019-02-01 09:34:10] [config] save-freq: 5000
[2019-02-01 09:34:10] [config] seed: 0
[2019-02-01 09:34:10] [config] sentencepiece-alphas:
[2019-02-01 09:34:10] [config]   - 0.2
[2019-02-01 09:34:10] [config]   - 0
[2019-02-01 09:34:10] [config] sentencepiece-max-lines: 10000000
[2019-02-01 09:34:10] [config] sentencepiece-options: ""
[2019-02-01 09:34:10] [config] shuffle-in-ram: false
[2019-02-01 09:34:10] [config] skip: false
[2019-02-01 09:34:10] [config] sqlite: ""
[2019-02-01 09:34:10] [config] sqlite-drop: false
[2019-02-01 09:34:10] [config] sync-sgd: true
[2019-02-01 09:34:10] [config] tempdir: /tmp
[2019-02-01 09:34:10] [config] tied-embeddings: false
[2019-02-01 09:34:10] [config] tied-embeddings-all: true
[2019-02-01 09:34:10] [config] tied-embeddings-src: false
[2019-02-01 09:34:10] [config] train-sets:
[2019-02-01 09:34:10] [config]   - ../data/corpus01.nrm.en.gz
[2019-02-01 09:34:10] [config]   - ../data/corpus01.nrm.cs.gz
[2019-02-01 09:34:10] [config] transformer-aan-activation: swish
[2019-02-01 09:34:10] [config] transformer-aan-depth: 2
[2019-02-01 09:34:10] [config] transformer-aan-nogate: false
[2019-02-01 09:34:10] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 09:34:10] [config] transformer-dim-aan: 2048
[2019-02-01 09:34:10] [config] transformer-dim-ffn: 2048
[2019-02-01 09:34:10] [config] transformer-dropout: 0.1
[2019-02-01 09:34:10] [config] transformer-dropout-attention: 0
[2019-02-01 09:34:10] [config] transformer-dropout-ffn: 0
[2019-02-01 09:34:10] [config] transformer-ffn-activation: swish
[2019-02-01 09:34:10] [config] transformer-ffn-depth: 2
[2019-02-01 09:34:10] [config] transformer-guided-alignment-layer: last
[2019-02-01 09:34:10] [config] transformer-heads: 8
[2019-02-01 09:34:10] [config] transformer-no-projection: false
[2019-02-01 09:34:10] [config] transformer-postprocess: da
[2019-02-01 09:34:10] [config] transformer-postprocess-emb: d
[2019-02-01 09:34:10] [config] transformer-preprocess: n
[2019-02-01 09:34:10] [config] transformer-tied-layers:
[2019-02-01 09:34:10] [config]   []
[2019-02-01 09:34:10] [config] transformer-train-positions: false
[2019-02-01 09:34:10] [config] type: transformer
[2019-02-01 09:34:10] [config] ulr: false
[2019-02-01 09:34:10] [config] ulr-dim-emb: 0
[2019-02-01 09:34:10] [config] ulr-dropout: 0
[2019-02-01 09:34:10] [config] ulr-keys-vectors: ""
[2019-02-01 09:34:10] [config] ulr-query-vectors: ""
[2019-02-01 09:34:10] [config] ulr-softmax-temperature: 1
[2019-02-01 09:34:10] [config] ulr-trainable-transformation: false
[2019-02-01 09:34:10] [config] valid-freq: 5000
[2019-02-01 09:34:10] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-02-01 09:34:10] [config] valid-max-length: 1000
[2019-02-01 09:34:10] [config] valid-metrics:
[2019-02-01 09:34:10] [config]   - ce-mean-words
[2019-02-01 09:34:10] [config]   - bleu-detok
[2019-02-01 09:34:10] [config] valid-mini-batch: 32
[2019-02-01 09:34:10] [config] valid-sets:
[2019-02-01 09:34:10] [config]   - ../data/newstest2016.en
[2019-02-01 09:34:10] [config]   - ../data/newstest2016.cs
[2019-02-01 09:34:10] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-02-01 09:34:10] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:34:10] [config] vocabs:
[2019-02-01 09:34:10] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:34:10] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:34:10] [config] word-penalty: 0
[2019-02-01 09:34:10] [config] workspace: 10000
[2019-02-01 09:34:10] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:34:10] Using synchronous training
[2019-02-01 09:34:10] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:34:10] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 09:34:10] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:34:10] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 09:34:10] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 09:34:10] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 09:34:11] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 09:34:12] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 09:34:12] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 09:34:13] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 09:34:13] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 09:34:13] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 09:34:13] [training] Using 4 GPUs
[2019-02-01 09:34:13] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:34:13] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 09:34:13] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:34:34] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 09:34:34] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 09:34:34] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 09:34:34] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 09:34:34] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 09:34:34] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 09:34:35] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 09:34:35] [training] Using 4 GPUs
[2019-02-01 09:34:35] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:34:35] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:34:36] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:34:36] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:34:36] Loading Adam parameters from ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 09:34:37] [memory] Reserving 115 MB, device gpu0
[2019-02-01 09:34:37] [memory] Reserving 115 MB, device gpu1
[2019-02-01 09:34:37] [memory] Reserving 115 MB, device gpu2
[2019-02-01 09:34:37] [memory] Reserving 115 MB, device gpu3
[2019-02-01 09:34:37] [training] Model reloaded from ./models/encs/00.baseline/model.npz
[2019-02-01 09:34:37] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 09:34:37] [data] Shuffling data
[2019-02-01 09:35:41] [data] Done reading 63898897 sentences
[2019-02-01 09:37:52] [data] Done shuffling 63898897 sentences to temp files
[2019-02-01 09:42:42] Training started
[2019-02-01 09:42:42] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu3
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu2
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu1
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu3
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu2
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:42:42] [memory] Reserving 230 MB, device gpu1
[2019-02-01 09:42:42] Loading model from ./models/encs/00.baseline/model.npz
[2019-02-01 09:42:43] [memory] Reserving 230 MB, device cpu0
[2019-02-01 09:42:43] [memory] Reserving 57 MB, device gpu0
[2019-02-01 09:42:43] [memory] Reserving 57 MB, device gpu1
[2019-02-01 09:42:43] [memory] Reserving 57 MB, device gpu2
[2019-02-01 09:42:43] [memory] Reserving 57 MB, device gpu3
[2019-02-01 09:43:21] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:43:21] [marian] Running on gpu-e-87 as process 110135 with command line:
[2019-02-01 09:43:21] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus01.nrm.en.gz ../data/corpus01.nrm.cs.gz --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --sentencepiece-alphas 0.2 0 --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-02-01 09:43:21] [config] after-batches: 0
[2019-02-01 09:43:21] [config] after-epochs: 0
[2019-02-01 09:43:21] [config] allow-unk: false
[2019-02-01 09:43:21] [config] beam-size: 8
[2019-02-01 09:43:21] [config] bert-class-symbol: "[CLS]"
[2019-02-01 09:43:21] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 09:43:21] [config] bert-masking-fraction: 0.15
[2019-02-01 09:43:21] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 09:43:21] [config] best-deep: false
[2019-02-01 09:43:21] [config] clip-gemm: 0
[2019-02-01 09:43:21] [config] clip-norm: 5
[2019-02-01 09:43:21] [config] cost-type: ce-mean
[2019-02-01 09:43:21] [config] cpu-threads: 0
[2019-02-01 09:43:21] [config] data-weighting-type: sentence
[2019-02-01 09:43:21] [config] dec-cell: gru
[2019-02-01 09:43:21] [config] dec-cell-base-depth: 2
[2019-02-01 09:43:21] [config] dec-cell-high-depth: 1
[2019-02-01 09:43:21] [config] dec-depth: 6
[2019-02-01 09:43:21] [config] devices:
[2019-02-01 09:43:21] [config]   - 0
[2019-02-01 09:43:21] [config]   - 1
[2019-02-01 09:43:21] [config]   - 2
[2019-02-01 09:43:21] [config]   - 3
[2019-02-01 09:43:21] [config] dim-emb: 512
[2019-02-01 09:43:21] [config] dim-rnn: 1024
[2019-02-01 09:43:21] [config] dim-vocabs:
[2019-02-01 09:43:21] [config]   - 32000
[2019-02-01 09:43:21] [config]   - 32000
[2019-02-01 09:43:21] [config] disp-first: 0
[2019-02-01 09:43:21] [config] disp-freq: 1000
[2019-02-01 09:43:21] [config] disp-label-counts: false
[2019-02-01 09:43:21] [config] dropout-rnn: 0
[2019-02-01 09:43:21] [config] dropout-src: 0
[2019-02-01 09:43:21] [config] dropout-trg: 0
[2019-02-01 09:43:21] [config] early-stopping: 5
[2019-02-01 09:43:21] [config] embedding-fix-src: false
[2019-02-01 09:43:21] [config] embedding-fix-trg: false
[2019-02-01 09:43:21] [config] embedding-normalization: false
[2019-02-01 09:43:21] [config] enc-cell: gru
[2019-02-01 09:43:21] [config] enc-cell-depth: 1
[2019-02-01 09:43:21] [config] enc-depth: 6
[2019-02-01 09:43:21] [config] enc-type: bidirectional
[2019-02-01 09:43:21] [config] exponential-smoothing: 0.0001
[2019-02-01 09:43:21] [config] grad-dropping-momentum: 0
[2019-02-01 09:43:21] [config] grad-dropping-rate: 0
[2019-02-01 09:43:21] [config] grad-dropping-warmup: 100
[2019-02-01 09:43:21] [config] guided-alignment: none
[2019-02-01 09:43:21] [config] guided-alignment-cost: mse
[2019-02-01 09:43:21] [config] guided-alignment-weight: 0.1
[2019-02-01 09:43:21] [config] ignore-model-config: false
[2019-02-01 09:43:21] [config] input-types:
[2019-02-01 09:43:21] [config]   []
[2019-02-01 09:43:21] [config] interpolate-env-vars: false
[2019-02-01 09:43:21] [config] keep-best: true
[2019-02-01 09:43:21] [config] label-smoothing: 0.1
[2019-02-01 09:43:21] [config] layer-normalization: true
[2019-02-01 09:43:21] [config] learn-rate: 0.0003
[2019-02-01 09:43:21] [config] log: ./models/encs/00.baseline/train.log
[2019-02-01 09:43:21] [config] log-level: info
[2019-02-01 09:43:21] [config] lr-decay: 0
[2019-02-01 09:43:21] [config] lr-decay-freq: 50000
[2019-02-01 09:43:21] [config] lr-decay-inv-sqrt:
[2019-02-01 09:43:21] [config]   - 16000
[2019-02-01 09:43:21] [config] lr-decay-repeat-warmup: false
[2019-02-01 09:43:21] [config] lr-decay-reset-optimizer: false
[2019-02-01 09:43:21] [config] lr-decay-start:
[2019-02-01 09:43:21] [config]   - 10
[2019-02-01 09:43:21] [config]   - 1
[2019-02-01 09:43:21] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 09:43:21] [config] lr-report: true
[2019-02-01 09:43:21] [config] lr-warmup: 16000
[2019-02-01 09:43:21] [config] lr-warmup-at-reload: false
[2019-02-01 09:43:21] [config] lr-warmup-cycle: false
[2019-02-01 09:43:21] [config] lr-warmup-start-rate: 0
[2019-02-01 09:43:21] [config] max-length: 120
[2019-02-01 09:43:21] [config] max-length-crop: false
[2019-02-01 09:43:21] [config] max-length-factor: 3
[2019-02-01 09:43:21] [config] maxi-batch: 1000
[2019-02-01 09:43:21] [config] maxi-batch-sort: trg
[2019-02-01 09:43:21] [config] mini-batch: 1000
[2019-02-01 09:43:21] [config] mini-batch-fit: true
[2019-02-01 09:43:21] [config] mini-batch-fit-step: 10
[2019-02-01 09:43:21] [config] mini-batch-overstuff: 1
[2019-02-01 09:43:21] [config] mini-batch-track-lr: false
[2019-02-01 09:43:21] [config] mini-batch-understuff: 1
[2019-02-01 09:43:21] [config] mini-batch-warmup: 0
[2019-02-01 09:43:21] [config] mini-batch-words: 0
[2019-02-01 09:43:21] [config] mini-batch-words-ref: 0
[2019-02-01 09:43:21] [config] model: ./models/encs/00.baseline/model.npz
[2019-02-01 09:43:21] [config] multi-loss-type: sum
[2019-02-01 09:43:21] [config] multi-node: false
[2019-02-01 09:43:21] [config] multi-node-overlap: true
[2019-02-01 09:43:21] [config] n-best: false
[2019-02-01 09:43:21] [config] no-nccl: false
[2019-02-01 09:43:21] [config] no-reload: false
[2019-02-01 09:43:21] [config] no-restore-corpus: false
[2019-02-01 09:43:21] [config] no-shuffle: false
[2019-02-01 09:43:21] [config] normalize: 1
[2019-02-01 09:43:21] [config] optimizer: adam
[2019-02-01 09:43:21] [config] optimizer-delay: 1
[2019-02-01 09:43:21] [config] optimizer-params:
[2019-02-01 09:43:21] [config]   - 0.9
[2019-02-01 09:43:21] [config]   - 0.98
[2019-02-01 09:43:21] [config]   - 1e-09
[2019-02-01 09:43:21] [config] overwrite: true
[2019-02-01 09:43:21] [config] quiet: false
[2019-02-01 09:43:21] [config] quiet-translation: true
[2019-02-01 09:43:21] [config] relative-paths: false
[2019-02-01 09:43:21] [config] right-left: false
[2019-02-01 09:43:21] [config] save-freq: 5000
[2019-02-01 09:43:21] [config] seed: 0
[2019-02-01 09:43:21] [config] sentencepiece-alphas:
[2019-02-01 09:43:21] [config]   - 0.2
[2019-02-01 09:43:21] [config]   - 0
[2019-02-01 09:43:21] [config] sentencepiece-max-lines: 10000000
[2019-02-01 09:43:21] [config] sentencepiece-options: ""
[2019-02-01 09:43:21] [config] shuffle-in-ram: false
[2019-02-01 09:43:21] [config] skip: false
[2019-02-01 09:43:21] [config] sqlite: ""
[2019-02-01 09:43:21] [config] sqlite-drop: false
[2019-02-01 09:43:21] [config] sync-sgd: true
[2019-02-01 09:43:21] [config] tempdir: /tmp
[2019-02-01 09:43:21] [config] tied-embeddings: false
[2019-02-01 09:43:21] [config] tied-embeddings-all: true
[2019-02-01 09:43:21] [config] tied-embeddings-src: false
[2019-02-01 09:43:21] [config] train-sets:
[2019-02-01 09:43:21] [config]   - ../data/corpus01.nrm.en.gz
[2019-02-01 09:43:21] [config]   - ../data/corpus01.nrm.cs.gz
[2019-02-01 09:43:21] [config] transformer-aan-activation: swish
[2019-02-01 09:43:21] [config] transformer-aan-depth: 2
[2019-02-01 09:43:21] [config] transformer-aan-nogate: false
[2019-02-01 09:43:21] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 09:43:21] [config] transformer-dim-aan: 2048
[2019-02-01 09:43:21] [config] transformer-dim-ffn: 2048
[2019-02-01 09:43:21] [config] transformer-dropout: 0.1
[2019-02-01 09:43:21] [config] transformer-dropout-attention: 0
[2019-02-01 09:43:21] [config] transformer-dropout-ffn: 0
[2019-02-01 09:43:21] [config] transformer-ffn-activation: swish
[2019-02-01 09:43:21] [config] transformer-ffn-depth: 2
[2019-02-01 09:43:21] [config] transformer-guided-alignment-layer: last
[2019-02-01 09:43:21] [config] transformer-heads: 8
[2019-02-01 09:43:21] [config] transformer-no-projection: false
[2019-02-01 09:43:21] [config] transformer-postprocess: da
[2019-02-01 09:43:21] [config] transformer-postprocess-emb: d
[2019-02-01 09:43:21] [config] transformer-preprocess: n
[2019-02-01 09:43:21] [config] transformer-tied-layers:
[2019-02-01 09:43:21] [config]   []
[2019-02-01 09:43:21] [config] transformer-train-positions: false
[2019-02-01 09:43:21] [config] type: transformer
[2019-02-01 09:43:21] [config] ulr: false
[2019-02-01 09:43:21] [config] ulr-dim-emb: 0
[2019-02-01 09:43:21] [config] ulr-dropout: 0
[2019-02-01 09:43:21] [config] ulr-keys-vectors: ""
[2019-02-01 09:43:21] [config] ulr-query-vectors: ""
[2019-02-01 09:43:21] [config] ulr-softmax-temperature: 1
[2019-02-01 09:43:21] [config] ulr-trainable-transformation: false
[2019-02-01 09:43:21] [config] valid-freq: 5000
[2019-02-01 09:43:21] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-02-01 09:43:21] [config] valid-max-length: 1000
[2019-02-01 09:43:21] [config] valid-metrics:
[2019-02-01 09:43:21] [config]   - ce-mean-words
[2019-02-01 09:43:21] [config]   - bleu-detok
[2019-02-01 09:43:21] [config] valid-mini-batch: 32
[2019-02-01 09:43:21] [config] valid-sets:
[2019-02-01 09:43:21] [config]   - ../data/newstest2016.en
[2019-02-01 09:43:21] [config]   - ../data/newstest2016.cs
[2019-02-01 09:43:21] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-02-01 09:43:21] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:43:21] [config] vocabs:
[2019-02-01 09:43:21] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:43:21] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:43:21] [config] word-penalty: 0
[2019-02-01 09:43:21] [config] workspace: 10000
[2019-02-01 09:43:21] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:43:21] Using synchronous training
[2019-02-01 09:43:21] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:43:21] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 09:43:21] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:43:21] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 09:43:21] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 09:43:21] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 09:43:22] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 09:43:23] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 09:43:23] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 09:43:24] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 09:43:24] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 09:43:24] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 09:43:24] [training] Using 4 GPUs
[2019-02-01 09:43:24] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:43:24] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 09:43:24] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:43:45] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 09:43:45] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 09:43:45] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 09:43:45] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 09:43:45] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 09:43:45] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 09:43:45] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 09:43:45] [training] Using 4 GPUs
[2019-02-01 09:43:45] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:43:46] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:43:46] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:43:46] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:43:47] Loading Adam parameters from ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 09:43:47] [memory] Reserving 115 MB, device gpu0
[2019-02-01 09:43:47] [memory] Reserving 115 MB, device gpu1
[2019-02-01 09:43:47] [memory] Reserving 115 MB, device gpu2
[2019-02-01 09:43:47] [memory] Reserving 115 MB, device gpu3
[2019-02-01 09:43:47] [training] Model reloaded from ./models/encs/00.baseline/model.npz
[2019-02-01 09:43:47] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 09:43:47] [data] Shuffling data
[2019-02-01 09:44:51] [data] Done reading 63898897 sentences
[2019-02-01 09:47:02] [data] Done shuffling 63898897 sentences to temp files
[2019-02-01 09:51:53] Training started
[2019-02-01 09:51:53] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-01 09:51:53] [memory] Reserving 230 MB, device gpu2
[2019-02-01 09:51:53] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:51:53] [memory] Reserving 230 MB, device gpu3
[2019-02-01 09:51:53] [memory] Reserving 230 MB, device gpu1
[2019-02-01 09:51:54] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:51:54] [memory] Reserving 230 MB, device gpu2
[2019-02-01 09:51:54] [memory] Reserving 230 MB, device gpu3
[2019-02-01 09:51:54] [memory] Reserving 230 MB, device gpu1
[2019-02-01 09:51:54] Loading model from ./models/encs/00.baseline/model.npz
[2019-02-01 09:51:54] [memory] Reserving 230 MB, device cpu0
[2019-02-01 09:51:54] [memory] Reserving 57 MB, device gpu0
[2019-02-01 09:51:54] [memory] Reserving 57 MB, device gpu1
[2019-02-01 09:51:54] [memory] Reserving 57 MB, device gpu2
[2019-02-01 09:51:54] [memory] Reserving 57 MB, device gpu3
[2019-02-01 09:52:22] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:52:22] [marian] Running on gpu-e-87 as process 111757 with command line:
[2019-02-01 09:52:22] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus01.nrm.en.gz ../data/corpus01.nrm.cs.gz --shuffle-in-ram --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --sentencepiece-alphas 0.2 0 --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-02-01 09:52:22] [config] after-batches: 0
[2019-02-01 09:52:22] [config] after-epochs: 0
[2019-02-01 09:52:22] [config] allow-unk: false
[2019-02-01 09:52:22] [config] beam-size: 8
[2019-02-01 09:52:22] [config] bert-class-symbol: "[CLS]"
[2019-02-01 09:52:22] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 09:52:22] [config] bert-masking-fraction: 0.15
[2019-02-01 09:52:22] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 09:52:22] [config] best-deep: false
[2019-02-01 09:52:22] [config] clip-gemm: 0
[2019-02-01 09:52:22] [config] clip-norm: 5
[2019-02-01 09:52:22] [config] cost-type: ce-mean
[2019-02-01 09:52:22] [config] cpu-threads: 0
[2019-02-01 09:52:22] [config] data-weighting-type: sentence
[2019-02-01 09:52:22] [config] dec-cell: gru
[2019-02-01 09:52:22] [config] dec-cell-base-depth: 2
[2019-02-01 09:52:22] [config] dec-cell-high-depth: 1
[2019-02-01 09:52:22] [config] dec-depth: 6
[2019-02-01 09:52:22] [config] devices:
[2019-02-01 09:52:22] [config]   - 0
[2019-02-01 09:52:22] [config]   - 1
[2019-02-01 09:52:22] [config]   - 2
[2019-02-01 09:52:22] [config]   - 3
[2019-02-01 09:52:22] [config] dim-emb: 512
[2019-02-01 09:52:22] [config] dim-rnn: 1024
[2019-02-01 09:52:22] [config] dim-vocabs:
[2019-02-01 09:52:22] [config]   - 32000
[2019-02-01 09:52:22] [config]   - 32000
[2019-02-01 09:52:22] [config] disp-first: 0
[2019-02-01 09:52:22] [config] disp-freq: 1000
[2019-02-01 09:52:22] [config] disp-label-counts: false
[2019-02-01 09:52:22] [config] dropout-rnn: 0
[2019-02-01 09:52:22] [config] dropout-src: 0
[2019-02-01 09:52:22] [config] dropout-trg: 0
[2019-02-01 09:52:22] [config] early-stopping: 5
[2019-02-01 09:52:22] [config] embedding-fix-src: false
[2019-02-01 09:52:22] [config] embedding-fix-trg: false
[2019-02-01 09:52:22] [config] embedding-normalization: false
[2019-02-01 09:52:22] [config] enc-cell: gru
[2019-02-01 09:52:22] [config] enc-cell-depth: 1
[2019-02-01 09:52:22] [config] enc-depth: 6
[2019-02-01 09:52:22] [config] enc-type: bidirectional
[2019-02-01 09:52:22] [config] exponential-smoothing: 0.0001
[2019-02-01 09:52:22] [config] grad-dropping-momentum: 0
[2019-02-01 09:52:22] [config] grad-dropping-rate: 0
[2019-02-01 09:52:22] [config] grad-dropping-warmup: 100
[2019-02-01 09:52:22] [config] guided-alignment: none
[2019-02-01 09:52:22] [config] guided-alignment-cost: mse
[2019-02-01 09:52:22] [config] guided-alignment-weight: 0.1
[2019-02-01 09:52:22] [config] ignore-model-config: false
[2019-02-01 09:52:22] [config] input-types:
[2019-02-01 09:52:22] [config]   []
[2019-02-01 09:52:22] [config] interpolate-env-vars: false
[2019-02-01 09:52:22] [config] keep-best: true
[2019-02-01 09:52:22] [config] label-smoothing: 0.1
[2019-02-01 09:52:22] [config] layer-normalization: true
[2019-02-01 09:52:22] [config] learn-rate: 0.0003
[2019-02-01 09:52:22] [config] log: ./models/encs/00.baseline/train.log
[2019-02-01 09:52:22] [config] log-level: info
[2019-02-01 09:52:22] [config] lr-decay: 0
[2019-02-01 09:52:22] [config] lr-decay-freq: 50000
[2019-02-01 09:52:22] [config] lr-decay-inv-sqrt:
[2019-02-01 09:52:22] [config]   - 16000
[2019-02-01 09:52:22] [config] lr-decay-repeat-warmup: false
[2019-02-01 09:52:22] [config] lr-decay-reset-optimizer: false
[2019-02-01 09:52:22] [config] lr-decay-start:
[2019-02-01 09:52:22] [config]   - 10
[2019-02-01 09:52:22] [config]   - 1
[2019-02-01 09:52:22] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 09:52:22] [config] lr-report: true
[2019-02-01 09:52:22] [config] lr-warmup: 16000
[2019-02-01 09:52:22] [config] lr-warmup-at-reload: false
[2019-02-01 09:52:22] [config] lr-warmup-cycle: false
[2019-02-01 09:52:22] [config] lr-warmup-start-rate: 0
[2019-02-01 09:52:22] [config] max-length: 120
[2019-02-01 09:52:22] [config] max-length-crop: false
[2019-02-01 09:52:22] [config] max-length-factor: 3
[2019-02-01 09:52:22] [config] maxi-batch: 1000
[2019-02-01 09:52:22] [config] maxi-batch-sort: trg
[2019-02-01 09:52:22] [config] mini-batch: 1000
[2019-02-01 09:52:22] [config] mini-batch-fit: true
[2019-02-01 09:52:22] [config] mini-batch-fit-step: 10
[2019-02-01 09:52:22] [config] mini-batch-overstuff: 1
[2019-02-01 09:52:22] [config] mini-batch-track-lr: false
[2019-02-01 09:52:22] [config] mini-batch-understuff: 1
[2019-02-01 09:52:22] [config] mini-batch-warmup: 0
[2019-02-01 09:52:22] [config] mini-batch-words: 0
[2019-02-01 09:52:22] [config] mini-batch-words-ref: 0
[2019-02-01 09:52:22] [config] model: ./models/encs/00.baseline/model.npz
[2019-02-01 09:52:22] [config] multi-loss-type: sum
[2019-02-01 09:52:22] [config] multi-node: false
[2019-02-01 09:52:22] [config] multi-node-overlap: true
[2019-02-01 09:52:22] [config] n-best: false
[2019-02-01 09:52:22] [config] no-nccl: false
[2019-02-01 09:52:22] [config] no-reload: false
[2019-02-01 09:52:22] [config] no-restore-corpus: false
[2019-02-01 09:52:22] [config] no-shuffle: false
[2019-02-01 09:52:22] [config] normalize: 1
[2019-02-01 09:52:22] [config] optimizer: adam
[2019-02-01 09:52:22] [config] optimizer-delay: 1
[2019-02-01 09:52:22] [config] optimizer-params:
[2019-02-01 09:52:22] [config]   - 0.9
[2019-02-01 09:52:22] [config]   - 0.98
[2019-02-01 09:52:22] [config]   - 1e-09
[2019-02-01 09:52:22] [config] overwrite: true
[2019-02-01 09:52:22] [config] quiet: false
[2019-02-01 09:52:22] [config] quiet-translation: true
[2019-02-01 09:52:22] [config] relative-paths: false
[2019-02-01 09:52:22] [config] right-left: false
[2019-02-01 09:52:22] [config] save-freq: 5000
[2019-02-01 09:52:22] [config] seed: 0
[2019-02-01 09:52:22] [config] sentencepiece-alphas:
[2019-02-01 09:52:22] [config]   - 0.2
[2019-02-01 09:52:22] [config]   - 0
[2019-02-01 09:52:22] [config] sentencepiece-max-lines: 10000000
[2019-02-01 09:52:22] [config] sentencepiece-options: ""
[2019-02-01 09:52:22] [config] shuffle-in-ram: true
[2019-02-01 09:52:22] [config] skip: false
[2019-02-01 09:52:22] [config] sqlite: ""
[2019-02-01 09:52:22] [config] sqlite-drop: false
[2019-02-01 09:52:22] [config] sync-sgd: true
[2019-02-01 09:52:22] [config] tempdir: /tmp
[2019-02-01 09:52:22] [config] tied-embeddings: false
[2019-02-01 09:52:22] [config] tied-embeddings-all: true
[2019-02-01 09:52:22] [config] tied-embeddings-src: false
[2019-02-01 09:52:22] [config] train-sets:
[2019-02-01 09:52:22] [config]   - ../data/corpus01.nrm.en.gz
[2019-02-01 09:52:22] [config]   - ../data/corpus01.nrm.cs.gz
[2019-02-01 09:52:22] [config] transformer-aan-activation: swish
[2019-02-01 09:52:22] [config] transformer-aan-depth: 2
[2019-02-01 09:52:22] [config] transformer-aan-nogate: false
[2019-02-01 09:52:22] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 09:52:22] [config] transformer-dim-aan: 2048
[2019-02-01 09:52:22] [config] transformer-dim-ffn: 2048
[2019-02-01 09:52:22] [config] transformer-dropout: 0.1
[2019-02-01 09:52:22] [config] transformer-dropout-attention: 0
[2019-02-01 09:52:22] [config] transformer-dropout-ffn: 0
[2019-02-01 09:52:22] [config] transformer-ffn-activation: swish
[2019-02-01 09:52:22] [config] transformer-ffn-depth: 2
[2019-02-01 09:52:22] [config] transformer-guided-alignment-layer: last
[2019-02-01 09:52:22] [config] transformer-heads: 8
[2019-02-01 09:52:22] [config] transformer-no-projection: false
[2019-02-01 09:52:22] [config] transformer-postprocess: da
[2019-02-01 09:52:22] [config] transformer-postprocess-emb: d
[2019-02-01 09:52:22] [config] transformer-preprocess: n
[2019-02-01 09:52:22] [config] transformer-tied-layers:
[2019-02-01 09:52:22] [config]   []
[2019-02-01 09:52:22] [config] transformer-train-positions: false
[2019-02-01 09:52:22] [config] type: transformer
[2019-02-01 09:52:22] [config] ulr: false
[2019-02-01 09:52:22] [config] ulr-dim-emb: 0
[2019-02-01 09:52:22] [config] ulr-dropout: 0
[2019-02-01 09:52:22] [config] ulr-keys-vectors: ""
[2019-02-01 09:52:22] [config] ulr-query-vectors: ""
[2019-02-01 09:52:22] [config] ulr-softmax-temperature: 1
[2019-02-01 09:52:22] [config] ulr-trainable-transformation: false
[2019-02-01 09:52:22] [config] valid-freq: 5000
[2019-02-01 09:52:22] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-02-01 09:52:22] [config] valid-max-length: 1000
[2019-02-01 09:52:22] [config] valid-metrics:
[2019-02-01 09:52:22] [config]   - ce-mean-words
[2019-02-01 09:52:22] [config]   - bleu-detok
[2019-02-01 09:52:22] [config] valid-mini-batch: 32
[2019-02-01 09:52:22] [config] valid-sets:
[2019-02-01 09:52:22] [config]   - ../data/newstest2016.en
[2019-02-01 09:52:22] [config]   - ../data/newstest2016.cs
[2019-02-01 09:52:22] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-02-01 09:52:22] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:52:22] [config] vocabs:
[2019-02-01 09:52:22] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:52:22] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:52:22] [config] word-penalty: 0
[2019-02-01 09:52:22] [config] workspace: 10000
[2019-02-01 09:52:22] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 09:52:22] Using synchronous training
[2019-02-01 09:52:22] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:52:23] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 09:52:23] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 09:52:23] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 09:52:23] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 09:52:23] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 09:52:23] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 09:52:24] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 09:52:25] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 09:52:25] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 09:52:25] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 09:52:25] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 09:52:25] [training] Using 4 GPUs
[2019-02-01 09:52:25] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:52:25] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 09:52:26] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:52:46] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 09:52:47] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 09:52:47] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 09:52:47] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 09:52:47] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 09:52:47] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 09:52:47] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 09:52:47] [training] Using 4 GPUs
[2019-02-01 09:52:47] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:52:47] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:52:48] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:52:48] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 09:52:48] Loading Adam parameters from ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 09:52:49] [memory] Reserving 115 MB, device gpu0
[2019-02-01 09:52:49] [memory] Reserving 115 MB, device gpu1
[2019-02-01 09:52:49] [memory] Reserving 115 MB, device gpu2
[2019-02-01 09:52:49] [memory] Reserving 115 MB, device gpu3
[2019-02-01 09:52:49] [training] Model reloaded from ./models/encs/00.baseline/model.npz
[2019-02-01 09:52:49] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 09:52:49] [data] Shuffling data
[2019-02-01 09:53:53] [data] Done reading 63898897 sentences
[2019-02-01 09:53:56] [data] Done shuffling 63898897 sentences (cached in RAM)
[2019-02-01 09:58:44] Training started
[2019-02-01 09:58:44] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu1
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu3
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu2
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu0
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu1
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu3
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device gpu2
[2019-02-01 09:58:44] Loading model from ./models/encs/00.baseline/model.npz
[2019-02-01 09:58:44] [memory] Reserving 230 MB, device cpu0
[2019-02-01 09:58:44] [memory] Reserving 57 MB, device gpu0
[2019-02-01 09:58:44] [memory] Reserving 57 MB, device gpu1
[2019-02-01 09:58:44] [memory] Reserving 57 MB, device gpu2
[2019-02-01 09:58:44] [memory] Reserving 57 MB, device gpu3
[2019-02-01 10:20:27] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 10:20:27] [marian] Running on gpu-e-71 as process 137355 with command line:
[2019-02-01 10:20:27] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./models/encs/00.baseline/model.npz --type transformer --train-sets ../data/corpus00.nrm.en.gz ../data/corpus00.nrm.cs.gz --shuffle-in-ram --vocabs ./models/encs/00.baseline/vocab.encs.spm ./models/encs/00.baseline/vocab.encs.spm --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./models/encs/00.baseline/devset.bpe.cs.output --quiet-translation --valid-sets ../data/newstest2016.en ../data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./models/encs/00.baseline/train.log --valid-log ./models/encs/00.baseline/valid.log
[2019-02-01 10:20:28] [config] after-batches: 0
[2019-02-01 10:20:28] [config] after-epochs: 0
[2019-02-01 10:20:28] [config] allow-unk: false
[2019-02-01 10:20:28] [config] beam-size: 8
[2019-02-01 10:20:28] [config] bert-class-symbol: "[CLS]"
[2019-02-01 10:20:28] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 10:20:28] [config] bert-masking-fraction: 0.15
[2019-02-01 10:20:28] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 10:20:28] [config] best-deep: false
[2019-02-01 10:20:28] [config] clip-gemm: 0
[2019-02-01 10:20:28] [config] clip-norm: 5
[2019-02-01 10:20:28] [config] cost-type: ce-mean
[2019-02-01 10:20:28] [config] cpu-threads: 0
[2019-02-01 10:20:28] [config] data-weighting-type: sentence
[2019-02-01 10:20:28] [config] dec-cell: gru
[2019-02-01 10:20:28] [config] dec-cell-base-depth: 2
[2019-02-01 10:20:28] [config] dec-cell-high-depth: 1
[2019-02-01 10:20:28] [config] dec-depth: 6
[2019-02-01 10:20:28] [config] devices:
[2019-02-01 10:20:28] [config]   - 0
[2019-02-01 10:20:28] [config]   - 1
[2019-02-01 10:20:28] [config]   - 2
[2019-02-01 10:20:28] [config]   - 3
[2019-02-01 10:20:28] [config] dim-emb: 512
[2019-02-01 10:20:28] [config] dim-rnn: 1024
[2019-02-01 10:20:28] [config] dim-vocabs:
[2019-02-01 10:20:28] [config]   - 32000
[2019-02-01 10:20:28] [config]   - 32000
[2019-02-01 10:20:28] [config] disp-first: 0
[2019-02-01 10:20:28] [config] disp-freq: 1000
[2019-02-01 10:20:28] [config] disp-label-counts: false
[2019-02-01 10:20:28] [config] dropout-rnn: 0
[2019-02-01 10:20:28] [config] dropout-src: 0
[2019-02-01 10:20:28] [config] dropout-trg: 0
[2019-02-01 10:20:28] [config] early-stopping: 5
[2019-02-01 10:20:28] [config] embedding-fix-src: false
[2019-02-01 10:20:28] [config] embedding-fix-trg: false
[2019-02-01 10:20:28] [config] embedding-normalization: false
[2019-02-01 10:20:28] [config] enc-cell: gru
[2019-02-01 10:20:28] [config] enc-cell-depth: 1
[2019-02-01 10:20:28] [config] enc-depth: 6
[2019-02-01 10:20:28] [config] enc-type: bidirectional
[2019-02-01 10:20:28] [config] exponential-smoothing: 0.0001
[2019-02-01 10:20:28] [config] grad-dropping-momentum: 0
[2019-02-01 10:20:28] [config] grad-dropping-rate: 0
[2019-02-01 10:20:28] [config] grad-dropping-warmup: 100
[2019-02-01 10:20:28] [config] guided-alignment: none
[2019-02-01 10:20:28] [config] guided-alignment-cost: mse
[2019-02-01 10:20:28] [config] guided-alignment-weight: 0.1
[2019-02-01 10:20:28] [config] ignore-model-config: false
[2019-02-01 10:20:28] [config] input-types:
[2019-02-01 10:20:28] [config]   []
[2019-02-01 10:20:28] [config] interpolate-env-vars: false
[2019-02-01 10:20:28] [config] keep-best: true
[2019-02-01 10:20:28] [config] label-smoothing: 0.1
[2019-02-01 10:20:28] [config] layer-normalization: true
[2019-02-01 10:20:28] [config] learn-rate: 0.0003
[2019-02-01 10:20:28] [config] log: ./models/encs/00.baseline/train.log
[2019-02-01 10:20:28] [config] log-level: info
[2019-02-01 10:20:28] [config] lr-decay: 0
[2019-02-01 10:20:28] [config] lr-decay-freq: 50000
[2019-02-01 10:20:28] [config] lr-decay-inv-sqrt:
[2019-02-01 10:20:28] [config]   - 16000
[2019-02-01 10:20:28] [config] lr-decay-repeat-warmup: false
[2019-02-01 10:20:28] [config] lr-decay-reset-optimizer: false
[2019-02-01 10:20:28] [config] lr-decay-start:
[2019-02-01 10:20:28] [config]   - 10
[2019-02-01 10:20:28] [config]   - 1
[2019-02-01 10:20:28] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 10:20:28] [config] lr-report: true
[2019-02-01 10:20:28] [config] lr-warmup: 16000
[2019-02-01 10:20:28] [config] lr-warmup-at-reload: false
[2019-02-01 10:20:28] [config] lr-warmup-cycle: false
[2019-02-01 10:20:28] [config] lr-warmup-start-rate: 0
[2019-02-01 10:20:28] [config] max-length: 120
[2019-02-01 10:20:28] [config] max-length-crop: false
[2019-02-01 10:20:28] [config] max-length-factor: 3
[2019-02-01 10:20:28] [config] maxi-batch: 1000
[2019-02-01 10:20:28] [config] maxi-batch-sort: trg
[2019-02-01 10:20:28] [config] mini-batch: 1000
[2019-02-01 10:20:28] [config] mini-batch-fit: true
[2019-02-01 10:20:28] [config] mini-batch-fit-step: 10
[2019-02-01 10:20:28] [config] mini-batch-overstuff: 1
[2019-02-01 10:20:28] [config] mini-batch-track-lr: false
[2019-02-01 10:20:28] [config] mini-batch-understuff: 1
[2019-02-01 10:20:28] [config] mini-batch-warmup: 0
[2019-02-01 10:20:28] [config] mini-batch-words: 0
[2019-02-01 10:20:28] [config] mini-batch-words-ref: 0
[2019-02-01 10:20:28] [config] model: ./models/encs/00.baseline/model.npz
[2019-02-01 10:20:28] [config] multi-loss-type: sum
[2019-02-01 10:20:28] [config] multi-node: false
[2019-02-01 10:20:28] [config] multi-node-overlap: true
[2019-02-01 10:20:28] [config] n-best: false
[2019-02-01 10:20:28] [config] no-nccl: false
[2019-02-01 10:20:28] [config] no-reload: false
[2019-02-01 10:20:28] [config] no-restore-corpus: false
[2019-02-01 10:20:28] [config] no-shuffle: false
[2019-02-01 10:20:28] [config] normalize: 1
[2019-02-01 10:20:28] [config] optimizer: adam
[2019-02-01 10:20:28] [config] optimizer-delay: 1
[2019-02-01 10:20:28] [config] optimizer-params:
[2019-02-01 10:20:28] [config]   - 0.9
[2019-02-01 10:20:28] [config]   - 0.98
[2019-02-01 10:20:28] [config]   - 1e-09
[2019-02-01 10:20:28] [config] overwrite: true
[2019-02-01 10:20:28] [config] quiet: false
[2019-02-01 10:20:28] [config] quiet-translation: true
[2019-02-01 10:20:28] [config] relative-paths: false
[2019-02-01 10:20:28] [config] right-left: false
[2019-02-01 10:20:28] [config] save-freq: 5000
[2019-02-01 10:20:28] [config] seed: 0
[2019-02-01 10:20:28] [config] sentencepiece-alphas:
[2019-02-01 10:20:28] [config]   []
[2019-02-01 10:20:28] [config] sentencepiece-max-lines: 10000000
[2019-02-01 10:20:28] [config] sentencepiece-options: ""
[2019-02-01 10:20:28] [config] shuffle-in-ram: true
[2019-02-01 10:20:28] [config] skip: false
[2019-02-01 10:20:28] [config] sqlite: ""
[2019-02-01 10:20:28] [config] sqlite-drop: false
[2019-02-01 10:20:28] [config] sync-sgd: true
[2019-02-01 10:20:28] [config] tempdir: /tmp
[2019-02-01 10:20:28] [config] tied-embeddings: false
[2019-02-01 10:20:28] [config] tied-embeddings-all: true
[2019-02-01 10:20:28] [config] tied-embeddings-src: false
[2019-02-01 10:20:28] [config] train-sets:
[2019-02-01 10:20:28] [config]   - ../data/corpus00.nrm.en.gz
[2019-02-01 10:20:28] [config]   - ../data/corpus00.nrm.cs.gz
[2019-02-01 10:20:28] [config] transformer-aan-activation: swish
[2019-02-01 10:20:28] [config] transformer-aan-depth: 2
[2019-02-01 10:20:28] [config] transformer-aan-nogate: false
[2019-02-01 10:20:28] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 10:20:28] [config] transformer-dim-aan: 2048
[2019-02-01 10:20:28] [config] transformer-dim-ffn: 2048
[2019-02-01 10:20:28] [config] transformer-dropout: 0.1
[2019-02-01 10:20:28] [config] transformer-dropout-attention: 0
[2019-02-01 10:20:28] [config] transformer-dropout-ffn: 0
[2019-02-01 10:20:28] [config] transformer-ffn-activation: swish
[2019-02-01 10:20:28] [config] transformer-ffn-depth: 2
[2019-02-01 10:20:28] [config] transformer-guided-alignment-layer: last
[2019-02-01 10:20:28] [config] transformer-heads: 8
[2019-02-01 10:20:28] [config] transformer-no-projection: false
[2019-02-01 10:20:28] [config] transformer-postprocess: da
[2019-02-01 10:20:28] [config] transformer-postprocess-emb: d
[2019-02-01 10:20:28] [config] transformer-preprocess: n
[2019-02-01 10:20:28] [config] transformer-tied-layers:
[2019-02-01 10:20:28] [config]   []
[2019-02-01 10:20:28] [config] transformer-train-positions: false
[2019-02-01 10:20:28] [config] type: transformer
[2019-02-01 10:20:28] [config] ulr: false
[2019-02-01 10:20:28] [config] ulr-dim-emb: 0
[2019-02-01 10:20:28] [config] ulr-dropout: 0
[2019-02-01 10:20:28] [config] ulr-keys-vectors: ""
[2019-02-01 10:20:28] [config] ulr-query-vectors: ""
[2019-02-01 10:20:28] [config] ulr-softmax-temperature: 1
[2019-02-01 10:20:28] [config] ulr-trainable-transformation: false
[2019-02-01 10:20:28] [config] valid-freq: 5000
[2019-02-01 10:20:28] [config] valid-log: ./models/encs/00.baseline/valid.log
[2019-02-01 10:20:28] [config] valid-max-length: 1000
[2019-02-01 10:20:28] [config] valid-metrics:
[2019-02-01 10:20:28] [config]   - ce-mean-words
[2019-02-01 10:20:28] [config]   - bleu-detok
[2019-02-01 10:20:28] [config] valid-mini-batch: 32
[2019-02-01 10:20:28] [config] valid-sets:
[2019-02-01 10:20:28] [config]   - ../data/newstest2016.en
[2019-02-01 10:20:28] [config]   - ../data/newstest2016.cs
[2019-02-01 10:20:28] [config] valid-translation-output: ./models/encs/00.baseline/devset.bpe.cs.output
[2019-02-01 10:20:28] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 10:20:28] [config] vocabs:
[2019-02-01 10:20:28] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 10:20:28] [config]   - ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 10:20:28] [config] word-penalty: 0
[2019-02-01 10:20:28] [config] workspace: 10000
[2019-02-01 10:20:28] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 10:20:28] Using synchronous training
[2019-02-01 10:20:28] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 10:20:28] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 10:20:28] [data] Loading SentencePiece vocabulary from file ./models/encs/00.baseline/vocab.encs.spm
[2019-02-01 10:20:28] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 10:20:28] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 10:20:28] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 10:20:29] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 10:20:30] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 10:20:30] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 10:20:31] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 10:20:31] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 10:20:31] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 10:20:31] [training] Using 4 GPUs
[2019-02-01 10:20:31] [memory] Reserving 230 MB, device gpu0
[2019-02-01 10:20:31] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 10:20:31] [memory] Reserving 230 MB, device gpu0
[2019-02-01 10:20:52] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 10:20:52] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 10:20:53] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 10:20:53] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 10:20:53] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 10:20:53] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 10:20:53] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 10:20:53] [training] Using 4 GPUs
[2019-02-01 10:20:53] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 10:20:54] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 10:20:54] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 10:20:54] Loading model from ./models/encs/00.baseline/model.npz.orig.npz
[2019-02-01 10:20:55] Loading Adam parameters from ./models/encs/00.baseline/model.npz.optimizer.npz
[2019-02-01 10:20:55] [memory] Reserving 115 MB, device gpu0
[2019-02-01 10:20:55] [memory] Reserving 115 MB, device gpu1
[2019-02-01 10:20:55] [memory] Reserving 115 MB, device gpu2
[2019-02-01 10:20:56] [memory] Reserving 115 MB, device gpu3
[2019-02-01 10:20:56] [training] Model reloaded from ./models/encs/00.baseline/model.npz
[2019-02-01 10:20:56] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 10:20:56] [data] Shuffling data
[2019-02-01 10:21:58] [data] Done reading 58408180 sentences
[2019-02-01 10:22:01] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-01 10:44:44] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 10:44:44] [marian] Running on gpu-e-71 as process 141248 with command line:
[2019-02-01 10:44:44] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./model.npz --type transformer --train-sets /home/cs-grun1/wmt19/data/corpus00.nrm.en.gz /home/cs-grun1/wmt19/data/corpus00.nrm.cs.gz --shuffle-in-ram --vocabs ./vocab.encs.spm ./vocab.encs.spm --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./devset.bpe.cs.output --quiet-translation --valid-sets /home/cs-grun1/wmt19/data/newstest2016.en /home/cs-grun1/wmt19/data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./train.log --valid-log ./valid.log
[2019-02-01 10:44:45] [config] after-batches: 0
[2019-02-01 10:44:45] [config] after-epochs: 0
[2019-02-01 10:44:45] [config] allow-unk: false
[2019-02-01 10:44:45] [config] beam-size: 8
[2019-02-01 10:44:45] [config] bert-class-symbol: "[CLS]"
[2019-02-01 10:44:45] [config] bert-mask-symbol: "[MASK]"
[2019-02-01 10:44:45] [config] bert-masking-fraction: 0.15
[2019-02-01 10:44:45] [config] bert-sep-symbol: "[SEP]"
[2019-02-01 10:44:45] [config] best-deep: false
[2019-02-01 10:44:45] [config] clip-gemm: 0
[2019-02-01 10:44:45] [config] clip-norm: 5
[2019-02-01 10:44:45] [config] cost-type: ce-mean
[2019-02-01 10:44:45] [config] cpu-threads: 0
[2019-02-01 10:44:45] [config] data-weighting-type: sentence
[2019-02-01 10:44:45] [config] dec-cell: gru
[2019-02-01 10:44:45] [config] dec-cell-base-depth: 2
[2019-02-01 10:44:45] [config] dec-cell-high-depth: 1
[2019-02-01 10:44:45] [config] dec-depth: 6
[2019-02-01 10:44:45] [config] devices:
[2019-02-01 10:44:45] [config]   - 0
[2019-02-01 10:44:45] [config]   - 1
[2019-02-01 10:44:45] [config]   - 2
[2019-02-01 10:44:45] [config]   - 3
[2019-02-01 10:44:45] [config] dim-emb: 512
[2019-02-01 10:44:45] [config] dim-rnn: 1024
[2019-02-01 10:44:45] [config] dim-vocabs:
[2019-02-01 10:44:45] [config]   - 32000
[2019-02-01 10:44:45] [config]   - 32000
[2019-02-01 10:44:45] [config] disp-first: 0
[2019-02-01 10:44:45] [config] disp-freq: 1000
[2019-02-01 10:44:45] [config] disp-label-counts: false
[2019-02-01 10:44:45] [config] dropout-rnn: 0
[2019-02-01 10:44:45] [config] dropout-src: 0
[2019-02-01 10:44:45] [config] dropout-trg: 0
[2019-02-01 10:44:45] [config] early-stopping: 5
[2019-02-01 10:44:45] [config] embedding-fix-src: false
[2019-02-01 10:44:45] [config] embedding-fix-trg: false
[2019-02-01 10:44:45] [config] embedding-normalization: false
[2019-02-01 10:44:45] [config] enc-cell: gru
[2019-02-01 10:44:45] [config] enc-cell-depth: 1
[2019-02-01 10:44:45] [config] enc-depth: 6
[2019-02-01 10:44:45] [config] enc-type: bidirectional
[2019-02-01 10:44:45] [config] exponential-smoothing: 0.0001
[2019-02-01 10:44:45] [config] grad-dropping-momentum: 0
[2019-02-01 10:44:45] [config] grad-dropping-rate: 0
[2019-02-01 10:44:45] [config] grad-dropping-warmup: 100
[2019-02-01 10:44:45] [config] guided-alignment: none
[2019-02-01 10:44:45] [config] guided-alignment-cost: mse
[2019-02-01 10:44:45] [config] guided-alignment-weight: 0.1
[2019-02-01 10:44:45] [config] ignore-model-config: false
[2019-02-01 10:44:45] [config] input-types:
[2019-02-01 10:44:45] [config]   []
[2019-02-01 10:44:45] [config] interpolate-env-vars: false
[2019-02-01 10:44:45] [config] keep-best: true
[2019-02-01 10:44:45] [config] label-smoothing: 0.1
[2019-02-01 10:44:45] [config] layer-normalization: true
[2019-02-01 10:44:45] [config] learn-rate: 0.0003
[2019-02-01 10:44:45] [config] log: ./train.log
[2019-02-01 10:44:45] [config] log-level: info
[2019-02-01 10:44:45] [config] lr-decay: 0
[2019-02-01 10:44:45] [config] lr-decay-freq: 50000
[2019-02-01 10:44:45] [config] lr-decay-inv-sqrt:
[2019-02-01 10:44:45] [config]   - 16000
[2019-02-01 10:44:45] [config] lr-decay-repeat-warmup: false
[2019-02-01 10:44:45] [config] lr-decay-reset-optimizer: false
[2019-02-01 10:44:45] [config] lr-decay-start:
[2019-02-01 10:44:45] [config]   - 10
[2019-02-01 10:44:45] [config]   - 1
[2019-02-01 10:44:45] [config] lr-decay-strategy: epoch+stalled
[2019-02-01 10:44:45] [config] lr-report: true
[2019-02-01 10:44:45] [config] lr-warmup: 16000
[2019-02-01 10:44:45] [config] lr-warmup-at-reload: false
[2019-02-01 10:44:45] [config] lr-warmup-cycle: false
[2019-02-01 10:44:45] [config] lr-warmup-start-rate: 0
[2019-02-01 10:44:45] [config] max-length: 120
[2019-02-01 10:44:45] [config] max-length-crop: false
[2019-02-01 10:44:45] [config] max-length-factor: 3
[2019-02-01 10:44:45] [config] maxi-batch: 1000
[2019-02-01 10:44:45] [config] maxi-batch-sort: trg
[2019-02-01 10:44:45] [config] mini-batch: 1000
[2019-02-01 10:44:45] [config] mini-batch-fit: true
[2019-02-01 10:44:45] [config] mini-batch-fit-step: 10
[2019-02-01 10:44:45] [config] mini-batch-overstuff: 1
[2019-02-01 10:44:45] [config] mini-batch-track-lr: false
[2019-02-01 10:44:45] [config] mini-batch-understuff: 1
[2019-02-01 10:44:45] [config] mini-batch-warmup: 0
[2019-02-01 10:44:45] [config] mini-batch-words: 0
[2019-02-01 10:44:45] [config] mini-batch-words-ref: 0
[2019-02-01 10:44:45] [config] model: ./model.npz
[2019-02-01 10:44:45] [config] multi-loss-type: sum
[2019-02-01 10:44:45] [config] multi-node: false
[2019-02-01 10:44:45] [config] multi-node-overlap: true
[2019-02-01 10:44:45] [config] n-best: false
[2019-02-01 10:44:45] [config] no-nccl: false
[2019-02-01 10:44:45] [config] no-reload: false
[2019-02-01 10:44:45] [config] no-restore-corpus: false
[2019-02-01 10:44:45] [config] no-shuffle: false
[2019-02-01 10:44:45] [config] normalize: 1
[2019-02-01 10:44:45] [config] optimizer: adam
[2019-02-01 10:44:45] [config] optimizer-delay: 1
[2019-02-01 10:44:45] [config] optimizer-params:
[2019-02-01 10:44:45] [config]   - 0.9
[2019-02-01 10:44:45] [config]   - 0.98
[2019-02-01 10:44:45] [config]   - 1e-09
[2019-02-01 10:44:45] [config] overwrite: true
[2019-02-01 10:44:45] [config] quiet: false
[2019-02-01 10:44:45] [config] quiet-translation: true
[2019-02-01 10:44:45] [config] relative-paths: false
[2019-02-01 10:44:45] [config] right-left: false
[2019-02-01 10:44:45] [config] save-freq: 5000
[2019-02-01 10:44:45] [config] seed: 0
[2019-02-01 10:44:45] [config] sentencepiece-alphas:
[2019-02-01 10:44:45] [config]   []
[2019-02-01 10:44:45] [config] sentencepiece-max-lines: 10000000
[2019-02-01 10:44:45] [config] sentencepiece-options: ""
[2019-02-01 10:44:45] [config] shuffle-in-ram: true
[2019-02-01 10:44:45] [config] skip: false
[2019-02-01 10:44:45] [config] sqlite: ""
[2019-02-01 10:44:45] [config] sqlite-drop: false
[2019-02-01 10:44:45] [config] sync-sgd: true
[2019-02-01 10:44:45] [config] tempdir: /tmp
[2019-02-01 10:44:45] [config] tied-embeddings: false
[2019-02-01 10:44:45] [config] tied-embeddings-all: true
[2019-02-01 10:44:45] [config] tied-embeddings-src: false
[2019-02-01 10:44:45] [config] train-sets:
[2019-02-01 10:44:45] [config]   - /home/cs-grun1/wmt19/data/corpus00.nrm.en.gz
[2019-02-01 10:44:45] [config]   - /home/cs-grun1/wmt19/data/corpus00.nrm.cs.gz
[2019-02-01 10:44:45] [config] transformer-aan-activation: swish
[2019-02-01 10:44:45] [config] transformer-aan-depth: 2
[2019-02-01 10:44:45] [config] transformer-aan-nogate: false
[2019-02-01 10:44:45] [config] transformer-decoder-autoreg: self-attention
[2019-02-01 10:44:45] [config] transformer-dim-aan: 2048
[2019-02-01 10:44:45] [config] transformer-dim-ffn: 2048
[2019-02-01 10:44:45] [config] transformer-dropout: 0.1
[2019-02-01 10:44:45] [config] transformer-dropout-attention: 0
[2019-02-01 10:44:45] [config] transformer-dropout-ffn: 0
[2019-02-01 10:44:45] [config] transformer-ffn-activation: swish
[2019-02-01 10:44:45] [config] transformer-ffn-depth: 2
[2019-02-01 10:44:45] [config] transformer-guided-alignment-layer: last
[2019-02-01 10:44:45] [config] transformer-heads: 8
[2019-02-01 10:44:45] [config] transformer-no-projection: false
[2019-02-01 10:44:45] [config] transformer-postprocess: da
[2019-02-01 10:44:45] [config] transformer-postprocess-emb: d
[2019-02-01 10:44:45] [config] transformer-preprocess: n
[2019-02-01 10:44:45] [config] transformer-tied-layers:
[2019-02-01 10:44:45] [config]   []
[2019-02-01 10:44:45] [config] transformer-train-positions: false
[2019-02-01 10:44:45] [config] type: transformer
[2019-02-01 10:44:45] [config] ulr: false
[2019-02-01 10:44:45] [config] ulr-dim-emb: 0
[2019-02-01 10:44:45] [config] ulr-dropout: 0
[2019-02-01 10:44:45] [config] ulr-keys-vectors: ""
[2019-02-01 10:44:45] [config] ulr-query-vectors: ""
[2019-02-01 10:44:45] [config] ulr-softmax-temperature: 1
[2019-02-01 10:44:45] [config] ulr-trainable-transformation: false
[2019-02-01 10:44:45] [config] valid-freq: 5000
[2019-02-01 10:44:45] [config] valid-log: ./valid.log
[2019-02-01 10:44:45] [config] valid-max-length: 1000
[2019-02-01 10:44:45] [config] valid-metrics:
[2019-02-01 10:44:45] [config]   - ce-mean-words
[2019-02-01 10:44:45] [config]   - bleu-detok
[2019-02-01 10:44:45] [config] valid-mini-batch: 32
[2019-02-01 10:44:45] [config] valid-sets:
[2019-02-01 10:44:45] [config]   - /home/cs-grun1/wmt19/data/newstest2016.en
[2019-02-01 10:44:45] [config]   - /home/cs-grun1/wmt19/data/newstest2016.cs
[2019-02-01 10:44:45] [config] valid-translation-output: ./devset.bpe.cs.output
[2019-02-01 10:44:45] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 10:44:45] [config] vocabs:
[2019-02-01 10:44:45] [config]   - ./vocab.encs.spm
[2019-02-01 10:44:45] [config]   - ./vocab.encs.spm
[2019-02-01 10:44:45] [config] word-penalty: 0
[2019-02-01 10:44:45] [config] workspace: 10000
[2019-02-01 10:44:45] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-01 10:44:45] Using synchronous training
[2019-02-01 10:44:45] [data] Loading SentencePiece vocabulary from file ./vocab.encs.spm
[2019-02-01 10:44:45] [data] Setting vocabulary size for input 0 to 32000
[2019-02-01 10:44:45] [data] Loading SentencePiece vocabulary from file ./vocab.encs.spm
[2019-02-01 10:44:45] [data] Setting vocabulary size for input 1 to 32000
[2019-02-01 10:44:45] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-01 10:44:45] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-01 10:44:46] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 10:44:46] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 10:44:47] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 10:44:48] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 10:44:48] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 10:44:48] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 10:44:48] [training] Using 4 GPUs
[2019-02-01 10:44:48] [memory] Reserving 230 MB, device gpu0
[2019-02-01 10:44:48] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-01 10:44:48] [memory] Reserving 230 MB, device gpu0
[2019-02-01 10:45:09] [batching] Done. Typical MB size is 27396 target words
[2019-02-01 10:45:09] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-01 10:45:09] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-01 10:45:09] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-01 10:45:09] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-01 10:45:09] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-01 10:45:09] [comm] NCCLCommunicator constructed successfully.
[2019-02-01 10:45:09] [training] Using 4 GPUs
[2019-02-01 10:45:09] Loading model from ./model.npz.orig.npz
[2019-02-01 10:45:10] Loading model from ./model.npz.orig.npz
[2019-02-01 10:45:11] Loading model from ./model.npz.orig.npz
[2019-02-01 10:45:11] Loading model from ./model.npz.orig.npz
[2019-02-01 10:45:11] Loading Adam parameters from ./model.npz.optimizer.npz
[2019-02-01 10:45:12] [memory] Reserving 115 MB, device gpu0
[2019-02-01 10:45:12] [memory] Reserving 115 MB, device gpu1
[2019-02-01 10:45:12] [memory] Reserving 115 MB, device gpu2
[2019-02-01 10:45:12] [memory] Reserving 115 MB, device gpu3
[2019-02-01 10:45:12] [training] Model reloaded from ./model.npz
[2019-02-01 10:45:12] [data] Restoring the corpus state to epoch 7, batch 280000
[2019-02-01 10:45:12] [data] Shuffling data
[2019-02-01 10:46:14] [data] Done reading 58408180 sentences
[2019-02-01 10:46:18] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-01 10:52:15] Training started
[2019-02-01 10:52:15] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu3
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu0
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu2
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu1
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu3
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu1
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu0
[2019-02-01 10:52:15] [memory] Reserving 230 MB, device gpu2
[2019-02-01 10:52:15] Loading model from ./model.npz
[2019-02-01 10:52:16] [memory] Reserving 230 MB, device cpu0
[2019-02-01 10:52:16] [memory] Reserving 57 MB, device gpu0
[2019-02-01 10:52:16] [memory] Reserving 57 MB, device gpu1
[2019-02-01 10:52:16] [memory] Reserving 57 MB, device gpu2
[2019-02-01 10:52:16] [memory] Reserving 57 MB, device gpu3
[2019-02-01 10:59:42] Ep. 7 : Up. 281000 : Sen. 5,630,740 : Cost 39.39997101 : Time 896.79s : 21693.93 words/s : L.r. 7.1586e-05
[2019-02-01 11:07:06] Ep. 7 : Up. 282000 : Sen. 6,887,803 : Cost 40.64555740 : Time 444.96s : 43662.88 words/s : L.r. 7.1459e-05
[2019-02-01 11:14:32] Ep. 7 : Up. 283000 : Sen. 8,140,346 : Cost 40.94089127 : Time 445.31s : 43682.30 words/s : L.r. 7.1333e-05
[2019-02-01 11:21:59] Ep. 7 : Up. 284000 : Sen. 9,407,291 : Cost 40.25247192 : Time 447.34s : 43563.95 words/s : L.r. 7.1207e-05
[2019-02-01 11:29:24] Ep. 7 : Up. 285000 : Sen. 10,664,855 : Cost 40.48670197 : Time 444.94s : 43384.52 words/s : L.r. 7.1082e-05
[2019-02-01 11:29:24] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 11:29:25] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 11:29:27] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 11:29:30] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 11:29:31] [valid] Ep. 7 : Up. 285000 : ce-mean-words : 1.57921 : new best
[2019-02-01 11:30:08] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-01 11:30:09] [valid] Ep. 7 : Up. 285000 : bleu-detok : 25.8256 : new best
[2019-02-01 11:37:37] Ep. 7 : Up. 286000 : Sen. 11,936,278 : Cost 40.44720459 : Time 492.86s : 39772.46 words/s : L.r. 7.0957e-05
[2019-02-01 11:45:00] Ep. 7 : Up. 287000 : Sen. 13,210,909 : Cost 39.87759781 : Time 443.26s : 43587.07 words/s : L.r. 7.0834e-05
[2019-02-01 11:52:26] Ep. 7 : Up. 288000 : Sen. 14,443,439 : Cost 41.34957886 : Time 445.76s : 43694.18 words/s : L.r. 7.0711e-05
[2019-02-01 11:59:51] Ep. 7 : Up. 289000 : Sen. 15,716,749 : Cost 40.21152878 : Time 445.05s : 43712.51 words/s : L.r. 7.0588e-05
[2019-02-01 12:07:14] Ep. 7 : Up. 290000 : Sen. 16,991,136 : Cost 39.75104523 : Time 443.21s : 43456.22 words/s : L.r. 7.0466e-05
[2019-02-01 12:07:14] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 12:07:15] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 12:07:17] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 12:07:20] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 12:07:21] [valid] Ep. 7 : Up. 290000 : ce-mean-words : 1.57895 : new best
[2019-02-01 12:07:57] [valid] Ep. 7 : Up. 290000 : bleu-detok : 25.6694 : stalled 1 times (last best: 25.8256)
[2019-02-01 12:15:23] Ep. 7 : Up. 291000 : Sen. 18,250,090 : Cost 40.63049698 : Time 488.43s : 39871.51 words/s : L.r. 7.0345e-05
[2019-02-01 12:22:49] Ep. 7 : Up. 292000 : Sen. 19,524,709 : Cost 40.01818466 : Time 446.51s : 43568.61 words/s : L.r. 7.0225e-05
[2019-02-01 12:30:14] Ep. 7 : Up. 293000 : Sen. 20,786,756 : Cost 40.16246796 : Time 444.57s : 43343.96 words/s : L.r. 7.0105e-05
[2019-02-01 12:37:41] Ep. 7 : Up. 294000 : Sen. 22,054,306 : Cost 40.58239746 : Time 447.74s : 43708.65 words/s : L.r. 6.9985e-05
[2019-02-01 12:45:09] Ep. 7 : Up. 295000 : Sen. 23,341,489 : Cost 39.64034653 : Time 447.99s : 43416.74 words/s : L.r. 6.9867e-05
[2019-02-01 12:45:09] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 12:45:11] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 12:45:12] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 12:45:16] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 12:45:17] [valid] Ep. 7 : Up. 295000 : ce-mean-words : 1.57842 : new best
[2019-02-01 12:45:55] [valid] Ep. 7 : Up. 295000 : bleu-detok : 25.6586 : stalled 2 times (last best: 25.8256)
[2019-02-01 12:53:19] Ep. 7 : Up. 296000 : Sen. 24,593,180 : Cost 40.63107300 : Time 489.71s : 39408.77 words/s : L.r. 6.9749e-05
[2019-02-01 13:00:46] Ep. 7 : Up. 297000 : Sen. 25,850,949 : Cost 40.94993591 : Time 447.32s : 43869.40 words/s : L.r. 6.9631e-05
[2019-02-01 13:08:12] Ep. 7 : Up. 298000 : Sen. 27,126,357 : Cost 39.75069809 : Time 445.58s : 43333.69 words/s : L.r. 6.9514e-05
[2019-02-01 13:15:37] Ep. 7 : Up. 299000 : Sen. 28,401,328 : Cost 39.97769165 : Time 444.53s : 43614.15 words/s : L.r. 6.9398e-05
[2019-02-01 13:23:02] Ep. 7 : Up. 300000 : Sen. 29,617,870 : Cost 42.18460083 : Time 445.49s : 43744.72 words/s : L.r. 6.9282e-05
[2019-02-01 13:23:02] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 13:23:03] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 13:23:05] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 13:23:09] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 13:23:09] [valid] Ep. 7 : Up. 300000 : ce-mean-words : 1.57694 : new best
[2019-02-01 13:23:50] [valid] Ep. 7 : Up. 300000 : bleu-detok : 25.6298 : stalled 3 times (last best: 25.8256)
[2019-02-01 13:31:17] Ep. 7 : Up. 301000 : Sen. 30,905,644 : Cost 39.67873383 : Time 494.53s : 39460.28 words/s : L.r. 6.9167e-05
[2019-02-01 13:38:41] Ep. 7 : Up. 302000 : Sen. 32,176,968 : Cost 40.03351593 : Time 444.76s : 43564.75 words/s : L.r. 6.9052e-05
[2019-02-01 13:46:06] Ep. 7 : Up. 303000 : Sen. 33,454,449 : Cost 39.84265137 : Time 444.77s : 43709.02 words/s : L.r. 6.8938e-05
[2019-02-01 13:53:30] Ep. 7 : Up. 304000 : Sen. 34,718,497 : Cost 40.18794250 : Time 444.28s : 43405.33 words/s : L.r. 6.8825e-05
[2019-02-01 14:00:56] Ep. 7 : Up. 305000 : Sen. 35,981,524 : Cost 40.56602859 : Time 445.86s : 43756.55 words/s : L.r. 6.8712e-05
[2019-02-01 14:00:56] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 14:00:57] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 14:00:59] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 14:01:02] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 14:01:03] [valid] Ep. 7 : Up. 305000 : ce-mean-words : 1.5751 : new best
[2019-02-01 14:01:44] [valid] Ep. 7 : Up. 305000 : bleu-detok : 25.7665 : stalled 4 times (last best: 25.8256)
[2019-02-01 14:09:09] Ep. 7 : Up. 306000 : Sen. 37,233,247 : Cost 40.46961594 : Time 493.11s : 39186.71 words/s : L.r. 6.8599e-05
[2019-02-01 14:16:36] Ep. 7 : Up. 307000 : Sen. 38,476,568 : Cost 41.50332642 : Time 446.49s : 43974.69 words/s : L.r. 6.8488e-05
[2019-02-01 14:23:56] Ep. 7 : Up. 308000 : Sen. 39,758,159 : Cost 39.00857925 : Time 439.86s : 43267.47 words/s : L.r. 6.8376e-05
[2019-02-01 14:31:24] Ep. 7 : Up. 309000 : Sen. 41,033,071 : Cost 40.39789963 : Time 448.05s : 43766.34 words/s : L.r. 6.8266e-05
[2019-02-01 14:38:49] Ep. 7 : Up. 310000 : Sen. 42,299,706 : Cost 40.33715820 : Time 445.53s : 43732.38 words/s : L.r. 6.8155e-05
[2019-02-01 14:38:49] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 14:38:51] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 14:38:52] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 14:38:56] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 14:38:57] [valid] Ep. 7 : Up. 310000 : ce-mean-words : 1.57453 : new best
[2019-02-01 14:39:36] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-01 14:39:36] [valid] Ep. 7 : Up. 310000 : bleu-detok : 25.8291 : new best
[2019-02-01 14:47:03] Ep. 7 : Up. 311000 : Sen. 43,562,005 : Cost 40.57206726 : Time 494.00s : 39474.63 words/s : L.r. 6.8046e-05
[2019-02-01 14:54:26] Ep. 7 : Up. 312000 : Sen. 44,848,310 : Cost 39.39006424 : Time 443.12s : 43457.95 words/s : L.r. 6.7937e-05
[2019-02-01 15:01:53] Ep. 7 : Up. 313000 : Sen. 46,086,937 : Cost 41.18004990 : Time 446.96s : 43493.24 words/s : L.r. 6.7828e-05
[2019-02-01 15:09:19] Ep. 7 : Up. 314000 : Sen. 47,368,235 : Cost 39.78398132 : Time 445.49s : 43576.32 words/s : L.r. 6.7720e-05
[2019-02-01 15:16:45] Ep. 7 : Up. 315000 : Sen. 48,616,666 : Cost 40.84552383 : Time 446.06s : 43601.37 words/s : L.r. 6.7612e-05
[2019-02-01 15:16:45] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 15:16:46] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 15:16:47] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 15:16:51] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 15:16:52] [valid] Ep. 7 : Up. 315000 : ce-mean-words : 1.57361 : new best
[2019-02-01 15:17:27] [valid] Ep. 7 : Up. 315000 : bleu-detok : 25.7643 : stalled 1 times (last best: 25.8291)
[2019-02-01 15:24:54] Ep. 7 : Up. 316000 : Sen. 49,902,533 : Cost 39.34263229 : Time 488.54s : 39538.39 words/s : L.r. 6.7505e-05
[2019-02-01 15:32:21] Ep. 7 : Up. 317000 : Sen. 51,161,983 : Cost 40.63921738 : Time 447.86s : 43580.80 words/s : L.r. 6.7399e-05
[2019-02-01 15:39:47] Ep. 7 : Up. 318000 : Sen. 52,399,259 : Cost 41.15491867 : Time 445.68s : 43542.09 words/s : L.r. 6.7293e-05
[2019-02-01 15:47:12] Ep. 7 : Up. 319000 : Sen. 53,681,477 : Cost 39.50706100 : Time 445.32s : 43401.94 words/s : L.r. 6.7187e-05
[2019-02-01 15:54:40] Ep. 7 : Up. 320000 : Sen. 54,960,751 : Cost 40.08639145 : Time 447.37s : 43637.19 words/s : L.r. 6.7082e-05
[2019-02-01 15:54:40] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 15:54:41] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 15:54:42] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 15:54:46] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 15:54:47] [valid] Ep. 7 : Up. 320000 : ce-mean-words : 1.57236 : new best
[2019-02-01 15:55:24] [valid] Ep. 7 : Up. 320000 : bleu-detok : 25.8176 : stalled 2 times (last best: 25.8291)
[2019-02-01 16:02:51] Ep. 7 : Up. 321000 : Sen. 56,232,441 : Cost 40.24801636 : Time 490.80s : 39754.32 words/s : L.r. 6.6977e-05
[2019-02-01 16:10:16] Ep. 7 : Up. 322000 : Sen. 57,473,862 : Cost 40.99131393 : Time 445.14s : 43578.51 words/s : L.r. 6.6873e-05
[2019-02-01 16:15:04] Seen 58282224 samples
[2019-02-01 16:15:04] Starting epoch 8
[2019-02-01 16:15:04] [data] Shuffling data
[2019-02-01 16:15:07] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-01 16:18:47] Ep. 8 : Up. 323000 : Sen. 426,572 : Cost 39.81194687 : Time 511.33s : 36597.36 words/s : L.r. 6.6770e-05
[2019-02-01 16:26:12] Ep. 8 : Up. 324000 : Sen. 1,670,648 : Cost 40.41574478 : Time 444.88s : 43464.24 words/s : L.r. 6.6667e-05
[2019-02-01 16:33:38] Ep. 8 : Up. 325000 : Sen. 2,946,111 : Cost 39.95830536 : Time 446.25s : 43583.93 words/s : L.r. 6.6564e-05
[2019-02-01 16:33:38] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 16:33:39] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 16:33:41] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 16:33:45] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 16:33:45] [valid] Ep. 8 : Up. 325000 : ce-mean-words : 1.57211 : new best
[2019-02-01 16:34:23] [valid] Ep. 8 : Up. 325000 : bleu-detok : 25.8115 : stalled 3 times (last best: 25.8291)
[2019-02-01 16:41:49] Ep. 8 : Up. 326000 : Sen. 4,247,961 : Cost 38.89406204 : Time 490.69s : 39452.78 words/s : L.r. 6.6462e-05
[2019-02-01 16:49:15] Ep. 8 : Up. 327000 : Sen. 5,492,230 : Cost 41.01097488 : Time 445.76s : 43625.79 words/s : L.r. 6.6360e-05
[2019-02-01 16:56:42] Ep. 8 : Up. 328000 : Sen. 6,735,408 : Cost 40.99441147 : Time 446.98s : 43599.89 words/s : L.r. 6.6259e-05
[2019-02-01 17:04:08] Ep. 8 : Up. 329000 : Sen. 8,016,260 : Cost 39.62186813 : Time 446.60s : 43486.38 words/s : L.r. 6.6158e-05
[2019-02-01 17:11:37] Ep. 8 : Up. 330000 : Sen. 9,294,067 : Cost 39.97888947 : Time 448.45s : 43573.27 words/s : L.r. 6.6058e-05
[2019-02-01 17:11:37] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 17:11:38] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 17:11:39] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 17:11:43] [valid] Ep. 8 : Up. 330000 : ce-mean-words : 1.57213 : stalled 1 times (last best: 1.57211)
[2019-02-01 17:12:21] [valid] Ep. 8 : Up. 330000 : bleu-detok : 25.717 : stalled 4 times (last best: 25.8291)
[2019-02-01 17:19:45] Ep. 8 : Up. 331000 : Sen. 10,567,829 : Cost 39.48533630 : Time 488.25s : 39409.11 words/s : L.r. 6.5958e-05
[2019-02-01 17:27:09] Ep. 8 : Up. 332000 : Sen. 11,802,829 : Cost 41.39566803 : Time 444.21s : 43875.17 words/s : L.r. 6.5859e-05
[2019-02-01 17:34:36] Ep. 8 : Up. 333000 : Sen. 13,088,352 : Cost 39.49485016 : Time 446.42s : 43485.46 words/s : L.r. 6.5760e-05
[2019-02-01 17:42:04] Ep. 8 : Up. 334000 : Sen. 14,326,388 : Cost 41.35229111 : Time 448.28s : 43727.95 words/s : L.r. 6.5661e-05
[2019-02-01 17:49:29] Ep. 8 : Up. 335000 : Sen. 15,613,034 : Cost 39.25726700 : Time 445.25s : 43289.19 words/s : L.r. 6.5563e-05
[2019-02-01 17:49:29] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 17:49:30] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 17:49:31] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 17:49:35] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 17:49:36] [valid] Ep. 8 : Up. 335000 : ce-mean-words : 1.57173 : new best
[2019-02-01 17:50:14] [valid] Ep. 8 : Up. 335000 : bleu-detok : 25.7316 : stalled 5 times (last best: 25.8291)
[2019-02-01 17:57:36] Ep. 8 : Up. 336000 : Sen. 16,862,253 : Cost 40.29116821 : Time 486.61s : 39493.78 words/s : L.r. 6.5465e-05
[2019-02-01 18:05:01] Ep. 8 : Up. 337000 : Sen. 18,136,103 : Cost 39.85220718 : Time 445.65s : 43618.57 words/s : L.r. 6.5368e-05
[2019-02-01 18:12:25] Ep. 8 : Up. 338000 : Sen. 19,395,406 : Cost 40.48807907 : Time 444.18s : 43657.12 words/s : L.r. 6.5271e-05
[2019-02-01 18:19:53] Ep. 8 : Up. 339000 : Sen. 20,674,920 : Cost 39.93020248 : Time 447.63s : 43754.52 words/s : L.r. 6.5175e-05
[2019-02-01 18:27:16] Ep. 8 : Up. 340000 : Sen. 21,921,782 : Cost 40.58534241 : Time 442.61s : 43607.24 words/s : L.r. 6.5079e-05
[2019-02-01 18:27:16] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 18:27:17] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 18:27:18] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 18:27:22] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 18:27:23] [valid] Ep. 8 : Up. 340000 : ce-mean-words : 1.57127 : new best
[2019-02-01 18:28:01] [valid] Ep. 8 : Up. 340000 : bleu-detok : 25.7547 : stalled 6 times (last best: 25.8291)
[2019-02-01 18:35:27] Ep. 8 : Up. 341000 : Sen. 23,177,487 : Cost 40.30978775 : Time 491.50s : 39462.95 words/s : L.r. 6.4984e-05
[2019-02-01 18:42:53] Ep. 8 : Up. 342000 : Sen. 24,432,506 : Cost 40.50395966 : Time 445.33s : 43480.80 words/s : L.r. 6.4889e-05
[2019-02-01 18:50:19] Ep. 8 : Up. 343000 : Sen. 25,727,257 : Cost 39.08653641 : Time 446.72s : 43424.37 words/s : L.r. 6.4794e-05
[2019-02-01 18:57:47] Ep. 8 : Up. 344000 : Sen. 26,982,217 : Cost 40.70023346 : Time 447.62s : 43499.15 words/s : L.r. 6.4700e-05
[2019-02-01 19:05:10] Ep. 8 : Up. 345000 : Sen. 28,229,747 : Cost 40.40349960 : Time 442.98s : 43500.51 words/s : L.r. 6.4606e-05
[2019-02-01 19:05:10] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 19:05:11] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 19:05:12] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 19:05:16] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 19:05:17] [valid] Ep. 8 : Up. 345000 : ce-mean-words : 1.57052 : new best
[2019-02-01 19:05:55] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-01 19:05:56] [valid] Ep. 8 : Up. 345000 : bleu-detok : 25.9375 : new best
[2019-02-01 19:13:22] Ep. 8 : Up. 346000 : Sen. 29,514,639 : Cost 39.53399277 : Time 492.42s : 39435.89 words/s : L.r. 6.4512e-05
[2019-02-01 19:20:49] Ep. 8 : Up. 347000 : Sen. 30,762,997 : Cost 41.02362442 : Time 446.80s : 43747.39 words/s : L.r. 6.4419e-05
[2019-02-01 19:28:14] Ep. 8 : Up. 348000 : Sen. 32,042,288 : Cost 39.67481232 : Time 444.67s : 43633.94 words/s : L.r. 6.4327e-05
[2019-02-01 19:35:38] Ep. 8 : Up. 349000 : Sen. 33,292,940 : Cost 40.62685776 : Time 444.59s : 43628.49 words/s : L.r. 6.4235e-05
[2019-02-01 19:43:06] Ep. 8 : Up. 350000 : Sen. 34,576,299 : Cost 39.73492432 : Time 447.38s : 43624.46 words/s : L.r. 6.4143e-05
[2019-02-01 19:43:06] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 19:43:07] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 19:43:08] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 19:43:12] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 19:43:13] [valid] Ep. 8 : Up. 350000 : ce-mean-words : 1.57009 : new best
[2019-02-01 19:43:50] [valid] Ep. 8 : Up. 350000 : bleu-detok : 25.8433 : stalled 1 times (last best: 25.9375)
[2019-02-01 19:51:15] Ep. 8 : Up. 351000 : Sen. 35,828,010 : Cost 40.35899353 : Time 489.75s : 39538.85 words/s : L.r. 6.4051e-05
[2019-02-01 19:58:44] Ep. 8 : Up. 352000 : Sen. 37,099,373 : Cost 40.38903046 : Time 448.20s : 43652.54 words/s : L.r. 6.3960e-05
[2019-02-01 20:06:09] Ep. 8 : Up. 353000 : Sen. 38,396,859 : Cost 39.03786850 : Time 445.70s : 43412.67 words/s : L.r. 6.3870e-05
[2019-02-01 20:13:36] Ep. 8 : Up. 354000 : Sen. 39,631,603 : Cost 41.39418793 : Time 447.05s : 43638.78 words/s : L.r. 6.3779e-05
[2019-02-01 20:21:02] Ep. 8 : Up. 355000 : Sen. 40,921,443 : Cost 39.19756317 : Time 445.81s : 43428.48 words/s : L.r. 6.3689e-05
[2019-02-01 20:21:02] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 20:21:03] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 20:21:05] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 20:21:09] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 20:21:09] [valid] Ep. 8 : Up. 355000 : ce-mean-words : 1.56927 : new best
[2019-02-01 20:21:48] [valid] Ep. 8 : Up. 355000 : bleu-detok : 25.9167 : stalled 2 times (last best: 25.9375)
[2019-02-01 20:29:15] Ep. 8 : Up. 356000 : Sen. 42,181,099 : Cost 40.54271317 : Time 492.67s : 39590.98 words/s : L.r. 6.3600e-05
[2019-02-01 20:36:37] Ep. 8 : Up. 357000 : Sen. 43,432,548 : Cost 40.17042160 : Time 442.52s : 43485.22 words/s : L.r. 6.3511e-05
[2019-02-01 20:44:02] Ep. 8 : Up. 358000 : Sen. 44,697,726 : Cost 40.10086823 : Time 444.51s : 43597.04 words/s : L.r. 6.3422e-05
[2019-02-01 20:51:27] Ep. 8 : Up. 359000 : Sen. 45,975,782 : Cost 39.87303162 : Time 445.02s : 43793.55 words/s : L.r. 6.3334e-05
[2019-02-01 20:58:51] Ep. 8 : Up. 360000 : Sen. 47,209,726 : Cost 41.03129959 : Time 444.53s : 43596.46 words/s : L.r. 6.3246e-05
[2019-02-01 20:58:51] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 20:58:53] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 20:58:54] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 20:58:58] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 20:58:59] [valid] Ep. 8 : Up. 360000 : ce-mean-words : 1.56791 : new best
[2019-02-01 20:59:37] [valid] Ep. 8 : Up. 360000 : bleu-detok : 25.8537 : stalled 3 times (last best: 25.9375)
[2019-02-01 21:07:04] Ep. 8 : Up. 361000 : Sen. 48,497,650 : Cost 39.57321167 : Time 492.32s : 39496.73 words/s : L.r. 6.3158e-05
[2019-02-01 21:14:30] Ep. 8 : Up. 362000 : Sen. 49,746,028 : Cost 40.78663254 : Time 445.76s : 43691.25 words/s : L.r. 6.3071e-05
[2019-02-01 21:21:54] Ep. 8 : Up. 363000 : Sen. 51,022,679 : Cost 39.65831757 : Time 444.82s : 43521.95 words/s : L.r. 6.2984e-05
[2019-02-01 21:29:20] Ep. 8 : Up. 364000 : Sen. 52,302,478 : Cost 39.77946854 : Time 445.92s : 43627.24 words/s : L.r. 6.2897e-05
[2019-02-01 21:36:45] Ep. 8 : Up. 365000 : Sen. 53,556,195 : Cost 40.11345673 : Time 444.96s : 43291.93 words/s : L.r. 6.2811e-05
[2019-02-01 21:36:45] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 21:36:46] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 21:36:48] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 21:36:51] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 21:36:52] [valid] Ep. 8 : Up. 365000 : ce-mean-words : 1.56702 : new best
[2019-02-01 21:37:31] [valid] Ep. 8 : Up. 365000 : bleu-detok : 25.8677 : stalled 4 times (last best: 25.9375)
[2019-02-01 21:44:58] Ep. 8 : Up. 366000 : Sen. 54,805,991 : Cost 41.03010941 : Time 493.07s : 39720.27 words/s : L.r. 6.2725e-05
[2019-02-01 21:52:23] Ep. 8 : Up. 367000 : Sen. 56,102,562 : Cost 39.05081940 : Time 444.70s : 43597.55 words/s : L.r. 6.2639e-05
[2019-02-01 21:59:51] Ep. 8 : Up. 368000 : Sen. 57,356,880 : Cost 40.75431442 : Time 447.64s : 43782.99 words/s : L.r. 6.2554e-05
[2019-02-01 22:05:25] Seen 58282224 samples
[2019-02-01 22:05:25] Starting epoch 9
[2019-02-01 22:05:25] [data] Shuffling data
[2019-02-01 22:05:29] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-01 22:08:20] Ep. 9 : Up. 369000 : Sen. 276,727 : Cost 40.38153839 : Time 508.96s : 36433.49 words/s : L.r. 6.2470e-05
[2019-02-01 22:15:44] Ep. 9 : Up. 370000 : Sen. 1,566,053 : Cost 39.18827820 : Time 444.80s : 43548.07 words/s : L.r. 6.2385e-05
[2019-02-01 22:15:44] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 22:15:46] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 22:15:47] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 22:15:51] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 22:15:52] [valid] Ep. 9 : Up. 370000 : ce-mean-words : 1.56629 : new best
[2019-02-01 22:16:32] [valid] Ep. 9 : Up. 370000 : bleu-detok : 25.8187 : stalled 5 times (last best: 25.9375)
[2019-02-01 22:24:00] Ep. 9 : Up. 371000 : Sen. 2,816,729 : Cost 40.64085388 : Time 495.23s : 39408.61 words/s : L.r. 6.2301e-05
[2019-02-01 22:31:26] Ep. 9 : Up. 372000 : Sen. 4,081,754 : Cost 40.06094742 : Time 446.21s : 43547.29 words/s : L.r. 6.2217e-05
[2019-02-01 22:38:49] Ep. 9 : Up. 373000 : Sen. 5,347,540 : Cost 39.65666199 : Time 443.60s : 43319.98 words/s : L.r. 6.2134e-05
[2019-02-01 22:46:20] Ep. 9 : Up. 374000 : Sen. 6,609,119 : Cost 40.49246216 : Time 450.34s : 43596.90 words/s : L.r. 6.2051e-05
[2019-02-01 22:53:46] Ep. 9 : Up. 375000 : Sen. 7,873,338 : Cost 40.02308655 : Time 446.07s : 43443.12 words/s : L.r. 6.1968e-05
[2019-02-01 22:53:46] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 22:53:47] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 22:53:48] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 22:53:52] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 22:53:53] [valid] Ep. 9 : Up. 375000 : ce-mean-words : 1.56626 : new best
[2019-02-01 22:54:33] [valid] Ep. 9 : Up. 375000 : bleu-detok : 25.9037 : stalled 6 times (last best: 25.9375)
[2019-02-01 23:02:00] Ep. 9 : Up. 376000 : Sen. 9,131,167 : Cost 40.46201324 : Time 494.24s : 39478.57 words/s : L.r. 6.1885e-05
[2019-02-01 23:09:26] Ep. 9 : Up. 377000 : Sen. 10,412,676 : Cost 39.53440475 : Time 445.88s : 43629.63 words/s : L.r. 6.1803e-05
[2019-02-01 23:16:50] Ep. 9 : Up. 378000 : Sen. 11,674,158 : Cost 40.03615189 : Time 443.55s : 43531.93 words/s : L.r. 6.1721e-05
[2019-02-01 23:24:16] Ep. 9 : Up. 379000 : Sen. 12,954,029 : Cost 39.71030045 : Time 446.58s : 43638.45 words/s : L.r. 6.1640e-05
[2019-02-01 23:31:43] Ep. 9 : Up. 380000 : Sen. 14,228,042 : Cost 39.93444824 : Time 446.41s : 43705.85 words/s : L.r. 6.1559e-05
[2019-02-01 23:31:43] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-01 23:31:44] Saving model weights and runtime parameters to ./model.npz
[2019-02-01 23:31:45] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-01 23:31:49] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-01 23:31:50] [valid] Ep. 9 : Up. 380000 : ce-mean-words : 1.56609 : new best
[2019-02-01 23:32:29] [valid] Ep. 9 : Up. 380000 : bleu-detok : 25.8919 : stalled 7 times (last best: 25.9375)
[2019-02-01 23:39:55] Ep. 9 : Up. 381000 : Sen. 15,472,156 : Cost 40.78197098 : Time 492.56s : 39346.72 words/s : L.r. 6.1478e-05
[2019-02-01 23:47:21] Ep. 9 : Up. 382000 : Sen. 16,727,741 : Cost 40.32298279 : Time 445.96s : 43627.52 words/s : L.r. 6.1397e-05
[2019-02-01 23:54:49] Ep. 9 : Up. 383000 : Sen. 18,012,632 : Cost 39.55649567 : Time 447.72s : 43471.86 words/s : L.r. 6.1317e-05
[2019-02-02 00:02:15] Ep. 9 : Up. 384000 : Sen. 19,281,898 : Cost 39.99783707 : Time 445.80s : 43540.09 words/s : L.r. 6.1237e-05
[2019-02-02 00:09:41] Ep. 9 : Up. 385000 : Sen. 20,523,092 : Cost 40.52525711 : Time 446.52s : 43397.40 words/s : L.r. 6.1158e-05
[2019-02-02 00:09:41] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 00:09:42] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 00:09:44] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 00:09:47] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 00:09:48] [valid] Ep. 9 : Up. 385000 : ce-mean-words : 1.56568 : new best
[2019-02-02 00:10:29] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-02 00:10:30] [valid] Ep. 9 : Up. 385000 : bleu-detok : 25.9578 : new best
[2019-02-02 00:17:55] Ep. 9 : Up. 386000 : Sen. 21,794,649 : Cost 40.04269409 : Time 494.13s : 39265.39 words/s : L.r. 6.1078e-05
[2019-02-02 00:25:21] Ep. 9 : Up. 387000 : Sen. 23,067,468 : Cost 39.63861847 : Time 445.46s : 43483.52 words/s : L.r. 6.0999e-05
[2019-02-02 00:32:50] Ep. 9 : Up. 388000 : Sen. 24,341,587 : Cost 40.29454422 : Time 449.55s : 43738.32 words/s : L.r. 6.0921e-05
[2019-02-02 00:40:14] Ep. 9 : Up. 389000 : Sen. 25,600,990 : Cost 39.68978500 : Time 443.44s : 43275.03 words/s : L.r. 6.0842e-05
[2019-02-02 00:47:40] Ep. 9 : Up. 390000 : Sen. 26,867,221 : Cost 39.93168259 : Time 445.89s : 43489.96 words/s : L.r. 6.0764e-05
[2019-02-02 00:47:40] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 00:47:41] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 00:47:42] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 00:47:46] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 00:47:47] [valid] Ep. 9 : Up. 390000 : ce-mean-words : 1.5654 : new best
[2019-02-02 00:48:27] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-02 00:48:27] [valid] Ep. 9 : Up. 390000 : bleu-detok : 26.1059 : new best
[2019-02-02 00:55:54] Ep. 9 : Up. 391000 : Sen. 28,131,110 : Cost 40.06206894 : Time 494.01s : 39316.25 words/s : L.r. 6.0687e-05
[2019-02-02 01:03:20] Ep. 9 : Up. 392000 : Sen. 29,392,469 : Cost 40.25695038 : Time 446.31s : 43525.98 words/s : L.r. 6.0609e-05
[2019-02-02 01:10:45] Ep. 9 : Up. 393000 : Sen. 30,653,251 : Cost 39.86607742 : Time 444.86s : 43381.62 words/s : L.r. 6.0532e-05
[2019-02-02 01:18:12] Ep. 9 : Up. 394000 : Sen. 31,923,859 : Cost 40.03374863 : Time 447.33s : 43615.02 words/s : L.r. 6.0455e-05
[2019-02-02 01:25:38] Ep. 9 : Up. 395000 : Sen. 33,173,063 : Cost 40.69159698 : Time 445.81s : 43593.97 words/s : L.r. 6.0379e-05
[2019-02-02 01:25:38] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 01:25:39] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 01:25:40] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 01:25:44] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 01:25:45] [valid] Ep. 9 : Up. 395000 : ce-mean-words : 1.5649 : new best
[2019-02-02 01:26:26] [valid] Ep. 9 : Up. 395000 : bleu-detok : 25.9425 : stalled 1 times (last best: 26.1059)
[2019-02-02 01:33:52] Ep. 9 : Up. 396000 : Sen. 34,446,854 : Cost 39.70718002 : Time 494.51s : 39291.05 words/s : L.r. 6.0302e-05
[2019-02-02 01:41:15] Ep. 9 : Up. 397000 : Sen. 35,692,856 : Cost 40.37262344 : Time 442.68s : 43505.44 words/s : L.r. 6.0226e-05
[2019-02-02 01:48:42] Ep. 9 : Up. 398000 : Sen. 36,978,858 : Cost 39.49330139 : Time 446.46s : 43671.58 words/s : L.r. 6.0151e-05
[2019-02-02 01:56:05] Ep. 9 : Up. 399000 : Sen. 38,231,135 : Cost 40.54500580 : Time 443.85s : 43699.05 words/s : L.r. 6.0075e-05
[2019-02-02 02:03:31] Ep. 9 : Up. 400000 : Sen. 39,502,702 : Cost 39.80711365 : Time 445.98s : 43603.17 words/s : L.r. 6.0000e-05
[2019-02-02 02:03:31] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 02:03:33] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 02:03:34] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 02:03:38] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 02:03:38] [valid] Ep. 9 : Up. 400000 : ce-mean-words : 1.56446 : new best
[2019-02-02 02:04:20] [valid] Ep. 9 : Up. 400000 : bleu-detok : 25.9895 : stalled 2 times (last best: 26.1059)
[2019-02-02 02:11:45] Ep. 9 : Up. 401000 : Sen. 40,775,136 : Cost 39.64837646 : Time 493.41s : 39270.56 words/s : L.r. 5.9925e-05
[2019-02-02 02:19:10] Ep. 9 : Up. 402000 : Sen. 42,035,685 : Cost 40.22729111 : Time 445.63s : 43488.07 words/s : L.r. 5.9851e-05
[2019-02-02 02:26:36] Ep. 9 : Up. 403000 : Sen. 43,307,973 : Cost 39.76238632 : Time 445.77s : 43494.21 words/s : L.r. 5.9776e-05
[2019-02-02 02:34:04] Ep. 9 : Up. 404000 : Sen. 44,562,475 : Cost 40.41910553 : Time 447.87s : 43542.93 words/s : L.r. 5.9702e-05
[2019-02-02 02:41:29] Ep. 9 : Up. 405000 : Sen. 45,825,911 : Cost 40.05511475 : Time 444.98s : 43510.86 words/s : L.r. 5.9628e-05
[2019-02-02 02:41:29] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 02:41:30] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 02:41:32] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 02:41:35] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 02:41:36] [valid] Ep. 9 : Up. 405000 : ce-mean-words : 1.56379 : new best
[2019-02-02 02:42:16] [valid] Ep. 9 : Up. 405000 : bleu-detok : 25.8847 : stalled 3 times (last best: 26.1059)
[2019-02-02 02:49:40] Ep. 9 : Up. 406000 : Sen. 47,088,623 : Cost 39.97498322 : Time 491.37s : 39356.96 words/s : L.r. 5.9555e-05
[2019-02-02 02:57:05] Ep. 9 : Up. 407000 : Sen. 48,346,863 : Cost 40.09477997 : Time 444.11s : 43557.52 words/s : L.r. 5.9482e-05
[2019-02-02 03:04:32] Ep. 9 : Up. 408000 : Sen. 49,607,549 : Cost 40.25872421 : Time 447.15s : 43491.36 words/s : L.r. 5.9409e-05
[2019-02-02 03:11:59] Ep. 9 : Up. 409000 : Sen. 50,894,508 : Cost 39.59227371 : Time 447.59s : 43671.73 words/s : L.r. 5.9336e-05
[2019-02-02 03:19:25] Ep. 9 : Up. 410000 : Sen. 52,143,492 : Cost 40.43083191 : Time 446.06s : 43461.66 words/s : L.r. 5.9264e-05
[2019-02-02 03:19:25] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 03:19:27] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 03:19:28] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 03:19:32] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 03:19:32] [valid] Ep. 9 : Up. 410000 : ce-mean-words : 1.56269 : new best
[2019-02-02 03:20:08] [valid] Ep. 9 : Up. 410000 : bleu-detok : 25.9069 : stalled 4 times (last best: 26.1059)
[2019-02-02 03:27:34] Ep. 9 : Up. 411000 : Sen. 53,408,799 : Cost 40.05148315 : Time 488.46s : 39681.51 words/s : L.r. 5.9192e-05
[2019-02-02 03:34:59] Ep. 9 : Up. 412000 : Sen. 54,680,341 : Cost 39.97224045 : Time 445.10s : 43719.93 words/s : L.r. 5.9120e-05
[2019-02-02 03:42:24] Ep. 9 : Up. 413000 : Sen. 55,946,678 : Cost 39.95199966 : Time 445.38s : 43574.35 words/s : L.r. 5.9048e-05
[2019-02-02 03:49:50] Ep. 9 : Up. 414000 : Sen. 57,206,797 : Cost 40.09953690 : Time 445.25s : 43613.84 words/s : L.r. 5.8977e-05
[2019-02-02 03:56:14] Seen 58282224 samples
[2019-02-02 03:56:14] Starting epoch 10
[2019-02-02 03:56:14] [data] Shuffling data
[2019-02-02 03:56:18] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-02 03:58:22] Ep. 10 : Up. 415000 : Sen. 157,196 : Cost 39.78154373 : Time 512.59s : 36664.96 words/s : L.r. 5.8906e-05
[2019-02-02 03:58:22] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 03:58:23] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 03:58:25] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 03:58:30] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 03:58:31] [valid] Ep. 10 : Up. 415000 : ce-mean-words : 1.56207 : new best
[2019-02-02 03:59:07] [valid] Ep. 10 : Up. 415000 : bleu-detok : 25.8402 : stalled 5 times (last best: 26.1059)
[2019-02-02 04:06:33] Ep. 10 : Up. 416000 : Sen. 1,395,100 : Cost 40.46808624 : Time 490.38s : 39369.12 words/s : L.r. 5.8835e-05
[2019-02-02 04:14:02] Ep. 10 : Up. 417000 : Sen. 2,710,969 : Cost 38.94596100 : Time 449.95s : 43778.28 words/s : L.r. 5.8764e-05
[2019-02-02 04:21:26] Ep. 10 : Up. 418000 : Sen. 3,948,356 : Cost 40.46693420 : Time 443.39s : 43375.73 words/s : L.r. 5.8694e-05
[2019-02-02 04:28:51] Ep. 10 : Up. 419000 : Sen. 5,207,042 : Cost 40.09773254 : Time 445.63s : 43516.96 words/s : L.r. 5.8624e-05
[2019-02-02 04:36:17] Ep. 10 : Up. 420000 : Sen. 6,455,959 : Cost 40.22838593 : Time 445.48s : 43382.87 words/s : L.r. 5.8554e-05
[2019-02-02 04:36:17] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 04:36:18] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 04:36:19] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 04:36:23] [valid] Ep. 10 : Up. 420000 : ce-mean-words : 1.56212 : stalled 1 times (last best: 1.56207)
[2019-02-02 04:36:59] [valid] Ep. 10 : Up. 420000 : bleu-detok : 25.8804 : stalled 6 times (last best: 26.1059)
[2019-02-02 04:44:26] Ep. 10 : Up. 421000 : Sen. 7,731,927 : Cost 39.65823746 : Time 489.09s : 39792.46 words/s : L.r. 5.8484e-05
[2019-02-02 04:51:53] Ep. 10 : Up. 422000 : Sen. 9,013,212 : Cost 39.66390228 : Time 447.36s : 43677.79 words/s : L.r. 5.8415e-05
[2019-02-02 04:59:19] Ep. 10 : Up. 423000 : Sen. 10,288,069 : Cost 39.75844193 : Time 445.88s : 43661.13 words/s : L.r. 5.8346e-05
[2019-02-02 05:06:44] Ep. 10 : Up. 424000 : Sen. 11,517,262 : Cost 41.10408020 : Time 444.32s : 43524.59 words/s : L.r. 5.8277e-05
[2019-02-02 05:14:08] Ep. 10 : Up. 425000 : Sen. 12,807,475 : Cost 38.84505081 : Time 444.07s : 43480.91 words/s : L.r. 5.8209e-05
[2019-02-02 05:14:08] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 05:14:09] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 05:14:10] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 05:14:14] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 05:14:15] [valid] Ep. 10 : Up. 425000 : ce-mean-words : 1.56205 : new best
[2019-02-02 05:14:53] [valid] Ep. 10 : Up. 425000 : bleu-detok : 25.9953 : stalled 7 times (last best: 26.1059)
[2019-02-02 05:22:21] Ep. 10 : Up. 426000 : Sen. 14,067,765 : Cost 40.29552460 : Time 493.64s : 39519.85 words/s : L.r. 5.8140e-05
[2019-02-02 05:29:50] Ep. 10 : Up. 427000 : Sen. 15,341,829 : Cost 39.94411087 : Time 448.25s : 43617.59 words/s : L.r. 5.8072e-05
[2019-02-02 05:37:12] Ep. 10 : Up. 428000 : Sen. 16,627,216 : Cost 38.95421982 : Time 442.77s : 43476.40 words/s : L.r. 5.8004e-05
[2019-02-02 05:44:39] Ep. 10 : Up. 429000 : Sen. 17,873,808 : Cost 40.77200699 : Time 446.69s : 43701.07 words/s : L.r. 5.7937e-05
[2019-02-02 05:52:02] Ep. 10 : Up. 430000 : Sen. 19,127,064 : Cost 40.01390457 : Time 443.02s : 43493.63 words/s : L.r. 5.7869e-05
[2019-02-02 05:52:02] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 05:52:03] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 05:52:05] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 05:52:09] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 05:52:09] [valid] Ep. 10 : Up. 430000 : ce-mean-words : 1.56204 : new best
[2019-02-02 05:52:48] [valid] Ep. 10 : Up. 430000 : bleu-detok : 26.0168 : stalled 8 times (last best: 26.1059)
[2019-02-02 06:00:16] Ep. 10 : Up. 431000 : Sen. 20,411,685 : Cost 39.51755905 : Time 493.93s : 39590.04 words/s : L.r. 5.7802e-05
[2019-02-02 06:07:43] Ep. 10 : Up. 432000 : Sen. 21,673,014 : Cost 40.46791458 : Time 447.37s : 43719.30 words/s : L.r. 5.7735e-05
[2019-02-02 06:15:10] Ep. 10 : Up. 433000 : Sen. 22,932,049 : Cost 40.05894470 : Time 446.87s : 43326.27 words/s : L.r. 5.7668e-05
[2019-02-02 06:22:38] Ep. 10 : Up. 434000 : Sen. 24,214,451 : Cost 39.43220139 : Time 448.27s : 43333.48 words/s : L.r. 5.7602e-05
[2019-02-02 06:30:02] Ep. 10 : Up. 435000 : Sen. 25,465,322 : Cost 40.38554764 : Time 443.90s : 43565.98 words/s : L.r. 5.7536e-05
[2019-02-02 06:30:02] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 06:30:04] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 06:30:05] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 06:30:09] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 06:30:09] [valid] Ep. 10 : Up. 435000 : ce-mean-words : 1.56199 : new best
[2019-02-02 06:30:48] [valid] Ep. 10 : Up. 435000 : bleu-detok : 25.9163 : stalled 9 times (last best: 26.1059)
[2019-02-02 06:38:15] Ep. 10 : Up. 436000 : Sen. 26,728,463 : Cost 40.03638458 : Time 492.85s : 39534.73 words/s : L.r. 5.7470e-05
[2019-02-02 06:45:41] Ep. 10 : Up. 437000 : Sen. 27,993,840 : Cost 39.82181168 : Time 445.86s : 43502.26 words/s : L.r. 5.7404e-05
[2019-02-02 06:53:09] Ep. 10 : Up. 438000 : Sen. 29,245,207 : Cost 40.67680359 : Time 448.07s : 43584.37 words/s : L.r. 5.7338e-05
[2019-02-02 07:00:34] Ep. 10 : Up. 439000 : Sen. 30,532,275 : Cost 39.03577805 : Time 445.00s : 43413.56 words/s : L.r. 5.7273e-05
[2019-02-02 07:08:04] Ep. 10 : Up. 440000 : Sen. 31,793,614 : Cost 40.45832062 : Time 449.51s : 43594.20 words/s : L.r. 5.7208e-05
[2019-02-02 07:08:04] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 07:08:05] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 07:08:06] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 07:08:10] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 07:08:11] [valid] Ep. 10 : Up. 440000 : ce-mean-words : 1.56142 : new best
[2019-02-02 07:08:50] [valid] Ep. 10 : Up. 440000 : bleu-detok : 25.8784 : stalled 10 times (last best: 26.1059)
[2019-02-02 07:16:16] Ep. 10 : Up. 441000 : Sen. 33,077,313 : Cost 39.35542297 : Time 491.99s : 39463.90 words/s : L.r. 5.7143e-05
[2019-02-02 07:23:40] Ep. 10 : Up. 442000 : Sen. 34,324,766 : Cost 40.40403366 : Time 444.03s : 43535.24 words/s : L.r. 5.7078e-05
[2019-02-02 07:31:07] Ep. 10 : Up. 443000 : Sen. 35,595,899 : Cost 39.99820709 : Time 447.40s : 43712.93 words/s : L.r. 5.7014e-05
[2019-02-02 07:38:32] Ep. 10 : Up. 444000 : Sen. 36,858,419 : Cost 39.80256271 : Time 444.94s : 43505.92 words/s : L.r. 5.6949e-05
[2019-02-02 07:46:00] Ep. 10 : Up. 445000 : Sen. 38,132,755 : Cost 39.79526138 : Time 447.50s : 43576.51 words/s : L.r. 5.6885e-05
[2019-02-02 07:46:00] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 07:46:01] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 07:46:02] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 07:46:06] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 07:46:07] [valid] Ep. 10 : Up. 445000 : ce-mean-words : 1.56099 : new best
[2019-02-02 07:46:44] [valid] Ep. 10 : Up. 445000 : bleu-detok : 25.8641 : stalled 11 times (last best: 26.1059)
[2019-02-02 07:54:09] Ep. 10 : Up. 446000 : Sen. 39,382,077 : Cost 40.43547440 : Time 489.24s : 39616.45 words/s : L.r. 5.6822e-05
[2019-02-02 08:01:32] Ep. 10 : Up. 447000 : Sen. 40,661,197 : Cost 39.20507050 : Time 443.47s : 43426.17 words/s : L.r. 5.6758e-05
[2019-02-02 08:08:58] Ep. 10 : Up. 448000 : Sen. 41,926,004 : Cost 40.08023071 : Time 445.88s : 43688.12 words/s : L.r. 5.6695e-05
[2019-02-02 08:16:21] Ep. 10 : Up. 449000 : Sen. 43,181,628 : Cost 40.11774063 : Time 443.32s : 43605.64 words/s : L.r. 5.6632e-05
[2019-02-02 08:23:50] Ep. 10 : Up. 450000 : Sen. 44,454,376 : Cost 40.11156464 : Time 448.26s : 43739.48 words/s : L.r. 5.6569e-05
[2019-02-02 08:23:50] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 08:23:51] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 08:23:52] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 08:23:56] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 08:23:57] [valid] Ep. 10 : Up. 450000 : ce-mean-words : 1.56076 : new best
[2019-02-02 08:24:32] [valid] Ep. 10 : Up. 450000 : bleu-detok : 25.852 : stalled 12 times (last best: 26.1059)
[2019-02-02 08:31:58] Ep. 10 : Up. 451000 : Sen. 45,737,542 : Cost 39.33527374 : Time 487.83s : 39735.10 words/s : L.r. 5.6506e-05
[2019-02-02 08:39:23] Ep. 10 : Up. 452000 : Sen. 46,988,580 : Cost 40.49326706 : Time 445.85s : 43717.00 words/s : L.r. 5.6443e-05
[2019-02-02 08:46:48] Ep. 10 : Up. 453000 : Sen. 48,241,578 : Cost 40.09347534 : Time 444.57s : 43434.36 words/s : L.r. 5.6381e-05
[2019-02-02 08:54:13] Ep. 10 : Up. 454000 : Sen. 49,518,265 : Cost 39.50582504 : Time 445.45s : 43514.04 words/s : L.r. 5.6319e-05
[2019-02-02 09:01:40] Ep. 10 : Up. 455000 : Sen. 50,775,690 : Cost 40.31108093 : Time 446.78s : 43570.17 words/s : L.r. 5.6257e-05
[2019-02-02 09:01:40] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 09:01:41] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 09:01:43] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 09:01:46] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 09:01:47] [valid] Ep. 10 : Up. 455000 : ce-mean-words : 1.56026 : new best
[2019-02-02 09:02:25] [valid] Ep. 10 : Up. 455000 : bleu-detok : 25.8104 : stalled 13 times (last best: 26.1059)
[2019-02-02 09:09:53] Ep. 10 : Up. 456000 : Sen. 52,047,395 : Cost 39.84662628 : Time 492.39s : 39641.67 words/s : L.r. 5.6195e-05
[2019-02-02 09:17:17] Ep. 10 : Up. 457000 : Sen. 53,308,000 : Cost 39.94072723 : Time 444.19s : 43583.02 words/s : L.r. 5.6134e-05
[2019-02-02 09:24:44] Ep. 10 : Up. 458000 : Sen. 54,601,063 : Cost 39.39191055 : Time 447.70s : 43683.37 words/s : L.r. 5.6072e-05
[2019-02-02 09:32:07] Ep. 10 : Up. 459000 : Sen. 55,844,927 : Cost 40.27843094 : Time 442.38s : 43469.88 words/s : L.r. 5.6011e-05
[2019-02-02 09:39:36] Ep. 10 : Up. 460000 : Sen. 57,117,953 : Cost 40.17055893 : Time 449.23s : 43709.17 words/s : L.r. 5.5950e-05
[2019-02-02 09:39:36] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 09:39:37] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 09:39:39] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 09:39:42] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 09:39:43] [valid] Ep. 10 : Up. 460000 : ce-mean-words : 1.55915 : new best
[2019-02-02 09:40:21] [valid] Ep. 10 : Up. 460000 : bleu-detok : 25.8253 : stalled 14 times (last best: 26.1059)
[2019-02-02 09:47:22] Seen 58282224 samples
[2019-02-02 09:47:22] Starting epoch 11
[2019-02-02 09:47:22] [data] Shuffling data
[2019-02-02 09:47:25] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-02 09:48:52] Ep. 11 : Up. 461000 : Sen. 42,896 : Cost 40.22494507 : Time 556.40s : 33555.39 words/s : L.r. 5.5890e-05
[2019-02-02 09:56:19] Ep. 11 : Up. 462000 : Sen. 1,328,107 : Cost 39.28746414 : Time 446.55s : 43667.34 words/s : L.r. 5.5829e-05
[2019-02-02 10:03:44] Ep. 11 : Up. 463000 : Sen. 2,571,138 : Cost 40.36520767 : Time 444.61s : 43477.43 words/s : L.r. 5.5769e-05
[2019-02-02 10:11:08] Ep. 11 : Up. 464000 : Sen. 3,853,225 : Cost 39.41038513 : Time 444.87s : 43622.12 words/s : L.r. 5.5709e-05
[2019-02-02 10:18:33] Ep. 11 : Up. 465000 : Sen. 5,109,141 : Cost 39.93700027 : Time 445.00s : 43490.80 words/s : L.r. 5.5649e-05
[2019-02-02 10:18:33] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 10:18:35] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 10:18:36] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 10:18:40] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 10:18:40] [valid] Ep. 11 : Up. 465000 : ce-mean-words : 1.55898 : new best
[2019-02-02 10:19:18] [valid] Ep. 11 : Up. 465000 : bleu-detok : 25.7967 : stalled 15 times (last best: 26.1059)
[2019-02-02 10:26:44] Ep. 11 : Up. 466000 : Sen. 6,362,853 : Cost 40.14191818 : Time 490.25s : 39622.99 words/s : L.r. 5.5589e-05
[2019-02-02 10:34:11] Ep. 11 : Up. 467000 : Sen. 7,656,379 : Cost 39.36922073 : Time 447.69s : 43803.88 words/s : L.r. 5.5529e-05
[2019-02-02 10:41:34] Ep. 11 : Up. 468000 : Sen. 8,907,348 : Cost 40.02390289 : Time 442.44s : 43486.86 words/s : L.r. 5.5470e-05
[2019-02-02 10:49:00] Ep. 11 : Up. 469000 : Sen. 10,168,859 : Cost 40.12106323 : Time 446.11s : 43706.12 words/s : L.r. 5.5411e-05
[2019-02-02 10:56:27] Ep. 11 : Up. 470000 : Sen. 11,431,324 : Cost 40.07492828 : Time 447.01s : 43661.22 words/s : L.r. 5.5352e-05
[2019-02-02 10:56:27] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 10:56:28] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 10:56:29] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 10:56:33] [valid] Ep. 11 : Up. 470000 : ce-mean-words : 1.5592 : stalled 1 times (last best: 1.55898)
[2019-02-02 10:57:11] [valid] Ep. 11 : Up. 470000 : bleu-detok : 25.9582 : stalled 16 times (last best: 26.1059)
[2019-02-02 11:04:36] Ep. 11 : Up. 471000 : Sen. 12,717,896 : Cost 39.35248947 : Time 489.41s : 39726.41 words/s : L.r. 5.5293e-05
[2019-02-02 11:12:01] Ep. 11 : Up. 472000 : Sen. 13,972,448 : Cost 40.07680130 : Time 444.58s : 43547.66 words/s : L.r. 5.5234e-05
[2019-02-02 11:19:29] Ep. 11 : Up. 473000 : Sen. 15,246,238 : Cost 39.89440536 : Time 447.62s : 43700.83 words/s : L.r. 5.5176e-05
[2019-02-02 11:26:55] Ep. 11 : Up. 474000 : Sen. 16,500,777 : Cost 40.08907318 : Time 445.98s : 43349.66 words/s : L.r. 5.5118e-05
[2019-02-02 11:34:23] Ep. 11 : Up. 475000 : Sen. 17,766,058 : Cost 39.98367310 : Time 448.68s : 43413.79 words/s : L.r. 5.5060e-05
[2019-02-02 11:34:23] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 11:34:24] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 11:34:26] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 11:34:29] [valid] Ep. 11 : Up. 475000 : ce-mean-words : 1.55914 : stalled 2 times (last best: 1.55898)
[2019-02-02 11:35:07] [valid] Ep. 11 : Up. 475000 : bleu-detok : 25.9976 : stalled 17 times (last best: 26.1059)
[2019-02-02 11:42:33] Ep. 11 : Up. 476000 : Sen. 19,036,680 : Cost 39.52960205 : Time 489.50s : 39627.88 words/s : L.r. 5.5002e-05
[2019-02-02 11:49:59] Ep. 11 : Up. 477000 : Sen. 20,303,653 : Cost 39.92293549 : Time 446.13s : 43634.77 words/s : L.r. 5.4944e-05
[2019-02-02 11:57:22] Ep. 11 : Up. 478000 : Sen. 21,563,854 : Cost 39.87584305 : Time 443.16s : 43597.70 words/s : L.r. 5.4887e-05
[2019-02-02 12:04:46] Ep. 11 : Up. 479000 : Sen. 22,834,716 : Cost 39.51605606 : Time 444.00s : 43627.05 words/s : L.r. 5.4829e-05
[2019-02-02 12:12:10] Ep. 11 : Up. 480000 : Sen. 24,088,673 : Cost 40.17765427 : Time 444.13s : 43598.02 words/s : L.r. 5.4772e-05
[2019-02-02 12:12:10] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 12:12:12] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 12:12:13] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 12:12:16] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 12:12:17] [valid] Ep. 11 : Up. 480000 : ce-mean-words : 1.55894 : new best
[2019-02-02 12:12:55] [valid] Ep. 11 : Up. 480000 : bleu-detok : 25.8674 : stalled 18 times (last best: 26.1059)
[2019-02-02 12:20:23] Ep. 11 : Up. 481000 : Sen. 25,358,258 : Cost 40.17345810 : Time 492.51s : 39846.99 words/s : L.r. 5.4715e-05
[2019-02-02 12:27:47] Ep. 11 : Up. 482000 : Sen. 26,589,896 : Cost 40.41379929 : Time 444.26s : 43185.20 words/s : L.r. 5.4659e-05
[2019-02-02 12:35:17] Ep. 11 : Up. 483000 : Sen. 27,889,111 : Cost 39.24231339 : Time 449.96s : 43667.61 words/s : L.r. 5.4602e-05
[2019-02-02 12:42:43] Ep. 11 : Up. 484000 : Sen. 29,165,074 : Cost 39.17039871 : Time 446.04s : 43304.48 words/s : L.r. 5.4545e-05
[2019-02-02 12:50:09] Ep. 11 : Up. 485000 : Sen. 30,448,381 : Cost 39.73669434 : Time 446.45s : 43744.82 words/s : L.r. 5.4489e-05
[2019-02-02 12:50:09] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 12:50:11] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 12:50:12] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 12:50:16] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 12:50:17] [valid] Ep. 11 : Up. 485000 : ce-mean-words : 1.55836 : new best
[2019-02-02 12:50:55] [valid] Ep. 11 : Up. 485000 : bleu-detok : 25.8827 : stalled 19 times (last best: 26.1059)
[2019-02-02 12:58:18] Ep. 11 : Up. 486000 : Sen. 31,682,402 : Cost 40.67181396 : Time 488.33s : 39477.19 words/s : L.r. 5.4433e-05
[2019-02-02 13:05:46] Ep. 11 : Up. 487000 : Sen. 32,955,714 : Cost 39.81288147 : Time 447.96s : 43672.47 words/s : L.r. 5.4377e-05
[2019-02-02 13:13:10] Ep. 11 : Up. 488000 : Sen. 34,234,564 : Cost 39.39038467 : Time 444.41s : 43600.74 words/s : L.r. 5.4321e-05
[2019-02-02 13:20:35] Ep. 11 : Up. 489000 : Sen. 35,488,034 : Cost 40.11581039 : Time 444.42s : 43511.79 words/s : L.r. 5.4266e-05
[2019-02-02 13:28:03] Ep. 11 : Up. 490000 : Sen. 36,739,216 : Cost 40.61067200 : Time 448.62s : 43779.90 words/s : L.r. 5.4210e-05
[2019-02-02 13:28:03] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 13:28:04] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 13:28:06] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 13:28:09] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 13:28:10] [valid] Ep. 11 : Up. 490000 : ce-mean-words : 1.55795 : new best
[2019-02-02 13:28:47] [valid] Ep. 11 : Up. 490000 : bleu-detok : 25.8556 : stalled 20 times (last best: 26.1059)
[2019-02-02 13:36:11] Ep. 11 : Up. 491000 : Sen. 38,022,460 : Cost 39.26237488 : Time 488.25s : 39621.98 words/s : L.r. 5.4155e-05
[2019-02-02 13:43:36] Ep. 11 : Up. 492000 : Sen. 39,280,777 : Cost 39.95038605 : Time 445.08s : 43612.74 words/s : L.r. 5.4100e-05
[2019-02-02 13:51:04] Ep. 11 : Up. 493000 : Sen. 40,567,662 : Cost 39.56959915 : Time 447.49s : 43657.45 words/s : L.r. 5.4045e-05
[2019-02-02 13:58:26] Ep. 11 : Up. 494000 : Sen. 41,828,301 : Cost 39.57735062 : Time 442.14s : 43535.60 words/s : L.r. 5.3991e-05
[2019-02-02 14:05:50] Ep. 11 : Up. 495000 : Sen. 43,067,470 : Cost 40.65647888 : Time 443.80s : 43653.48 words/s : L.r. 5.3936e-05
[2019-02-02 14:05:50] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 14:05:51] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 14:05:53] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 14:05:56] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 14:05:57] [valid] Ep. 11 : Up. 495000 : ce-mean-words : 1.55743 : new best
[2019-02-02 14:06:35] [valid] Ep. 11 : Up. 495000 : bleu-detok : 25.7981 : stalled 21 times (last best: 26.1059)
[2019-02-02 14:13:58] Ep. 11 : Up. 496000 : Sen. 44,319,480 : Cost 40.14334488 : Time 487.91s : 39590.67 words/s : L.r. 5.3882e-05
[2019-02-02 14:21:26] Ep. 11 : Up. 497000 : Sen. 45,617,527 : Cost 39.02814865 : Time 447.77s : 43647.63 words/s : L.r. 5.3827e-05
[2019-02-02 14:28:53] Ep. 11 : Up. 498000 : Sen. 46,861,325 : Cost 40.79795456 : Time 447.30s : 43724.61 words/s : L.r. 5.3773e-05
[2019-02-02 14:36:17] Ep. 11 : Up. 499000 : Sen. 48,136,314 : Cost 39.31994629 : Time 444.57s : 43424.67 words/s : L.r. 5.3719e-05
[2019-02-02 14:43:44] Ep. 11 : Up. 500000 : Sen. 49,404,824 : Cost 39.84162521 : Time 446.08s : 43688.36 words/s : L.r. 5.3666e-05
[2019-02-02 14:43:44] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 14:43:45] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 14:43:46] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 14:43:50] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 14:43:50] [valid] Ep. 11 : Up. 500000 : ce-mean-words : 1.55645 : new best
[2019-02-02 14:44:26] [valid] Ep. 11 : Up. 500000 : bleu-detok : 25.8392 : stalled 22 times (last best: 26.1059)
[2019-02-02 14:51:52] Ep. 11 : Up. 501000 : Sen. 50,679,660 : Cost 39.52625275 : Time 488.68s : 39653.99 words/s : L.r. 5.3612e-05
[2019-02-02 14:59:19] Ep. 11 : Up. 502000 : Sen. 51,940,979 : Cost 40.18310928 : Time 446.85s : 43669.25 words/s : L.r. 5.3559e-05
[2019-02-02 15:06:43] Ep. 11 : Up. 503000 : Sen. 53,200,001 : Cost 39.62813187 : Time 444.01s : 43267.00 words/s : L.r. 5.3505e-05
[2019-02-02 15:14:11] Ep. 11 : Up. 504000 : Sen. 54,477,179 : Cost 39.77236557 : Time 447.47s : 43580.27 words/s : L.r. 5.3452e-05
[2019-02-02 15:21:37] Ep. 11 : Up. 505000 : Sen. 55,710,270 : Cost 40.59668732 : Time 446.54s : 43380.45 words/s : L.r. 5.3399e-05
[2019-02-02 15:21:37] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 15:21:38] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 15:21:40] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 15:21:43] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 15:21:44] [valid] Ep. 11 : Up. 505000 : ce-mean-words : 1.5556 : new best
[2019-02-02 15:22:21] [valid] Ep. 11 : Up. 505000 : bleu-detok : 25.9789 : stalled 23 times (last best: 26.1059)
[2019-02-02 15:29:49] Ep. 11 : Up. 506000 : Sen. 56,994,644 : Cost 39.61555862 : Time 491.57s : 39783.66 words/s : L.r. 5.3347e-05
[2019-02-02 15:37:07] Ep. 11 : Up. 507000 : Sen. 58,210,846 : Cost 40.26400757 : Time 438.10s : 42979.58 words/s : L.r. 5.3294e-05
[2019-02-02 15:37:29] Seen 58282224 samples
[2019-02-02 15:37:29] Starting epoch 12
[2019-02-02 15:37:29] [data] Shuffling data
[2019-02-02 15:37:33] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-02 15:45:43] Ep. 12 : Up. 508000 : Sen. 1,189,713 : Cost 39.25842667 : Time 516.42s : 37080.82 words/s : L.r. 5.3241e-05
[2019-02-02 15:53:11] Ep. 12 : Up. 509000 : Sen. 2,469,063 : Cost 39.61463928 : Time 447.95s : 43746.10 words/s : L.r. 5.3189e-05
[2019-02-02 16:00:35] Ep. 12 : Up. 510000 : Sen. 3,701,734 : Cost 40.69182587 : Time 443.65s : 43447.10 words/s : L.r. 5.3137e-05
[2019-02-02 16:00:35] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 16:00:36] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 16:00:37] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 16:00:41] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 16:00:42] [valid] Ep. 12 : Up. 510000 : ce-mean-words : 1.55536 : new best
[2019-02-02 16:01:19] [valid] Ep. 12 : Up. 510000 : bleu-detok : 25.9621 : stalled 24 times (last best: 26.1059)
[2019-02-02 16:08:46] Ep. 12 : Up. 511000 : Sen. 4,982,328 : Cost 39.35852432 : Time 491.27s : 39629.26 words/s : L.r. 5.3085e-05
[2019-02-02 16:16:10] Ep. 12 : Up. 512000 : Sen. 6,253,082 : Cost 39.66551971 : Time 443.67s : 43675.69 words/s : L.r. 5.3033e-05
[2019-02-02 16:23:36] Ep. 12 : Up. 513000 : Sen. 7,536,813 : Cost 38.97526169 : Time 446.40s : 43372.10 words/s : L.r. 5.2981e-05
[2019-02-02 16:31:02] Ep. 12 : Up. 514000 : Sen. 8,767,264 : Cost 40.94451141 : Time 446.26s : 43616.19 words/s : L.r. 5.2930e-05
[2019-02-02 16:38:30] Ep. 12 : Up. 515000 : Sen. 10,059,544 : Cost 38.92814636 : Time 447.30s : 43421.25 words/s : L.r. 5.2878e-05
[2019-02-02 16:38:30] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 16:38:31] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 16:38:32] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 16:38:36] [valid] Ep. 12 : Up. 515000 : ce-mean-words : 1.55554 : stalled 1 times (last best: 1.55536)
[2019-02-02 16:39:15] [valid] Ep. 12 : Up. 515000 : bleu-detok : 25.9675 : stalled 25 times (last best: 26.1059)
[2019-02-02 16:46:41] Ep. 12 : Up. 516000 : Sen. 11,300,008 : Cost 40.83825302 : Time 490.88s : 39757.45 words/s : L.r. 5.2827e-05
[2019-02-02 16:54:09] Ep. 12 : Up. 517000 : Sen. 12,597,464 : Cost 39.01382828 : Time 448.02s : 43716.40 words/s : L.r. 5.2776e-05
[2019-02-02 17:01:31] Ep. 12 : Up. 518000 : Sen. 13,856,168 : Cost 39.71492767 : Time 442.80s : 43473.73 words/s : L.r. 5.2725e-05
[2019-02-02 17:08:55] Ep. 12 : Up. 519000 : Sen. 15,109,861 : Cost 39.93229294 : Time 443.81s : 43619.05 words/s : L.r. 5.2674e-05
[2019-02-02 17:16:20] Ep. 12 : Up. 520000 : Sen. 16,356,874 : Cost 40.46653748 : Time 444.89s : 43615.21 words/s : L.r. 5.2623e-05
[2019-02-02 17:16:20] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 17:16:21] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 17:16:23] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 17:16:26] [valid] Ep. 12 : Up. 520000 : ce-mean-words : 1.55539 : stalled 2 times (last best: 1.55536)
[2019-02-02 17:17:05] [valid] Ep. 12 : Up. 520000 : bleu-detok : 25.9541 : stalled 26 times (last best: 26.1059)
[2019-02-02 17:24:32] Ep. 12 : Up. 521000 : Sen. 17,645,835 : Cost 39.03139496 : Time 491.66s : 39572.08 words/s : L.r. 5.2573e-05
[2019-02-02 17:31:56] Ep. 12 : Up. 522000 : Sen. 18,890,781 : Cost 40.42002487 : Time 444.30s : 43585.73 words/s : L.r. 5.2523e-05
[2019-02-02 17:39:21] Ep. 12 : Up. 523000 : Sen. 20,162,257 : Cost 39.34570694 : Time 445.03s : 43422.70 words/s : L.r. 5.2472e-05
[2019-02-02 17:46:51] Ep. 12 : Up. 524000 : Sen. 21,441,278 : Cost 39.90815353 : Time 449.58s : 43715.79 words/s : L.r. 5.2422e-05
[2019-02-02 17:54:14] Ep. 12 : Up. 525000 : Sen. 22,692,488 : Cost 40.00196075 : Time 443.82s : 43443.56 words/s : L.r. 5.2372e-05
[2019-02-02 17:54:14] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 17:54:16] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 17:54:17] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 17:54:21] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 17:54:21] [valid] Ep. 12 : Up. 525000 : ce-mean-words : 1.55456 : new best
[2019-02-02 17:55:01] [valid] Ep. 12 : Up. 525000 : bleu-detok : 25.9601 : stalled 27 times (last best: 26.1059)
[2019-02-02 18:02:24] Ep. 12 : Up. 526000 : Sen. 23,960,440 : Cost 39.39047241 : Time 489.92s : 39325.00 words/s : L.r. 5.2322e-05
[2019-02-02 18:09:51] Ep. 12 : Up. 527000 : Sen. 25,228,566 : Cost 39.86658096 : Time 446.26s : 43650.71 words/s : L.r. 5.2273e-05
[2019-02-02 18:17:13] Ep. 12 : Up. 528000 : Sen. 26,465,787 : Cost 40.23480225 : Time 442.33s : 43528.59 words/s : L.r. 5.2223e-05
[2019-02-02 18:24:39] Ep. 12 : Up. 529000 : Sen. 27,757,368 : Cost 39.13336945 : Time 445.93s : 43718.25 words/s : L.r. 5.2174e-05
[2019-02-02 18:32:04] Ep. 12 : Up. 530000 : Sen. 29,006,825 : Cost 40.33035660 : Time 444.71s : 43637.77 words/s : L.r. 5.2125e-05
[2019-02-02 18:32:04] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 18:32:05] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 18:32:06] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 18:32:10] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 18:32:11] [valid] Ep. 12 : Up. 530000 : ce-mean-words : 1.55363 : new best
[2019-02-02 18:32:51] [valid] Ep. 12 : Up. 530000 : bleu-detok : 25.9012 : stalled 28 times (last best: 26.1059)
[2019-02-02 18:40:20] Ep. 12 : Up. 531000 : Sen. 30,302,757 : Cost 39.22870255 : Time 496.59s : 39504.46 words/s : L.r. 5.2076e-05
[2019-02-02 18:47:46] Ep. 12 : Up. 532000 : Sen. 31,534,284 : Cost 40.85757828 : Time 446.15s : 43597.15 words/s : L.r. 5.2027e-05
[2019-02-02 18:55:12] Ep. 12 : Up. 533000 : Sen. 32,810,266 : Cost 39.41069412 : Time 445.68s : 43469.85 words/s : L.r. 5.1978e-05
[2019-02-02 19:02:35] Ep. 12 : Up. 534000 : Sen. 34,055,090 : Cost 40.22333908 : Time 442.89s : 43470.93 words/s : L.r. 5.1929e-05
[2019-02-02 19:10:01] Ep. 12 : Up. 535000 : Sen. 35,346,459 : Cost 38.90575790 : Time 446.31s : 43445.18 words/s : L.r. 5.1881e-05
[2019-02-02 19:10:01] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 19:10:02] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 19:10:04] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 19:10:08] [valid] Ep. 12 : Up. 535000 : ce-mean-words : 1.55387 : stalled 1 times (last best: 1.55363)
[2019-02-02 19:10:46] [valid] Ep. 12 : Up. 535000 : bleu-detok : 25.9769 : stalled 29 times (last best: 26.1059)
[2019-02-02 19:18:14] Ep. 12 : Up. 536000 : Sen. 36,587,331 : Cost 41.05096054 : Time 492.26s : 39835.27 words/s : L.r. 5.1832e-05
[2019-02-02 19:25:39] Ep. 12 : Up. 537000 : Sen. 37,881,311 : Cost 38.79632187 : Time 445.24s : 43549.65 words/s : L.r. 5.1784e-05
[2019-02-02 19:33:03] Ep. 12 : Up. 538000 : Sen. 39,134,418 : Cost 39.98495102 : Time 444.15s : 43546.09 words/s : L.r. 5.1736e-05
[2019-02-02 19:40:27] Ep. 12 : Up. 539000 : Sen. 40,384,359 : Cost 40.30281448 : Time 444.31s : 43665.87 words/s : L.r. 5.1688e-05
[2019-02-02 19:47:52] Ep. 12 : Up. 540000 : Sen. 41,659,352 : Cost 39.37848663 : Time 445.14s : 43592.35 words/s : L.r. 5.1640e-05
[2019-02-02 19:47:52] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 19:47:54] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 19:47:55] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 19:47:58] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 19:47:59] [valid] Ep. 12 : Up. 540000 : ce-mean-words : 1.55351 : new best
[2019-02-02 19:48:38] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-02 19:48:39] [valid] Ep. 12 : Up. 540000 : bleu-detok : 26.1317 : new best
[2019-02-02 19:56:05] Ep. 12 : Up. 541000 : Sen. 42,947,168 : Cost 39.27717972 : Time 493.07s : 39462.58 words/s : L.r. 5.1592e-05
[2019-02-02 20:03:34] Ep. 12 : Up. 542000 : Sen. 44,205,224 : Cost 40.34001160 : Time 448.32s : 43660.07 words/s : L.r. 5.1544e-05
[2019-02-02 20:10:57] Ep. 12 : Up. 543000 : Sen. 45,441,720 : Cost 40.30421066 : Time 442.97s : 43469.15 words/s : L.r. 5.1497e-05
[2019-02-02 20:18:23] Ep. 12 : Up. 544000 : Sen. 46,738,499 : Cost 38.81180573 : Time 446.48s : 43390.79 words/s : L.r. 5.1450e-05
[2019-02-02 20:25:49] Ep. 12 : Up. 545000 : Sen. 47,990,103 : Cost 40.20866394 : Time 445.51s : 43572.30 words/s : L.r. 5.1402e-05
[2019-02-02 20:25:49] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 20:25:50] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 20:25:51] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 20:25:55] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 20:25:55] [valid] Ep. 12 : Up. 545000 : ce-mean-words : 1.55297 : new best
[2019-02-02 20:26:32] Saving model weights and runtime parameters to ./model.npz.best-bleu-detok.npz
[2019-02-02 20:26:33] [valid] Ep. 12 : Up. 545000 : bleu-detok : 26.2383 : new best
[2019-02-02 20:34:00] Ep. 12 : Up. 546000 : Sen. 49,248,674 : Cost 40.25429535 : Time 491.36s : 39780.20 words/s : L.r. 5.1355e-05
[2019-02-02 20:41:24] Ep. 12 : Up. 547000 : Sen. 50,529,721 : Cost 39.05316925 : Time 443.63s : 43429.10 words/s : L.r. 5.1308e-05
[2019-02-02 20:48:52] Ep. 12 : Up. 548000 : Sen. 51,798,285 : Cost 39.73581696 : Time 448.40s : 43448.62 words/s : L.r. 5.1261e-05
[2019-02-02 20:56:18] Ep. 12 : Up. 549000 : Sen. 53,047,347 : Cost 40.38016129 : Time 446.08s : 43555.84 words/s : L.r. 5.1215e-05
[2019-02-02 21:03:42] Ep. 12 : Up. 550000 : Sen. 54,330,123 : Cost 39.00318909 : Time 443.44s : 43506.59 words/s : L.r. 5.1168e-05
[2019-02-02 21:03:42] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 21:03:43] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 21:03:44] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 21:03:48] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 21:03:49] [valid] Ep. 12 : Up. 550000 : ce-mean-words : 1.5525 : new best
[2019-02-02 21:04:27] [valid] Ep. 12 : Up. 550000 : bleu-detok : 26.0979 : stalled 1 times (last best: 26.2383)
[2019-02-02 21:11:54] Ep. 12 : Up. 551000 : Sen. 55,593,236 : Cost 40.10642624 : Time 492.33s : 39704.15 words/s : L.r. 5.1122e-05
[2019-02-02 21:19:20] Ep. 12 : Up. 552000 : Sen. 56,840,224 : Cost 40.40061188 : Time 445.57s : 43557.17 words/s : L.r. 5.1075e-05
[2019-02-02 21:26:45] Ep. 12 : Up. 553000 : Sen. 58,077,453 : Cost 40.17552567 : Time 445.51s : 43083.34 words/s : L.r. 5.1029e-05
[2019-02-02 21:27:56] Seen 58282224 samples
[2019-02-02 21:27:56] Starting epoch 13
[2019-02-02 21:27:56] [data] Shuffling data
[2019-02-02 21:27:59] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-02 21:35:19] Ep. 13 : Up. 554000 : Sen. 1,048,800 : Cost 39.06026459 : Time 514.42s : 36808.33 words/s : L.r. 5.0983e-05
[2019-02-02 21:42:43] Ep. 13 : Up. 555000 : Sen. 2,314,610 : Cost 39.31361771 : Time 443.89s : 43378.39 words/s : L.r. 5.0937e-05
[2019-02-02 21:42:43] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 21:42:45] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 21:42:46] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 21:42:50] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-02 21:42:50] [valid] Ep. 13 : Up. 555000 : ce-mean-words : 1.55206 : new best
[2019-02-02 21:43:29] [valid] Ep. 13 : Up. 555000 : bleu-detok : 26.0728 : stalled 2 times (last best: 26.2383)
[2019-02-02 21:50:56] Ep. 13 : Up. 556000 : Sen. 3,573,328 : Cost 40.16275787 : Time 493.08s : 39664.81 words/s : L.r. 5.0891e-05
[2019-02-02 21:58:24] Ep. 13 : Up. 557000 : Sen. 4,848,352 : Cost 39.72497940 : Time 447.56s : 43685.57 words/s : L.r. 5.0846e-05
[2019-02-02 22:05:47] Ep. 13 : Up. 558000 : Sen. 6,117,813 : Cost 39.35236740 : Time 442.93s : 43586.18 words/s : L.r. 5.0800e-05
[2019-02-02 22:13:12] Ep. 13 : Up. 559000 : Sen. 7,385,035 : Cost 39.53421783 : Time 444.92s : 43512.24 words/s : L.r. 5.0755e-05
[2019-02-02 22:20:38] Ep. 13 : Up. 560000 : Sen. 8,638,997 : Cost 40.06506729 : Time 446.39s : 43633.10 words/s : L.r. 5.0709e-05
[2019-02-02 22:20:38] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-02 22:20:39] Saving model weights and runtime parameters to ./model.npz
[2019-02-02 22:20:41] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-02 22:20:44] [valid] Ep. 13 : Up. 560000 : ce-mean-words : 1.55267 : stalled 1 times (last best: 1.55206)
[2019-02-02 22:21:20] [valid] Ep. 13 : Up. 560000 : bleu-detok : 26.0091 : stalled 3 times (last best: 26.2383)
[2019-02-02 22:28:46] Ep. 13 : Up. 561000 : Sen. 9,912,550 : Cost 39.46085358 : Time 488.09s : 39757.95 words/s : L.r. 5.0664e-05
[2019-02-02 22:36:11] Ep. 13 : Up. 562000 : Sen. 11,169,428 : Cost 39.83577347 : Time 444.28s : 43614.16 words/s : L.r. 5.0619e-05
[2019-02-02 22:43:37] Ep. 13 : Up. 563000 : Sen. 12,455,963 : Cost 39.34234619 : Time 446.17s : 43688.51 words/s : L.r. 5.0574e-05
[2019-02-04 12:03:26] [marian] Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-04 12:03:26] [marian] Running on gpu-e-60 as process 89948 with command line:
[2019-02-04 12:03:26] [marian] /home/cs-grun1/marian/marian-dev/build-spm/marian --model ./model.npz --type transformer --train-sets /home/cs-grun1/wmt19/data/corpus00.nrm.en.gz /home/cs-grun1/wmt19/data/corpus00.nrm.cs.gz --shuffle-in-ram --vocabs ./vocab.encs.spm ./vocab.encs.spm --dim-vocabs 32000 32000 --max-length 120 --mini-batch-fit -w 10000 --mini-batch 1000 --maxi-batch 1000 --devices 0 1 2 3 --sync-sgd --layer-normalization --tied-embeddings-all --exponential-smoothing --transformer-dropout 0.1 --label-smoothing 0.1 --transformer-preprocess n --transformer-postprocess da --transformer-heads 8 --enc-depth 6 --dec-depth 6 --learn-rate 0.0003 --lr-report --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --disp-first 10 --valid-metrics ce-mean-words bleu-detok --valid-translation-output ./devset.bpe.cs.output --quiet-translation --valid-sets /home/cs-grun1/wmt19/data/newstest2016.en /home/cs-grun1/wmt19/data/newstest2016.cs --valid-mini-batch 32 --beam-size 8 --normalize 1.0 --early-stopping 5 --overwrite --keep-best --log ./train.log --valid-log ./valid.log
[2019-02-04 12:03:26] [config] after-batches: 0
[2019-02-04 12:03:26] [config] after-epochs: 0
[2019-02-04 12:03:26] [config] allow-unk: false
[2019-02-04 12:03:26] [config] beam-size: 8
[2019-02-04 12:03:26] [config] bert-class-symbol: "[CLS]"
[2019-02-04 12:03:26] [config] bert-mask-symbol: "[MASK]"
[2019-02-04 12:03:26] [config] bert-masking-fraction: 0.15
[2019-02-04 12:03:26] [config] bert-sep-symbol: "[SEP]"
[2019-02-04 12:03:26] [config] best-deep: false
[2019-02-04 12:03:26] [config] clip-gemm: 0
[2019-02-04 12:03:26] [config] clip-norm: 5
[2019-02-04 12:03:26] [config] cost-type: ce-mean
[2019-02-04 12:03:26] [config] cpu-threads: 0
[2019-02-04 12:03:26] [config] data-weighting-type: sentence
[2019-02-04 12:03:26] [config] dec-cell: gru
[2019-02-04 12:03:26] [config] dec-cell-base-depth: 2
[2019-02-04 12:03:26] [config] dec-cell-high-depth: 1
[2019-02-04 12:03:26] [config] dec-depth: 6
[2019-02-04 12:03:26] [config] devices:
[2019-02-04 12:03:26] [config]   - 0
[2019-02-04 12:03:26] [config]   - 1
[2019-02-04 12:03:26] [config]   - 2
[2019-02-04 12:03:26] [config]   - 3
[2019-02-04 12:03:26] [config] dim-emb: 512
[2019-02-04 12:03:26] [config] dim-rnn: 1024
[2019-02-04 12:03:26] [config] dim-vocabs:
[2019-02-04 12:03:26] [config]   - 32000
[2019-02-04 12:03:26] [config]   - 32000
[2019-02-04 12:03:26] [config] disp-first: 10
[2019-02-04 12:03:26] [config] disp-freq: 1000
[2019-02-04 12:03:26] [config] disp-label-counts: false
[2019-02-04 12:03:26] [config] dropout-rnn: 0
[2019-02-04 12:03:26] [config] dropout-src: 0
[2019-02-04 12:03:26] [config] dropout-trg: 0
[2019-02-04 12:03:26] [config] early-stopping: 5
[2019-02-04 12:03:26] [config] embedding-fix-src: false
[2019-02-04 12:03:26] [config] embedding-fix-trg: false
[2019-02-04 12:03:26] [config] embedding-normalization: false
[2019-02-04 12:03:26] [config] enc-cell: gru
[2019-02-04 12:03:26] [config] enc-cell-depth: 1
[2019-02-04 12:03:26] [config] enc-depth: 6
[2019-02-04 12:03:26] [config] enc-type: bidirectional
[2019-02-04 12:03:26] [config] exponential-smoothing: 0.0001
[2019-02-04 12:03:26] [config] grad-dropping-momentum: 0
[2019-02-04 12:03:26] [config] grad-dropping-rate: 0
[2019-02-04 12:03:26] [config] grad-dropping-warmup: 100
[2019-02-04 12:03:26] [config] guided-alignment: none
[2019-02-04 12:03:26] [config] guided-alignment-cost: mse
[2019-02-04 12:03:26] [config] guided-alignment-weight: 0.1
[2019-02-04 12:03:26] [config] ignore-model-config: false
[2019-02-04 12:03:26] [config] input-types:
[2019-02-04 12:03:26] [config]   []
[2019-02-04 12:03:26] [config] interpolate-env-vars: false
[2019-02-04 12:03:26] [config] keep-best: true
[2019-02-04 12:03:26] [config] label-smoothing: 0.1
[2019-02-04 12:03:26] [config] layer-normalization: true
[2019-02-04 12:03:26] [config] learn-rate: 0.0003
[2019-02-04 12:03:26] [config] log: ./train.log
[2019-02-04 12:03:26] [config] log-level: info
[2019-02-04 12:03:26] [config] lr-decay: 0
[2019-02-04 12:03:26] [config] lr-decay-freq: 50000
[2019-02-04 12:03:26] [config] lr-decay-inv-sqrt:
[2019-02-04 12:03:26] [config]   - 16000
[2019-02-04 12:03:26] [config] lr-decay-repeat-warmup: false
[2019-02-04 12:03:26] [config] lr-decay-reset-optimizer: false
[2019-02-04 12:03:26] [config] lr-decay-start:
[2019-02-04 12:03:26] [config]   - 10
[2019-02-04 12:03:26] [config]   - 1
[2019-02-04 12:03:26] [config] lr-decay-strategy: epoch+stalled
[2019-02-04 12:03:26] [config] lr-report: true
[2019-02-04 12:03:26] [config] lr-warmup: 16000
[2019-02-04 12:03:26] [config] lr-warmup-at-reload: false
[2019-02-04 12:03:26] [config] lr-warmup-cycle: false
[2019-02-04 12:03:26] [config] lr-warmup-start-rate: 0
[2019-02-04 12:03:26] [config] max-length: 120
[2019-02-04 12:03:26] [config] max-length-crop: false
[2019-02-04 12:03:26] [config] max-length-factor: 3
[2019-02-04 12:03:26] [config] maxi-batch: 1000
[2019-02-04 12:03:26] [config] maxi-batch-sort: trg
[2019-02-04 12:03:26] [config] mini-batch: 1000
[2019-02-04 12:03:26] [config] mini-batch-fit: true
[2019-02-04 12:03:26] [config] mini-batch-fit-step: 10
[2019-02-04 12:03:26] [config] mini-batch-overstuff: 1
[2019-02-04 12:03:26] [config] mini-batch-track-lr: false
[2019-02-04 12:03:26] [config] mini-batch-understuff: 1
[2019-02-04 12:03:26] [config] mini-batch-warmup: 0
[2019-02-04 12:03:26] [config] mini-batch-words: 0
[2019-02-04 12:03:26] [config] mini-batch-words-ref: 0
[2019-02-04 12:03:26] [config] model: ./model.npz
[2019-02-04 12:03:26] [config] multi-loss-type: sum
[2019-02-04 12:03:26] [config] multi-node: false
[2019-02-04 12:03:26] [config] multi-node-overlap: true
[2019-02-04 12:03:26] [config] n-best: false
[2019-02-04 12:03:26] [config] no-nccl: false
[2019-02-04 12:03:26] [config] no-reload: false
[2019-02-04 12:03:26] [config] no-restore-corpus: false
[2019-02-04 12:03:26] [config] no-shuffle: false
[2019-02-04 12:03:26] [config] normalize: 1
[2019-02-04 12:03:26] [config] optimizer: adam
[2019-02-04 12:03:26] [config] optimizer-delay: 1
[2019-02-04 12:03:26] [config] optimizer-params:
[2019-02-04 12:03:26] [config]   - 0.9
[2019-02-04 12:03:26] [config]   - 0.98
[2019-02-04 12:03:26] [config]   - 1e-09
[2019-02-04 12:03:26] [config] overwrite: true
[2019-02-04 12:03:26] [config] quiet: false
[2019-02-04 12:03:26] [config] quiet-translation: true
[2019-02-04 12:03:26] [config] relative-paths: false
[2019-02-04 12:03:26] [config] right-left: false
[2019-02-04 12:03:26] [config] save-freq: 5000
[2019-02-04 12:03:26] [config] seed: 0
[2019-02-04 12:03:26] [config] sentencepiece-alphas:
[2019-02-04 12:03:26] [config]   []
[2019-02-04 12:03:26] [config] sentencepiece-max-lines: 10000000
[2019-02-04 12:03:26] [config] sentencepiece-options: ""
[2019-02-04 12:03:26] [config] shuffle-in-ram: true
[2019-02-04 12:03:26] [config] skip: false
[2019-02-04 12:03:26] [config] sqlite: ""
[2019-02-04 12:03:26] [config] sqlite-drop: false
[2019-02-04 12:03:26] [config] sync-sgd: true
[2019-02-04 12:03:26] [config] tempdir: /tmp
[2019-02-04 12:03:26] [config] tied-embeddings: false
[2019-02-04 12:03:26] [config] tied-embeddings-all: true
[2019-02-04 12:03:26] [config] tied-embeddings-src: false
[2019-02-04 12:03:26] [config] train-sets:
[2019-02-04 12:03:26] [config]   - /home/cs-grun1/wmt19/data/corpus00.nrm.en.gz
[2019-02-04 12:03:26] [config]   - /home/cs-grun1/wmt19/data/corpus00.nrm.cs.gz
[2019-02-04 12:03:26] [config] transformer-aan-activation: swish
[2019-02-04 12:03:26] [config] transformer-aan-depth: 2
[2019-02-04 12:03:26] [config] transformer-aan-nogate: false
[2019-02-04 12:03:26] [config] transformer-decoder-autoreg: self-attention
[2019-02-04 12:03:26] [config] transformer-dim-aan: 2048
[2019-02-04 12:03:26] [config] transformer-dim-ffn: 2048
[2019-02-04 12:03:26] [config] transformer-dropout: 0.1
[2019-02-04 12:03:26] [config] transformer-dropout-attention: 0
[2019-02-04 12:03:26] [config] transformer-dropout-ffn: 0
[2019-02-04 12:03:26] [config] transformer-ffn-activation: swish
[2019-02-04 12:03:26] [config] transformer-ffn-depth: 2
[2019-02-04 12:03:26] [config] transformer-guided-alignment-layer: last
[2019-02-04 12:03:26] [config] transformer-heads: 8
[2019-02-04 12:03:26] [config] transformer-no-projection: false
[2019-02-04 12:03:26] [config] transformer-postprocess: da
[2019-02-04 12:03:26] [config] transformer-postprocess-emb: d
[2019-02-04 12:03:26] [config] transformer-preprocess: n
[2019-02-04 12:03:26] [config] transformer-tied-layers:
[2019-02-04 12:03:26] [config]   []
[2019-02-04 12:03:26] [config] transformer-train-positions: false
[2019-02-04 12:03:26] [config] type: transformer
[2019-02-04 12:03:26] [config] ulr: false
[2019-02-04 12:03:26] [config] ulr-dim-emb: 0
[2019-02-04 12:03:26] [config] ulr-dropout: 0
[2019-02-04 12:03:26] [config] ulr-keys-vectors: ""
[2019-02-04 12:03:26] [config] ulr-query-vectors: ""
[2019-02-04 12:03:26] [config] ulr-softmax-temperature: 1
[2019-02-04 12:03:26] [config] ulr-trainable-transformation: false
[2019-02-04 12:03:26] [config] valid-freq: 5000
[2019-02-04 12:03:26] [config] valid-log: ./valid.log
[2019-02-04 12:03:26] [config] valid-max-length: 1000
[2019-02-04 12:03:26] [config] valid-metrics:
[2019-02-04 12:03:26] [config]   - ce-mean-words
[2019-02-04 12:03:26] [config]   - bleu-detok
[2019-02-04 12:03:26] [config] valid-mini-batch: 32
[2019-02-04 12:03:26] [config] valid-sets:
[2019-02-04 12:03:26] [config]   - /home/cs-grun1/wmt19/data/newstest2016.en
[2019-02-04 12:03:26] [config]   - /home/cs-grun1/wmt19/data/newstest2016.cs
[2019-02-04 12:03:26] [config] valid-translation-output: ./devset.bpe.cs.output
[2019-02-04 12:03:26] [config] version: v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-04 12:03:26] [config] vocabs:
[2019-02-04 12:03:26] [config]   - ./vocab.encs.spm
[2019-02-04 12:03:26] [config]   - ./vocab.encs.spm
[2019-02-04 12:03:26] [config] word-penalty: 0
[2019-02-04 12:03:26] [config] workspace: 10000
[2019-02-04 12:03:26] [config] Loaded model has been created with Marian v1.7.7 50d64de 2019-01-25 08:52:30 -0800
[2019-02-04 12:03:26] Using synchronous training
[2019-02-04 12:03:26] [data] Loading SentencePiece vocabulary from file ./vocab.encs.spm
[2019-02-04 12:03:26] [data] Setting vocabulary size for input 0 to 32000
[2019-02-04 12:03:26] [data] Loading SentencePiece vocabulary from file ./vocab.encs.spm
[2019-02-04 12:03:26] [data] Setting vocabulary size for input 1 to 32000
[2019-02-04 12:03:27] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-04 12:03:27] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-04 12:03:27] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-04 12:03:28] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-04 12:03:29] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-04 12:03:29] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-04 12:03:29] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-04 12:03:30] [comm] NCCLCommunicator constructed successfully.
[2019-02-04 12:03:30] [training] Using 4 GPUs
[2019-02-04 12:03:30] [memory] Reserving 230 MB, device gpu0
[2019-02-04 12:03:30] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-04 12:03:30] [memory] Reserving 230 MB, device gpu0
[2019-02-04 12:03:51] [batching] Done. Typical MB size is 27396 target words
[2019-02-04 12:03:51] [memory] Extending reserved space to 10112 MB (device gpu0)
[2019-02-04 12:03:51] [memory] Extending reserved space to 10112 MB (device gpu1)
[2019-02-04 12:03:51] [memory] Extending reserved space to 10112 MB (device gpu2)
[2019-02-04 12:03:51] [memory] Extending reserved space to 10112 MB (device gpu3)
[2019-02-04 12:03:51] [comm] Using NCCL 2.3.7 for GPU communication
[2019-02-04 12:03:51] [comm] NCCLCommunicator constructed successfully.
[2019-02-04 12:03:51] [training] Using 4 GPUs
[2019-02-04 12:03:51] Loading model from ./model.npz.orig.npz
[2019-02-04 12:03:52] Loading model from ./model.npz.orig.npz
[2019-02-04 12:03:52] Loading model from ./model.npz.orig.npz
[2019-02-04 12:03:53] Loading model from ./model.npz.orig.npz
[2019-02-04 12:03:53] Loading Adam parameters from ./model.npz.optimizer.npz
[2019-02-04 12:03:54] [memory] Reserving 115 MB, device gpu0
[2019-02-04 12:03:54] [memory] Reserving 115 MB, device gpu1
[2019-02-04 12:03:54] [memory] Reserving 115 MB, device gpu2
[2019-02-04 12:03:54] [memory] Reserving 115 MB, device gpu3
[2019-02-04 12:03:54] [training] Model reloaded from ./model.npz
[2019-02-04 12:03:54] [data] Restoring the corpus state to epoch 13, batch 560000
[2019-02-04 12:03:54] [data] Shuffling data
[2019-02-04 12:04:56] [data] Done reading 58408180 sentences
[2019-02-04 12:05:00] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-04 12:15:49] Training started
[2019-02-04 12:15:49] [training] Batches are processed as 1 process(es) x 4 devices/process
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu3
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu1
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu2
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu0
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu2
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu3
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu1
[2019-02-04 12:15:49] [memory] Reserving 230 MB, device gpu0
[2019-02-04 12:15:49] Loading model from ./model.npz
[2019-02-04 12:15:50] [memory] Reserving 230 MB, device cpu0
[2019-02-04 12:15:50] [memory] Reserving 57 MB, device gpu0
[2019-02-04 12:15:50] [memory] Reserving 57 MB, device gpu1
[2019-02-04 12:15:50] [memory] Reserving 57 MB, device gpu2
[2019-02-04 12:15:50] [memory] Reserving 57 MB, device gpu3
[2019-02-04 12:23:15] Ep. 13 : Up. 561000 : Sen. 9,912,550 : Cost 39.45892715 : Time 1188.91s : 16322.02 words/s : L.r. 5.0664e-05
[2019-02-04 12:30:39] Ep. 13 : Up. 562000 : Sen. 11,169,428 : Cost 39.83327866 : Time 443.91s : 43650.11 words/s : L.r. 5.0619e-05
[2019-02-04 12:38:05] Ep. 13 : Up. 563000 : Sen. 12,455,963 : Cost 39.34035110 : Time 445.70s : 43734.54 words/s : L.r. 5.0574e-05
[2019-02-04 12:45:31] Ep. 13 : Up. 564000 : Sen. 13,717,660 : Cost 39.78511810 : Time 445.83s : 43586.98 words/s : L.r. 5.0529e-05
[2019-02-04 12:52:55] Ep. 13 : Up. 565000 : Sen. 14,964,788 : Cost 40.29730988 : Time 444.49s : 43624.13 words/s : L.r. 5.0484e-05
[2019-02-04 12:52:55] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 12:52:57] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 12:52:58] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 12:53:02] [valid] Ep. 13 : Up. 565000 : ce-mean-words : 1.5525 : stalled 1 times (last best: 1.55206)
[2019-02-04 12:53:41] [valid] Ep. 13 : Up. 565000 : bleu-detok : 25.9506 : stalled 3 times (last best: 26.2383)
[2019-02-04 13:01:08] Ep. 13 : Up. 566000 : Sen. 16,222,592 : Cost 40.21081543 : Time 492.70s : 39647.55 words/s : L.r. 5.0440e-05
[2019-02-04 13:08:32] Ep. 13 : Up. 567000 : Sen. 17,501,887 : Cost 39.09981918 : Time 443.83s : 43517.75 words/s : L.r. 5.0395e-05
[2019-02-04 13:15:57] Ep. 13 : Up. 568000 : Sen. 18,762,480 : Cost 39.88196564 : Time 445.46s : 43693.64 words/s : L.r. 5.0351e-05
[2019-02-04 13:23:22] Ep. 13 : Up. 569000 : Sen. 20,034,068 : Cost 39.36888123 : Time 444.41s : 43497.80 words/s : L.r. 5.0307e-05
[2019-02-04 13:30:47] Ep. 13 : Up. 570000 : Sen. 21,311,900 : Cost 39.44251251 : Time 445.43s : 43585.31 words/s : L.r. 5.0262e-05
[2019-02-04 13:30:47] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 13:30:48] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 13:30:50] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 13:30:53] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 13:30:54] [valid] Ep. 13 : Up. 570000 : ce-mean-words : 1.55164 : new best
[2019-02-04 13:31:33] [valid] Ep. 13 : Up. 570000 : bleu-detok : 26.1338 : stalled 4 times (last best: 26.2383)
[2019-02-04 13:38:56] Ep. 13 : Up. 571000 : Sen. 22,532,318 : Cost 40.70898819 : Time 488.88s : 39349.37 words/s : L.r. 5.0218e-05
[2019-02-04 13:46:20] Ep. 13 : Up. 572000 : Sen. 23,821,872 : Cost 39.12512970 : Time 444.33s : 43841.68 words/s : L.r. 5.0175e-05
[2019-02-04 13:53:46] Ep. 13 : Up. 573000 : Sen. 25,071,659 : Cost 40.15693283 : Time 445.71s : 43529.74 words/s : L.r. 5.0131e-05
[2019-02-04 14:01:11] Ep. 13 : Up. 574000 : Sen. 26,373,721 : Cost 38.46004105 : Time 444.39s : 43588.38 words/s : L.r. 5.0087e-05
[2019-02-04 14:08:38] Ep. 13 : Up. 575000 : Sen. 27,620,546 : Cost 40.78642654 : Time 447.03s : 43884.83 words/s : L.r. 5.0043e-05
[2019-02-04 14:08:38] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 14:08:39] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 14:08:40] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 14:08:43] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 14:08:44] [valid] Ep. 13 : Up. 575000 : ce-mean-words : 1.55052 : new best
[2019-02-04 14:09:22] [valid] Ep. 13 : Up. 575000 : bleu-detok : 26.0125 : stalled 5 times (last best: 26.2383)
[2019-02-04 14:16:45] Ep. 13 : Up. 576000 : Sen. 28,876,302 : Cost 39.98512268 : Time 487.80s : 39631.43 words/s : L.r. 5.0000e-05
[2019-02-04 14:24:11] Ep. 13 : Up. 577000 : Sen. 30,147,852 : Cost 39.44698715 : Time 445.80s : 43565.04 words/s : L.r. 4.9957e-05
[2019-02-04 14:31:36] Ep. 13 : Up. 578000 : Sen. 31,431,875 : Cost 39.20763397 : Time 444.38s : 43729.36 words/s : L.r. 4.9913e-05
[2019-02-04 14:38:58] Ep. 13 : Up. 579000 : Sen. 32,656,087 : Cost 40.71283340 : Time 442.56s : 43490.61 words/s : L.r. 4.9870e-05
[2019-02-04 14:46:25] Ep. 13 : Up. 580000 : Sen. 33,932,578 : Cost 39.67712021 : Time 446.67s : 43799.59 words/s : L.r. 4.9827e-05
[2019-02-04 14:46:25] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 14:46:26] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 14:46:27] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 14:46:31] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 14:46:32] [valid] Ep. 13 : Up. 580000 : ce-mean-words : 1.55011 : new best
[2019-02-04 14:47:09] [valid] Ep. 13 : Up. 580000 : bleu-detok : 25.9546 : stalled 6 times (last best: 26.2383)
[2019-02-04 14:54:34] Ep. 13 : Up. 581000 : Sen. 35,193,425 : Cost 39.81877899 : Time 488.88s : 39603.09 words/s : L.r. 4.9784e-05
[2019-02-04 15:02:02] Ep. 13 : Up. 582000 : Sen. 36,471,165 : Cost 39.62075806 : Time 448.12s : 43752.01 words/s : L.r. 4.9742e-05
[2019-02-04 15:09:25] Ep. 13 : Up. 583000 : Sen. 37,736,019 : Cost 39.64480972 : Time 443.36s : 43657.13 words/s : L.r. 4.9699e-05
[2019-02-04 15:16:48] Ep. 13 : Up. 584000 : Sen. 38,987,544 : Cost 39.93606949 : Time 442.68s : 43533.71 words/s : L.r. 4.9656e-05
[2019-02-04 15:24:12] Ep. 13 : Up. 585000 : Sen. 40,277,832 : Cost 38.91045380 : Time 444.27s : 43655.57 words/s : L.r. 4.9614e-05
[2019-02-04 15:24:12] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 15:24:13] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 15:24:15] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 15:24:18] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 15:24:19] [valid] Ep. 13 : Up. 585000 : ce-mean-words : 1.54948 : new best
[2019-02-04 15:24:57] [valid] Ep. 13 : Up. 585000 : bleu-detok : 25.9573 : stalled 7 times (last best: 26.2383)
[2019-02-04 15:32:23] Ep. 13 : Up. 586000 : Sen. 41,511,046 : Cost 40.97192764 : Time 490.76s : 39652.51 words/s : L.r. 4.9572e-05
[2019-02-04 15:39:48] Ep. 13 : Up. 587000 : Sen. 42,810,438 : Cost 38.56017685 : Time 445.18s : 43553.91 words/s : L.r. 4.9529e-05
[2019-02-04 15:47:11] Ep. 13 : Up. 588000 : Sen. 44,044,433 : Cost 40.49159241 : Time 443.45s : 43612.27 words/s : L.r. 4.9487e-05
[2019-02-04 15:54:39] Ep. 13 : Up. 589000 : Sen. 45,300,642 : Cost 40.24863434 : Time 447.90s : 43683.62 words/s : L.r. 4.9445e-05
[2019-02-04 16:02:05] Ep. 13 : Up. 590000 : Sen. 46,589,912 : Cost 39.24477005 : Time 445.64s : 43710.13 words/s : L.r. 4.9403e-05
[2019-02-04 16:02:05] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 16:02:06] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 16:02:08] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 16:02:11] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 16:02:12] [valid] Ep. 13 : Up. 590000 : ce-mean-words : 1.54907 : new best
[2019-02-04 16:02:50] [valid] Ep. 13 : Up. 590000 : bleu-detok : 25.9797 : stalled 8 times (last best: 26.2383)
[2019-02-04 16:10:14] Ep. 13 : Up. 591000 : Sen. 47,831,330 : Cost 40.43262100 : Time 489.29s : 39576.32 words/s : L.r. 4.9361e-05
[2019-02-04 16:17:39] Ep. 13 : Up. 592000 : Sen. 49,116,309 : Cost 39.09534836 : Time 444.30s : 43622.40 words/s : L.r. 4.9320e-05
[2019-02-04 16:25:03] Ep. 13 : Up. 593000 : Sen. 50,362,570 : Cost 40.29961014 : Time 444.67s : 43604.96 words/s : L.r. 4.9278e-05
[2019-02-04 16:32:26] Ep. 13 : Up. 594000 : Sen. 51,635,958 : Cost 39.13586807 : Time 443.05s : 43503.49 words/s : L.r. 4.9237e-05
[2019-02-04 16:39:53] Ep. 13 : Up. 595000 : Sen. 52,899,745 : Cost 40.03994751 : Time 447.13s : 43735.86 words/s : L.r. 4.9195e-05
[2019-02-04 16:39:53] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 16:39:55] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 16:39:56] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 16:40:00] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 16:40:00] [valid] Ep. 13 : Up. 595000 : ce-mean-words : 1.54873 : new best
[2019-02-04 16:40:39] [valid] Ep. 13 : Up. 595000 : bleu-detok : 25.9281 : stalled 9 times (last best: 26.2383)
[2019-02-04 16:48:05] Ep. 13 : Up. 596000 : Sen. 54,172,475 : Cost 39.66087723 : Time 491.45s : 39618.70 words/s : L.r. 4.9154e-05
[2019-02-04 16:55:30] Ep. 13 : Up. 597000 : Sen. 55,450,421 : Cost 39.31937790 : Time 444.88s : 43601.72 words/s : L.r. 4.9113e-05
[2019-02-04 17:02:57] Ep. 13 : Up. 598000 : Sen. 56,704,604 : Cost 40.34552002 : Time 447.54s : 43736.12 words/s : L.r. 4.9072e-05
[2019-02-04 17:10:23] Ep. 13 : Up. 599000 : Sen. 57,973,630 : Cost 39.66460419 : Time 445.93s : 43522.88 words/s : L.r. 4.9031e-05
[2019-02-04 17:12:19] Seen 58282224 samples
[2019-02-04 17:12:19] Starting epoch 14
[2019-02-04 17:12:19] [data] Shuffling data
[2019-02-04 17:12:22] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-04 17:18:55] Ep. 14 : Up. 600000 : Sen. 899,900 : Cost 39.91201782 : Time 511.45s : 36515.49 words/s : L.r. 4.8990e-05
[2019-02-04 17:18:55] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 17:18:56] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 17:18:57] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 17:19:01] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 17:19:01] [valid] Ep. 14 : Up. 600000 : ce-mean-words : 1.54799 : new best
[2019-02-04 17:19:40] [valid] Ep. 14 : Up. 600000 : bleu-detok : 26.0085 : stalled 10 times (last best: 26.2383)
[2019-02-04 17:27:06] Ep. 14 : Up. 601000 : Sen. 2,185,196 : Cost 39.06668091 : Time 491.62s : 39536.08 words/s : L.r. 4.8949e-05
[2019-02-04 17:34:33] Ep. 14 : Up. 602000 : Sen. 3,431,190 : Cost 40.23829651 : Time 446.26s : 43598.44 words/s : L.r. 4.8908e-05
[2019-02-04 17:41:57] Ep. 14 : Up. 603000 : Sen. 4,687,636 : Cost 39.99302673 : Time 444.25s : 43688.26 words/s : L.r. 4.8868e-05
[2019-02-04 17:49:21] Ep. 14 : Up. 604000 : Sen. 5,978,786 : Cost 38.76969528 : Time 444.53s : 43571.09 words/s : L.r. 4.8827e-05
[2019-02-04 17:56:46] Ep. 14 : Up. 605000 : Sen. 7,224,654 : Cost 40.16606522 : Time 444.42s : 43623.89 words/s : L.r. 4.8787e-05
[2019-02-04 17:56:46] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 17:56:47] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 17:56:48] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 17:56:52] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 17:56:53] [valid] Ep. 14 : Up. 605000 : ce-mean-words : 1.54796 : new best
[2019-02-04 17:57:31] [valid] Ep. 14 : Up. 605000 : bleu-detok : 25.9669 : stalled 11 times (last best: 26.2383)
[2019-02-04 18:04:58] Ep. 14 : Up. 606000 : Sen. 8,501,945 : Cost 39.66070557 : Time 492.62s : 39806.90 words/s : L.r. 4.8747e-05
[2019-02-04 18:12:22] Ep. 14 : Up. 607000 : Sen. 9,758,036 : Cost 39.62588882 : Time 443.82s : 43451.48 words/s : L.r. 4.8707e-05
[2019-02-04 18:19:48] Ep. 14 : Up. 608000 : Sen. 11,038,490 : Cost 39.30102158 : Time 445.99s : 43603.13 words/s : L.r. 4.8666e-05
[2019-02-04 18:27:14] Ep. 14 : Up. 609000 : Sen. 12,300,813 : Cost 39.94159317 : Time 445.64s : 43734.54 words/s : L.r. 4.8626e-05
[2019-02-04 18:34:37] Ep. 14 : Up. 610000 : Sen. 13,573,283 : Cost 39.07176590 : Time 442.76s : 43585.00 words/s : L.r. 4.8587e-05
[2019-02-04 18:34:37] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 18:34:38] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 18:34:39] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 18:34:43] [valid] Ep. 14 : Up. 610000 : ce-mean-words : 1.54803 : stalled 1 times (last best: 1.54796)
[2019-02-04 18:35:21] [valid] Ep. 14 : Up. 610000 : bleu-detok : 25.9465 : stalled 12 times (last best: 26.2383)
[2019-02-04 18:42:47] Ep. 14 : Up. 611000 : Sen. 14,842,394 : Cost 39.62561417 : Time 490.46s : 39632.68 words/s : L.r. 4.8547e-05
[2019-02-04 18:50:10] Ep. 14 : Up. 612000 : Sen. 16,087,512 : Cost 40.10707474 : Time 443.32s : 43533.76 words/s : L.r. 4.8507e-05
[2019-02-04 18:57:36] Ep. 14 : Up. 613000 : Sen. 17,355,381 : Cost 39.89157104 : Time 445.95s : 43832.23 words/s : L.r. 4.8468e-05
[2019-02-04 19:05:02] Ep. 14 : Up. 614000 : Sen. 18,614,799 : Cost 39.85327148 : Time 445.98s : 43589.63 words/s : L.r. 4.8428e-05
[2019-02-04 19:12:28] Ep. 14 : Up. 615000 : Sen. 19,890,568 : Cost 39.35028839 : Time 445.36s : 43524.69 words/s : L.r. 4.8389e-05
[2019-02-04 19:12:28] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 19:12:29] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 19:12:30] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 19:12:34] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 19:12:35] [valid] Ep. 14 : Up. 615000 : ce-mean-words : 1.54785 : new best
[2019-02-04 19:13:14] [valid] Ep. 14 : Up. 615000 : bleu-detok : 25.8763 : stalled 13 times (last best: 26.2383)
[2019-02-04 19:20:39] Ep. 14 : Up. 616000 : Sen. 21,155,846 : Cost 39.68536377 : Time 491.09s : 39546.96 words/s : L.r. 4.8349e-05
[2019-02-04 19:28:04] Ep. 14 : Up. 617000 : Sen. 22,404,887 : Cost 40.19434357 : Time 445.15s : 43669.38 words/s : L.r. 4.8310e-05
[2019-02-04 19:35:27] Ep. 14 : Up. 618000 : Sen. 23,705,536 : Cost 38.46050262 : Time 442.97s : 43541.44 words/s : L.r. 4.8271e-05
[2019-02-04 19:42:53] Ep. 14 : Up. 619000 : Sen. 24,940,646 : Cost 40.72164154 : Time 446.16s : 43663.42 words/s : L.r. 4.8232e-05
[2019-02-04 19:50:20] Ep. 14 : Up. 620000 : Sen. 26,233,426 : Cost 39.13315964 : Time 447.47s : 43653.98 words/s : L.r. 4.8193e-05
[2019-02-04 19:50:20] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 19:50:22] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 19:50:23] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 19:50:27] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 19:50:28] [valid] Ep. 14 : Up. 620000 : ce-mean-words : 1.54773 : new best
[2019-02-04 19:51:05] [valid] Ep. 14 : Up. 620000 : bleu-detok : 25.9704 : stalled 14 times (last best: 26.2383)
[2019-02-04 19:58:31] Ep. 14 : Up. 621000 : Sen. 27,493,447 : Cost 39.80086517 : Time 490.66s : 39674.00 words/s : L.r. 4.8154e-05
[2019-02-04 20:05:54] Ep. 14 : Up. 622000 : Sen. 28,763,469 : Cost 39.49115372 : Time 442.82s : 43604.87 words/s : L.r. 4.8116e-05
[2019-02-04 20:13:20] Ep. 14 : Up. 623000 : Sen. 30,025,664 : Cost 39.74340439 : Time 446.23s : 43627.75 words/s : L.r. 4.8077e-05
[2019-02-04 20:20:43] Ep. 14 : Up. 624000 : Sen. 31,284,674 : Cost 39.63547516 : Time 442.39s : 43481.19 words/s : L.r. 4.8038e-05
[2019-02-04 20:28:11] Ep. 14 : Up. 625000 : Sen. 32,542,688 : Cost 40.32704544 : Time 448.47s : 43833.64 words/s : L.r. 4.8000e-05
[2019-02-04 20:28:11] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 20:28:12] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 20:28:14] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 20:28:17] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 20:28:18] [valid] Ep. 14 : Up. 625000 : ce-mean-words : 1.54712 : new best
[2019-02-04 20:28:56] [valid] Ep. 14 : Up. 625000 : bleu-detok : 26.0168 : stalled 15 times (last best: 26.2383)
[2019-02-04 20:36:20] Ep. 14 : Up. 626000 : Sen. 33,823,466 : Cost 39.10143280 : Time 489.03s : 39625.49 words/s : L.r. 4.7962e-05
[2019-02-04 20:43:46] Ep. 14 : Up. 627000 : Sen. 35,076,794 : Cost 40.10439682 : Time 445.49s : 43626.03 words/s : L.r. 4.7923e-05
[2019-02-04 20:51:10] Ep. 14 : Up. 628000 : Sen. 36,370,608 : Cost 38.60044098 : Time 444.55s : 43475.24 words/s : L.r. 4.7885e-05
[2019-02-04 20:58:36] Ep. 14 : Up. 629000 : Sen. 37,627,043 : Cost 40.26934433 : Time 446.26s : 43761.45 words/s : L.r. 4.7847e-05
[2019-02-04 21:06:02] Ep. 14 : Up. 630000 : Sen. 38,890,674 : Cost 39.68292236 : Time 445.15s : 43662.96 words/s : L.r. 4.7809e-05
[2019-02-04 21:06:02] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 21:06:03] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 21:06:04] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 21:06:08] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 21:06:08] [valid] Ep. 14 : Up. 630000 : ce-mean-words : 1.54656 : new best
[2019-02-04 21:06:45] [valid] Ep. 14 : Up. 630000 : bleu-detok : 26.0056 : stalled 16 times (last best: 26.2383)
[2019-02-04 21:14:08] Ep. 14 : Up. 631000 : Sen. 40,125,153 : Cost 40.47176361 : Time 486.88s : 39679.84 words/s : L.r. 4.7771e-05
[2019-02-04 21:21:33] Ep. 14 : Up. 632000 : Sen. 41,409,323 : Cost 39.21504974 : Time 444.39s : 43651.40 words/s : L.r. 4.7733e-05
[2019-02-04 21:28:57] Ep. 14 : Up. 633000 : Sen. 42,665,129 : Cost 39.89318466 : Time 444.68s : 43594.74 words/s : L.r. 4.7696e-05
[2019-02-04 21:36:21] Ep. 14 : Up. 634000 : Sen. 43,925,574 : Cost 39.67938614 : Time 443.96s : 43651.46 words/s : L.r. 4.7658e-05
[2019-02-04 21:43:48] Ep. 14 : Up. 635000 : Sen. 45,186,706 : Cost 40.02268982 : Time 446.34s : 43722.37 words/s : L.r. 4.7621e-05
[2019-02-04 21:43:48] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 21:43:49] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 21:43:50] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 21:43:54] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 21:43:55] [valid] Ep. 14 : Up. 635000 : ce-mean-words : 1.54615 : new best
[2019-02-04 21:44:34] [valid] Ep. 14 : Up. 635000 : bleu-detok : 25.9921 : stalled 17 times (last best: 26.2383)
[2019-02-04 21:51:57] Ep. 14 : Up. 636000 : Sen. 46,452,668 : Cost 39.21066666 : Time 488.77s : 39262.15 words/s : L.r. 4.7583e-05
[2019-02-04 21:59:24] Ep. 14 : Up. 637000 : Sen. 47,721,216 : Cost 39.74916077 : Time 447.42s : 43689.30 words/s : L.r. 4.7546e-05
[2019-02-04 22:06:47] Ep. 14 : Up. 638000 : Sen. 48,982,226 : Cost 39.80130386 : Time 443.47s : 43675.59 words/s : L.r. 4.7508e-05
[2019-02-04 22:14:13] Ep. 14 : Up. 639000 : Sen. 50,243,885 : Cost 39.90222549 : Time 445.63s : 43696.91 words/s : L.r. 4.7471e-05
[2019-02-04 22:21:36] Ep. 14 : Up. 640000 : Sen. 51,508,657 : Cost 39.37254333 : Time 442.78s : 43498.75 words/s : L.r. 4.7434e-05
[2019-02-04 22:21:36] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 22:21:37] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 22:21:38] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 22:21:42] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 22:21:43] [valid] Ep. 14 : Up. 640000 : ce-mean-words : 1.54576 : new best
[2019-02-04 22:22:21] [valid] Ep. 14 : Up. 640000 : bleu-detok : 26.1691 : stalled 18 times (last best: 26.2383)
[2019-02-04 22:29:48] Ep. 14 : Up. 641000 : Sen. 52,768,699 : Cost 40.05809784 : Time 492.54s : 39670.65 words/s : L.r. 4.7397e-05
[2019-02-04 22:37:14] Ep. 14 : Up. 642000 : Sen. 54,051,032 : Cost 39.26136017 : Time 445.77s : 43575.19 words/s : L.r. 4.7360e-05
[2019-02-04 22:44:42] Ep. 14 : Up. 643000 : Sen. 55,319,133 : Cost 39.89667511 : Time 448.06s : 43683.72 words/s : L.r. 4.7323e-05
[2019-02-04 22:52:04] Ep. 14 : Up. 644000 : Sen. 56,590,935 : Cost 39.28642273 : Time 442.22s : 43512.79 words/s : L.r. 4.7287e-05
[2019-02-04 22:59:32] Ep. 14 : Up. 645000 : Sen. 57,847,309 : Cost 40.14143372 : Time 447.75s : 43776.85 words/s : L.r. 4.7250e-05
[2019-02-04 22:59:32] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 22:59:33] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 22:59:35] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 22:59:38] Saving model weights and runtime parameters to ./model.npz.best-ce-mean-words.npz
[2019-02-04 22:59:39] [valid] Ep. 14 : Up. 645000 : ce-mean-words : 1.54542 : new best
[2019-02-04 23:00:16] [valid] Ep. 14 : Up. 645000 : bleu-detok : 25.9866 : stalled 19 times (last best: 26.2383)
[2019-02-04 23:02:58] Seen 58282224 samples
[2019-02-04 23:02:58] Starting epoch 15
[2019-02-04 23:02:58] [data] Shuffling data
[2019-02-04 23:03:01] [data] Done shuffling 58408180 sentences (cached in RAM)
[2019-02-04 23:08:49] Ep. 15 : Up. 646000 : Sen. 785,502 : Cost 39.65499878 : Time 556.46s : 33633.08 words/s : L.r. 4.7213e-05
[2019-02-04 23:16:11] Ep. 15 : Up. 647000 : Sen. 2,043,440 : Cost 39.41191483 : Time 442.29s : 43494.13 words/s : L.r. 4.7177e-05
[2019-02-04 23:23:36] Ep. 15 : Up. 648000 : Sen. 3,313,289 : Cost 39.36585236 : Time 445.07s : 43606.41 words/s : L.r. 4.7140e-05
[2019-02-04 23:31:03] Ep. 15 : Up. 649000 : Sen. 4,573,364 : Cost 40.13103485 : Time 446.88s : 43795.03 words/s : L.r. 4.7104e-05
[2019-02-04 23:38:27] Ep. 15 : Up. 650000 : Sen. 5,841,575 : Cost 39.31846619 : Time 443.69s : 43572.53 words/s : L.r. 4.7068e-05
[2019-02-04 23:38:27] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-04 23:38:28] Saving model weights and runtime parameters to ./model.npz
[2019-02-04 23:38:29] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-04 23:38:33] [valid] Ep. 15 : Up. 650000 : ce-mean-words : 1.54571 : stalled 1 times (last best: 1.54542)
[2019-02-04 23:39:12] [valid] Ep. 15 : Up. 650000 : bleu-detok : 26.0767 : stalled 20 times (last best: 26.2383)
[2019-02-04 23:46:37] Ep. 15 : Up. 651000 : Sen. 7,101,185 : Cost 39.78768539 : Time 490.62s : 39557.76 words/s : L.r. 4.7032e-05
[2019-02-04 23:54:02] Ep. 15 : Up. 652000 : Sen. 8,372,659 : Cost 39.31119156 : Time 445.14s : 43531.40 words/s : L.r. 4.6996e-05
[2019-02-05 00:01:27] Ep. 15 : Up. 653000 : Sen. 9,619,735 : Cost 40.11455536 : Time 445.08s : 43689.94 words/s : L.r. 4.6960e-05
[2019-02-05 00:08:52] Ep. 15 : Up. 654000 : Sen. 10,914,032 : Cost 38.81807709 : Time 444.76s : 43658.27 words/s : L.r. 4.6924e-05
[2019-02-05 00:16:16] Ep. 15 : Up. 655000 : Sen. 12,151,046 : Cost 40.33757782 : Time 443.73s : 43638.26 words/s : L.r. 4.6888e-05
[2019-02-05 00:16:16] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-05 00:16:17] Saving model weights and runtime parameters to ./model.npz
[2019-02-05 00:16:19] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-05 00:16:22] [valid] Ep. 15 : Up. 655000 : ce-mean-words : 1.54584 : stalled 2 times (last best: 1.54542)
[2019-02-05 00:17:02] [valid] Ep. 15 : Up. 655000 : bleu-detok : 26.0264 : stalled 21 times (last best: 26.2383)
[2019-02-05 00:24:28] Ep. 15 : Up. 656000 : Sen. 13,401,678 : Cost 40.39342499 : Time 491.75s : 39612.03 words/s : L.r. 4.6852e-05
[2019-02-05 00:31:53] Ep. 15 : Up. 657000 : Sen. 14,689,897 : Cost 38.62476349 : Time 445.29s : 43449.66 words/s : L.r. 4.6816e-05
[2019-02-05 00:39:17] Ep. 15 : Up. 658000 : Sen. 15,943,623 : Cost 39.98592758 : Time 444.19s : 43679.13 words/s : L.r. 4.6781e-05
[2019-02-05 00:46:44] Ep. 15 : Up. 659000 : Sen. 17,232,283 : Cost 39.14357376 : Time 446.72s : 43674.98 words/s : L.r. 4.6745e-05
[2019-02-05 00:54:09] Ep. 15 : Up. 660000 : Sen. 18,469,739 : Cost 40.66743088 : Time 445.61s : 43765.28 words/s : L.r. 4.6710e-05
[2019-02-05 00:54:09] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-05 00:54:11] Saving model weights and runtime parameters to ./model.npz
[2019-02-05 00:54:12] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-05 00:54:15] [valid] Ep. 15 : Up. 660000 : ce-mean-words : 1.54581 : stalled 3 times (last best: 1.54542)
[2019-02-05 00:54:56] [valid] Ep. 15 : Up. 660000 : bleu-detok : 25.9575 : stalled 22 times (last best: 26.2383)
[2019-02-05 01:02:20] Ep. 15 : Up. 661000 : Sen. 19,740,142 : Cost 39.30229568 : Time 490.13s : 39371.93 words/s : L.r. 4.6675e-05
[2019-02-05 01:09:46] Ep. 15 : Up. 662000 : Sen. 21,013,556 : Cost 39.42262268 : Time 446.16s : 43656.45 words/s : L.r. 4.6639e-05
[2019-02-05 01:17:12] Ep. 15 : Up. 663000 : Sen. 22,283,049 : Cost 39.66905975 : Time 446.34s : 43713.89 words/s : L.r. 4.6604e-05
[2019-02-05 01:24:36] Ep. 15 : Up. 664000 : Sen. 23,522,312 : Cost 40.32412720 : Time 443.99s : 43563.31 words/s : L.r. 4.6569e-05
[2019-02-05 01:32:00] Ep. 15 : Up. 665000 : Sen. 24,817,305 : Cost 38.50065613 : Time 444.35s : 43494.45 words/s : L.r. 4.6534e-05
[2019-02-05 01:32:00] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-05 01:32:02] Saving model weights and runtime parameters to ./model.npz
[2019-02-05 01:32:03] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-05 01:32:07] [valid] Ep. 15 : Up. 665000 : ce-mean-words : 1.5459 : stalled 4 times (last best: 1.54542)
[2019-02-05 01:32:46] [valid] Ep. 15 : Up. 665000 : bleu-detok : 25.9729 : stalled 23 times (last best: 26.2383)
[2019-02-05 01:40:12] Ep. 15 : Up. 666000 : Sen. 26,077,685 : Cost 39.99228668 : Time 491.89s : 39645.90 words/s : L.r. 4.6499e-05
[2019-02-05 01:47:36] Ep. 15 : Up. 667000 : Sen. 27,309,798 : Cost 40.72215271 : Time 443.57s : 43637.25 words/s : L.r. 4.6464e-05
[2019-02-05 01:55:00] Ep. 15 : Up. 668000 : Sen. 28,607,272 : Cost 38.61730957 : Time 444.27s : 43790.45 words/s : L.r. 4.6429e-05
[2019-02-05 02:02:27] Ep. 15 : Up. 669000 : Sen. 29,860,962 : Cost 40.22146606 : Time 447.12s : 43615.41 words/s : L.r. 4.6395e-05
[2019-02-05 02:09:52] Ep. 15 : Up. 670000 : Sen. 31,128,821 : Cost 39.38370895 : Time 444.34s : 43519.45 words/s : L.r. 4.6360e-05
[2019-02-05 02:09:52] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-05 02:09:53] Saving model weights and runtime parameters to ./model.npz
[2019-02-05 02:09:54] Saving Adam parameters to ./model.npz.optimizer.npz
[2019-02-05 02:09:58] [valid] Ep. 15 : Up. 670000 : ce-mean-words : 1.5456 : stalled 5 times (last best: 1.54542)
[2019-02-05 02:10:37] [valid] Ep. 15 : Up. 670000 : bleu-detok : 25.9093 : stalled 24 times (last best: 26.2383)
[2019-02-05 02:10:37] Training finished
[2019-02-05 02:10:38] Saving model weights and runtime parameters to ./model.npz.orig.npz
[2019-02-05 02:10:39] Saving model weights and runtime parameters to ./model.npz
[2019-02-05 02:10:40] Saving Adam parameters to ./model.npz.optimizer.npz
